import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import os
import shutil

# === FILE PATHS ===
csv_path = './data/combined.csv'
temp_parquet_dir = './data/temp_chunks'
final_parquet_path = './data/combined.parquet'
target_map_path = './data/cleaned_parts/target_map.csv'

# === STEP -1: Fail-fast Precheck ===
print("ğŸ” Running prechecks...")

if not os.path.exists(csv_path):
    raise FileNotFoundError(f"âŒ CSV file not found: {csv_path}")

if not os.path.exists(target_map_path):
    raise FileNotFoundError(f"âŒ Target map CSV not found: {target_map_path}")

final_dir = os.path.dirname(final_parquet_path)
if not os.path.exists(final_dir):
    print(f"ğŸ“ Final output directory not found. Creating: {final_dir}")
    os.makedirs(final_dir, exist_ok=True)

try:
    test_file = os.path.join(final_dir, '.__test_write__.tmp')
    with open(test_file, 'w') as f:
        f.write('test')
    os.remove(test_file)
except Exception as e:
    raise PermissionError(f"âŒ Cannot write to directory: {final_dir} â€” {e}")

# === STEP 0: Ensure folders exist ===
os.makedirs('./data', exist_ok=True)
os.makedirs('./data/cleaned_parts', exist_ok=True)

# === STEP 0.5: Clean previous temp and output files ===
if os.path.exists(temp_parquet_dir):
    print(f"ğŸ§¹ Removing existing temp folder: {temp_parquet_dir}")
    shutil.rmtree(temp_parquet_dir)
os.makedirs(temp_parquet_dir, exist_ok=True)
print("âœ… Temp chunk folder created fresh.")

if os.path.exists(final_parquet_path):
    print(f"ğŸ—‘ï¸ Removing existing Parquet file: {final_parquet_path}")
    os.remove(final_parquet_path)
    print("âœ… Existing combined.parquet removed.")

# === STEP 1: Convert CSV to temporary Parquet chunks ===
print("ğŸ” Converting CSV to temporary Parquet chunks...")
chunk_files = []
for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=1_000_000, dtype=str)):
    chunk = chunk.drop_duplicates()
    chunk_path = os.path.join(temp_parquet_dir, f"chunk_{i}.parquet")
    chunk.to_parquet(chunk_path, index=False, engine='pyarrow')
    chunk_files.append(chunk_path)
print(f"âœ… {len(chunk_files)} chunks written to {temp_parquet_dir}")

# === STEP 2: Load and prepare target map ===
print("ğŸ“¥ Loading target map...")
target_map = pd.read_csv(target_map_path, usecols=['CMPNT_CAT_CD_DESC', 'Final Category'], dtype=str)
target_map = target_map.drop_duplicates()
target_map['CMPNT_CAT_CD_DESC'] = target_map['CMPNT_CAT_CD_DESC'].astype('category')

# === STEP 3: Merge chunks and write to final Parquet (overwrite) ===
print("ğŸ” Merging chunks and writing to combined.parquet...")
writer = None

for chunk_file in chunk_files:
    chunk = pd.read_parquet(chunk_file)
    chunk = chunk.drop_duplicates()
    chunk['CMPNT_CAT_CD_DESC'] = chunk['CMPNT_CAT_CD_DESC'].astype('category')

    merged = chunk.merge(target_map, how='left', on='CMPNT_CAT_CD_DESC')
    table = pa.Table.from_pandas(merged)

    if writer is None:
        writer = pq.ParquetWriter(final_parquet_path, table.schema, compression='snappy')

    writer.write_table(table)

if writer:
    writer.close()
    print("âœ… Final Parquet file written at:", final_parquet_path)
else:
    print("âš ï¸ No chunks processed. Final file not written.")

# === STEP 4: Clean up temporary chunks ===
shutil.rmtree(temp_parquet_dir)
print("ğŸ§¹ Temporary chunk files deleted.")
