# pipeline_runner.py

from azureml.core import Workspace, Environment, Experiment
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data.datapath import DataPath
from azureml.data import OutputFileDatasetConfig
from azureml.core import Dataset
from azureml.core.authentication import ServicePrincipalAuthentication
import os
import sys
from azureml.data.datapath import DataPath
from azureml.core import Datastore

from datetime import datetime
today = datetime.today().strftime("%d%m%Y")

# === Load workspace ===
# === Hardcoded AML workspace config ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"

def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        print("Using Service Principal Authentication")
        tenant_id = args[1]
        client_id = args[2]
        client_secret = args[3]

        sp_auth = ServicePrincipalAuthentication(
            tenant_id=tenant_id,
            service_principal_id=client_id,
            service_principal_password=client_secret
        )

        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME,
            auth=sp_auth
        )
    else:
        print("Using default local authentication")
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME
        )

# === Initialize workspace ===
ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
#ws = Workspace.from_config(path=".azureml/dev_config.json")
compute_name = "llm-gpu-cluster-2"
env = Environment.get(workspace=ws, name="Bom_X_Evaluator")
default_ds = Datastore.get(ws, "xbomrefadlsg2")

run_config = RunConfiguration()
run_config.target = compute_name
run_config.environment = env

# === Output and input shared folders ===
data_step1_out = PipelineData(name="data_step1_out", is_directory=True)
data_step1a_mapped = PipelineData(name="data_step1a_mapped", is_directory=True)
data_step1a_unmapped = PipelineData(name="data_step1a_unmapped", is_directory=True)
data_step2_out = PipelineData(name="data_step2_out", is_directory=True)
data_step3_out = PipelineData(name="data_step3_out", is_directory=True)
data_step4_out = PipelineData(name="data_step4_out", is_directory=True)
#data_step1a_key_output = PipelineData(name="data_step1a_key_output", is_directory=True)
today = datetime.today().strftime("%d%m%Y")

#default_ds = ws.get_default_datastore()
today = datetime.today().strftime("%d%m%Y")

# Temporary output from Step 1a
step1a_key_output_temp = OutputFileDatasetConfig(name='step1a_key_output_temp')

# Final output to ADLS from Step 5
final_output_to_adls = OutputFileDatasetConfig(
    name='final_predictions_to_adls',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)


custom_output_path = DataPath(
    datastore=default_ds,
    path_on_datastore=f"hbom_category_prediction/hbom_category_prediction_{today}/"
)

today = datetime.today().strftime("%d%m%Y")

data_step1a_key_output = OutputFileDatasetConfig(
    name='data_step1a_key_output',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)

# === Upload high_conf_mapping.csv from local folder to datastore ===
#default_ds = ws.get_default_datastore()

# === Upload high_conf_mapping.csv from local folder to datastore ===
default_ds1 = ws.get_default_datastore()

# Upload file as before
default_ds1.upload_files(
    files=['./local_data/high_conf_direct_mapping.csv'],
    target_path='high_conf_map',   # <== IMPORTANT: Folder name
    overwrite=True,
    show_progress=True
)

# Register or create file dataset
high_conf_dataset = Dataset.File.from_files((default_ds1, 'high_conf_map'))
high_conf_mount = high_conf_dataset.as_named_input('high_conf_map').as_mount()

default_ds1.upload_files(
    files=['./local_data/abbreviation_expension_updated.csv'],
    target_path='abbrev_map',
    overwrite=True,
    show_progress=True
)
abbrev_dataset = Dataset.File.from_files((default_ds1, 'abbrev_map'))
abbrev_mount = abbrev_dataset.as_named_input('abbrev_map').as_mount()

# === Step 1: Data Pull ===
data_pull_step = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", data_step1_out],
    outputs=[data_step1_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 1a: High Confidence Merge ===
high_conf_merge_step = PythonScriptStep(
    name="Step 1a - High Confidence Merge",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    inputs=[data_step1_out, high_conf_mount],
    arguments=[
    "--input_path", data_step1_out,
    "--mapping_csv", high_conf_mount,
    "--mapped_output", data_step1a_mapped,
    "--needs_model_output", data_step1a_unmapped,
    "--key_output", step1a_key_output_temp
],
    outputs=[data_step1a_mapped, data_step1a_unmapped, step1a_key_output_temp],

    #outputs=[data_step1a_mapped, data_step1a_unmapped],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 2: Clean Text ===
clean_text_step = PythonScriptStep(
    name="Step 2 - Text Cleaning",
    script_name="step_2_clean_text.py",
    source_directory="scripts",
    inputs=[data_step2_out],
    arguments=["--input_path", data_step2_out, "--output_path", data_step3_out],
    outputs=[data_step3_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 3: Abbreviation Expansion ===
abbrev_expand_step = PythonScriptStep(
    name="Step 3 - Expand Abbreviations",
    script_name="step_3_expand_abbreviation.py",
    source_directory="scripts",
    inputs=[data_step1a_unmapped, abbrev_mount],  # ⬅️ changed from data_step2_out
    arguments=[
        "--input_path", data_step1a_unmapped,
        "--abbrev_map", abbrev_mount,
        "--output_path", data_step2_out   # output will now feed into cleaning step
    ],
    outputs=[data_step2_out],  # renamed for consistency
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 4: Feature Engineering ===
feature_eng_step = PythonScriptStep(
    name="Step 4 - Feature Engineering",
    script_name="step_4_feature_engineering.py",
    source_directory="scripts",
    inputs=[data_step3_out],
    arguments=["--input_path", data_step3_out, "--output_path", data_step4_out],
    outputs=[data_step4_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 5: Run Inference ===
inference_step = PythonScriptStep(
    name="Step 5 - Run Inference",
    script_name="step_5_run_inference.py",
    source_directory="scripts",
    inputs=[data_step4_out, data_step1a_mapped, step1a_key_output_temp],
    arguments=[
        "--input_path", data_step4_out,
        "--additional_mapped_dir", data_step1a_mapped,
        "--key_output_dir", step1a_key_output_temp,
        "--final_output_dir", final_output_to_adls
    ],
    outputs=[final_output_to_adls],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Build pipeline ===
pipeline = Pipeline(workspace=ws, steps=[
    data_pull_step,
    high_conf_merge_step,
    abbrev_expand_step,
    clean_text_step, 
    feature_eng_step,
    inference_step
])

pipeline.validate()

# === Submit pipeline ===
experiment = Experiment(ws, "full_inference_pipeline")
pipeline_run = experiment.submit(pipeline)
pipeline_run.wait_for_completion(show_output=True)
