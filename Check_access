from pydub import AudioSegment
import azure.cognitiveservices.speech as speechsdk

def translate_audio(audio_path, translated_wav, from_lang, to_lang):
    voice_map = {
        "fr": "fr-FR-DeniseNeural",
        "de": "de-DE-KatjaNeural",
        "es": "es-ES-ElviraNeural",
        "it": "it-IT-ElsaNeural",
        "pt": "pt-BR-FranciscaNeural",
        "zh-Hans": "zh-CN-XiaoxiaoNeural",
        "ja": "ja-JP-NanamiNeural",
        "ko": "ko-KR-SunHiNeural",
        "ru": "ru-RU-DariyaNeural",
        "hi": "hi-IN-SwaraNeural",
        "ar": "ar-EG-SalmaNeural",
        "tr": "tr-TR-EmelNeural",
        "nl": "nl-NL-ColetteNeural",
        "pl": "pl-PL-AgnieszkaNeural",
        "th": "th-TH-PremNeural",
        "sv": "sv-SE-SofieNeural",
        "cs": "cs-CZ-VlastaNeural",
        "hu": "hu-HU-NoemiNeural"
    }

    translation_config = speechsdk.translation.SpeechTranslationConfig(
        subscription=SPEECH_KEY,
        endpoint=CUSTOM_ENDPOINT,
        speech_recognition_language=from_lang
    )
    translation_config.add_target_language(to_lang)
    translation_config.voice_name = voice_map.get(to_lang, "fr-FR-DeniseNeural")

    audio_config = speechsdk.audio.AudioConfig(filename=audio_path)
    recognizer = speechsdk.translation.TranslationRecognizer(
        translation_config=translation_config, audio_config=audio_config
    )

    translated_chunks = []

    def handle_translation(evt):
        if to_lang in evt.result.translations:
            translated_chunks.append(evt.result.translations[to_lang])

    def handle_canceled(evt):
        print("CANCELED:", evt)

    recognizer.recognized.connect(handle_translation)
    recognizer.canceled.connect(handle_canceled)

    recognizer.start_continuous_recognition()
    recognizer.session_started.connect(lambda evt: print("Session started"))
    recognizer.session_stopped.connect(lambda evt: print("Session stopped"))

    # Wait until recognition is complete
    done = False

    def stop_cb(evt):
        nonlocal done
        done = True

    recognizer.session_stopped.connect(stop_cb)
    recognizer.canceled.connect(stop_cb)

    while not done:
        import time
        time.sleep(1)

    recognizer.stop_continuous_recognition()

    full_translation = " ".join(translated_chunks)

    # Synthesize translated text to audio
    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, endpoint=CUSTOM_ENDPOINT)
    speech_config.speech_synthesis_voice_name = translation_config.voice_name
    audio_out = speechsdk.audio.AudioOutputConfig(filename=translated_wav)

    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_out)
    synthesizer.speak_text_async(full_translation).get()

    # Pad if necessary
    original_audio = AudioSegment.from_wav(audio_path)
    translated_audio = AudioSegment.from_wav(translated_wav)

    if len(translated_audio) < len(original_audio):
        silence = AudioSegment.silent(duration=len(original_audio) - len(translated_audio))
        padded_audio = translated_audio + silence
        padded_audio.export(translated_wav, format="wav")

    return full_translation
