import os
import gradio as gr
import subprocess
import azure.cognitiveservices.speech as speechsdk
import uuid

# Azure Speech settings
SPEECH_KEY = os.environ.get("SPEECH__SERVICE__KEY")
CUSTOM_ENDPOINT = "https://<your-custom-subdomain>.cognitiveservices.azure.com/"

# Path to ffmpeg.exe
FFMPEG_PATH = r"C:\\Users\\<YourUsername>\\Downloads\\ffmpeg.exe"  # Update with your actual path

# Supported languages
SOURCE_LANGUAGES = {
    "English (US)": "en-US",
    "French (France)": "fr-FR",
    "German": "de-DE",
    "Italian": "it-IT",
    "Spanish": "es-ES",
    "Portuguese (Brazil)": "pt-BR",
    "Chinese (Mandarin)": "zh-CN",
    "Japanese": "ja-JP",
    "Korean": "ko-KR",
    "Russian": "ru-RU",
    "Hindi": "hi-IN",
    "Arabic": "ar-EG",
    "Turkish": "tr-TR",
    "Dutch": "nl-NL",
    "Polish": "pl-PL",
    "Thai": "th-TH",
    "Swedish": "sv-SE",
    "Czech": "cs-CZ",
    "Hungarian": "hu-HU"
}

TARGET_LANGUAGES = {
    "French": "fr",
    "German": "de",
    "Spanish": "es",
    "Italian": "it",
    "Portuguese": "pt",
    "Chinese": "zh-Hans",
    "Japanese": "ja",
    "Korean": "ko",
    "Russian": "ru",
    "Hindi": "hi",
    "Arabic": "ar",
    "Turkish": "tr",
    "Dutch": "nl",
    "Polish": "pl",
    "Thai": "th",
    "Swedish": "sv",
    "Czech": "cs",
    "Hungarian": "hu"
}

def extract_audio(video_path, audio_path):
    command = [
        FFMPEG_PATH, "-y", "-i", video_path,
        "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path
    ]
    subprocess.run(command, check=True)

def translate_audio(audio_path, translated_wav, from_lang, to_lang):
    voice_map = {
        "fr": "fr-FR-DeniseNeural",
        "de": "de-DE-KatjaNeural",
        "es": "es-ES-ElviraNeural",
        "it": "it-IT-ElsaNeural",
        "pt": "pt-BR-FranciscaNeural",
        "zh-Hans": "zh-CN-XiaoxiaoNeural",
        "ja": "ja-JP-NanamiNeural",
        "ko": "ko-KR-SunHiNeural",
        "ru": "ru-RU-DariyaNeural",
        "hi": "hi-IN-SwaraNeural",
        "ar": "ar-EG-SalmaNeural",
        "tr": "tr-TR-EmelNeural",
        "nl": "nl-NL-ColetteNeural",
        "pl": "pl-PL-AgnieszkaNeural",
        "th": "th-TH-PremNeural",
        "sv": "sv-SE-SofieNeural",
        "cs": "cs-CZ-VlastaNeural",
        "hu": "hu-HU-NoemiNeural"
    }

    translation_config = speechsdk.translation.SpeechTranslationConfig(
        endpoint=CUSTOM_ENDPOINT, subscription=SPEECH_KEY
    )
    translation_config.speech_recognition_language = from_lang
    translation_config.add_target_language(to_lang)
    translation_config.voice_name = voice_map.get(to_lang, "fr-FR-DeniseNeural")

    audio_config = speechsdk.audio.AudioConfig(filename=audio_path)
    recognizer = speechsdk.translation.TranslationRecognizer(
        translation_config=translation_config, audio_config=audio_config
    )

    result = recognizer.recognize_once()
    if result.reason != speechsdk.ResultReason.TranslatedSpeech:
        raise Exception("Translation failed: " + str(result.reason))

    translated_text = result.translations[to_lang]

    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, endpoint=CUSTOM_ENDPOINT)
    speech_config.speech_synthesis_voice_name = translation_config.voice_name
    audio_out = speechsdk.audio.AudioOutputConfig(filename=translated_wav)

    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_out)
    synthesizer.speak_text_async(translated_text).get()

    return translated_text

def stitch_audio_to_video(original_video, translated_audio, output_video):
    command = [
        FFMPEG_PATH, "-y", "-i", original_video,
        "-i", translated_audio, "-c:v", "copy",
        "-map", "0:v:0", "-map", "1:a:0", "-shortest",
        output_video
    ]
    subprocess.run(command, check=True)

def process_video(video, from_lang_label, to_lang_label):
    from_lang = SOURCE_LANGUAGES[from_lang_label]
    to_lang = TARGET_LANGUAGES[to_lang_label]

    uid = str(uuid.uuid4())
    video_path = f"video_{uid}.mp4"
    audio_path = f"audio_{uid}.wav"
    translated_wav = f"translated_{uid}.wav"
    output_path = f"translated_video_{uid}.mp4"

    with open(video_path, "wb") as f:
        f.write(video.read())

    extract_audio(video_path, audio_path)
    translate_audio(audio_path, translated_wav, from_lang, to_lang)
    stitch_audio_to_video(video_path, translated_wav, output_path)

    return output_path

gui = gr.Interface(
    fn=process_video,
    inputs=[
        gr.Video(label="Upload Video"),
        gr.Dropdown(choices=list(SOURCE_LANGUAGES.keys()), label="Source Language", value="English (US)"),
        gr.Dropdown(choices=list(TARGET_LANGUAGES.keys()), label="Target Language", value="French")
    ],
    outputs=gr.Video(label="Translated Video"),
    title="ðŸŽ¬ Video Translator with Azure AI",
    description="Upload a video, select source and target languages, and get back a translated video with new audio."
)

gui.launch()
