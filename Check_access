import os
import argparse
import pandas as pd
import psutil
import sqlite3
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor
from azureml.core import Run

AI_OUTPUT_COLUMNS = [
    "AI_FINAL_CATEGORY", "AI_FINAL_CATEGORY_CONFIDENCE", "AI_MATCHING_REASON_FINAL_CATEGORY",
    "AI_FINAL_SUBCATEGORY", "AI_FINAL_SUBCATEGORY_CONFIDENCE", "AI_MATCHING_REASON_FINAL_SUBCATEGORY"
]

DEDUP_DB = "dedup.db"
SIZE_THRESHOLD_MB = 120
MAX_WORKERS = 14
BATCH_SIZE = 1000

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def setup_db(db_path=DEDUP_DB):
    conn = sqlite3.connect(db_path, timeout=30)
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("CREATE TABLE IF NOT EXISTS processed_keys (composite_key TEXT PRIMARY KEY)")
    return conn

def is_key_seen(conn, key):
    cur = conn.execute("SELECT 1 FROM processed_keys WHERE composite_key = ?", (key,))
    return cur.fetchone() is not None

def insert_new_keys(conn, keys):
    keys = list(set(keys))
    for i in range(0, len(keys), BATCH_SIZE):
        batch = [(k,) for k in keys[i:i+BATCH_SIZE]]
        with conn:
            conn.executemany("INSERT OR IGNORE INTO processed_keys (composite_key) VALUES (?)", batch)

def load_predictions(inference_output_dir):
    all_predictions = []
    for file in os.listdir(inference_output_dir):
        if file.endswith(".parquet"):
            try:
                df = pd.read_parquet(os.path.join(inference_output_dir, file))
                required_cols = ["CMPNT_MATL_NUM"] + AI_OUTPUT_COLUMNS
                if all(col in df.columns for col in required_cols):
                    df = df[required_cols].dropna(subset=["CMPNT_MATL_NUM"])
                    all_predictions.append(df)
            except Exception as e:
                log(f" Failed to load {file}: {e}")
    return pd.concat(all_predictions, ignore_index=True) if all_predictions else None

def process_key_file(file_path, pred_df, final_output_dir):
    try:
        conn = setup_db()
        key_df = pd.read_parquet(file_path)

        if "composite_key" not in key_df.columns or "CMPNT_MATL_NUM" not in key_df.columns:
            log(f" Skipping {file_path}, missing required columns")
            return

        key_df["already_seen"] = key_df["composite_key"].apply(lambda k: is_key_seen(conn, k))
        key_df = key_df[~key_df["already_seen"]].drop(columns="already_seen")

        if key_df.empty:
            log(f" All rows already processed: {os.path.basename(file_path)}")
            return

        merged_df = key_df.merge(pred_df, how="left", on="CMPNT_MATL_NUM")
        new_keys = key_df["composite_key"].dropna().unique().tolist()
        insert_new_keys(conn, new_keys)

        merged_df = merged_df.drop("composite_key", axis=1, errors="ignore")
        output_path = os.path.join(final_output_dir, os.path.basename(file_path))
        merged_df.to_parquet(output_path, index=False)

        log(f" Processed {file_path} â†’ {len(merged_df)} rows")
        log(f" RAM usage: {psutil.virtual_memory().percent}%")
        conn.close()
    except Exception as e:
        log(f" Error in file {file_path}: {e}")

def classify_files_by_size(dir_path, threshold_mb=SIZE_THRESHOLD_MB):
    small, large = [], []
    for file in os.listdir(dir_path):
        if file.endswith(".parquet"):
            full_path = os.path.join(dir_path, file)
            size_mb = os.path.getsize(full_path) / (1024 * 1024)
            (small if size_mb <= threshold_mb else large).append(full_path)
    return small, large

def main(inference_output_dir, key_output_dir, final_output_dir):
    run = Run.get_context()
    os.makedirs(final_output_dir, exist_ok=True)

    log(" Loading prediction files...")
    pred_df = load_predictions(inference_output_dir)
    if pred_df is None:
        log(" No valid prediction files found.")
        return

    log(" Classifying key files by size...")
    small_files, large_files = classify_files_by_size(key_output_dir)

    log(f" {len(small_files)} small files â†’ multiprocessing")
    log(f" {len(large_files)} large files â†’ sequential")

    # Sequential large files
    for file_path in large_files:
        process_key_file(file_path, pred_df, final_output_dir)

    # Parallel small files
    if small_files:
        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for path in small_files:
                executor.submit(process_key_file, path, pred_df, final_output_dir)

    log(" All files processed successfully.")

    # âœ… Concatenate, Deduplicate and Save Final File
    try:
        log(" Concatenating all final output parquet files into one...")

        all_dfs = []
        file_list = []
        raw_total_rows = 0

        for file in os.listdir(final_output_dir):
            if file.endswith(".parquet") and file != "merged_final_output.parquet":
                file_path = os.path.join(final_output_dir, file)
                df = pd.read_parquet(file_path)
                all_dfs.append(df)
                raw_total_rows += len(df)
                file_list.append(file)

        if not all_dfs:
            log(" No output files found to merge.")
            return

        merged_df = pd.concat(all_dfs, ignore_index=True).drop_duplicates()
        final_path = os.path.join(final_output_dir, "merged_final_output.parquet")
        merged_df.to_parquet(final_path, index=False)

        # âœ… Logging merge diagnostics
        log(f"ðŸ“‚ Files merged: {len(file_list)}")
        log(f"ðŸ“Š Total rows before merge: {raw_total_rows}")
        log(f"âœ… Final merged rows (after deduplication): {len(merged_df)}")
        log(f"ðŸ§¹ Duplicate rows removed: {raw_total_rows - len(merged_df)}")

        # Upload to AzureML
        run.upload_file("merged_final_output.parquet", final_path)
        log("Final file uploaded to AzureML run artifacts.")

        # âœ… Delete intermediate parquet files
        for file in file_list:
            os.remove(os.path.join(final_output_dir, file))
        log("Intermediate parquet files deleted after merge.")

    except Exception as e:
        log(f"Error during final merge: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--inference_output_dir", required=True)
    parser.add_argument("--key_output_dir", required=True)
    parser.add_argument("--final_output_dir", required=True)
    args = parser.parse_args()

    main(args.inference_output_dir, args.key_output_dir, args.final_output_dir)
