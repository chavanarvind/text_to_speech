import os
import time

from azureml.core import Run, Dataset, Datastore, Workspace

from cross_hound.config import OUTPUT_COLS
from cross_hound.constants import NON_DUPLICATES_SET, ERP_PART_ID, TRU_SPEC_ID, TRU_SPEC_DESC, UNIFIED_MC_COL
from cross_hound.evaluation import SCORE_COL, create_product_summary, evaluate_version, create_classification_summary


def save_results(datastore_name,
                 erp_discarded,
                 matches,
                 output_dir,
                 save,
                 tru_discarded,
                 run_params,
                 upload_to_ds=True):
    matches = matches.sort_values(by=[UNIFIED_MC_COL,
                                      'SRC_SYS_CD',
                                      'RGN_CD',
                                      'PLNT_CD',
                                      SCORE_COL,
                                      'MATL_NUM',
                                      'CMPNT_MATL_TYPE_CD'],
                                  ascending=[True, True, True, True, False, True, True])

    # TODO: Fill up Region per Material Code
    try:
        matches['RGN_CD'] = matches.groupby(UNIFIED_MC_COL)['RGN_CD'].transform(lambda x: x.fillna(method='ffill')
                                                                                .fillna(method='bfill'))
        matches.loc[matches['PLNT_CD'].str.startswith('External'), 'PLNT_CD'] = '#'
    except Exception as e:
        print(e)
        print(matches[matches['PLNT_CD'].isna()][NON_DUPLICATES_SET].isna())

    matches = matches.drop_duplicates(subset=NON_DUPLICATES_SET)

    try:
        matches_to_save = verify_and_filter_valid_records(matches)
    except Exception as e:
        print('Verification Failed')
        print(e)
        matches_to_save = matches

    df_counts = create_product_summary(matches_to_save)
    pc_summary = create_classification_summary(matches_to_save)

    try:
        evaluate_version(matches_to_save, df_counts, run_params, tru_discarded)
    except Exception as e:
        print('Evaluation failed with error:')
        print(e)
        print('Will continue to save')

    if save:
        curr_time = time.strftime("%m%d%Y", time.localtime())

        mapping_for_output = {f'{ERP_PART_ID}_original': ERP_PART_ID,
                              'FG_NM': 'FG_NM',
                              'FG_SPEC_NM': 'FG_SPEC_NM',
                              'FD_NM': 'FD_SPEC',
                              f'{TRU_SPEC_ID}_original': 'CHILD_NM',
                              TRU_SPEC_DESC: 'CHILD_TITLE_DESC',
                              'TEMPL_TYPE_CD': 'TEMPL_TYPE_CD',
                              UNIFIED_MC_COL: 'MATL_NUM',
                              'is_matched': 'IS_MATCHED',
                              'confidence': 'CONFIDENCE',
                              'Matching Reason': 'MATCHING_REASON',
                              }

        # Drop original MATL_NUM and CMPNT_MATL_NUM column before renaming, otherwise it gets duplicated in output
        matches_to_save.drop(columns=['MATL_NUM', 'CMPNT_MATL_NUM', 'CHILD_NM'], inplace=True)
        matches_to_save = matches_to_save.rename(columns=mapping_for_output)

        # Create placeholders
        matches_to_save['VERIFICATION_STATUS'] = None
        matches_to_save['COMMENTS'] = None
        matches_to_save = matches_to_save[OUTPUT_COLS]

        matches_to_save.to_csv(f'{output_dir}/bom_tru_xref_{curr_time}.csv', index=False)

        matches.drop(columns=['embedding_tru', 'embedding_erp']).to_csv(
            f'{output_dir}/raw_matches_all_columns_{curr_time}.csv', index=False)

        df_counts.to_csv(f'{output_dir}/products_summary_{curr_time}.csv', index=False)
        pc_summary.to_csv(f'{output_dir}/pc_template_prediction_summary_{curr_time}.csv', index=False)

        if upload_to_ds:
            run = Run.get_context()
            if run.identity.startswith('OfflineRun'):  # We are running local.
                subscription_id = '06eaff3d-17f0-43e5-9619-fb73df4372cb'
                resource_group = 'azr-weh-dto-aml-production'
                workspace_name = 'AML-DTO-Marmot-prod'
                ws = Workspace(subscription_id, resource_group, workspace_name)
            else:  # We are running a pipeline inside azureml
                ws = run.experiment.workspace

            upload_dir = os.path.join(output_dir, 'upload_dir')
            os.makedirs(upload_dir, exist_ok=True)

            matches_to_save.to_csv(f'{upload_dir}/bom_tru_xref_{curr_time}.csv', index=False)
            debug_id = "665880"
            debug_child = "PC-0000249"

            print(" CHECKPOINT 5: save_result code")
            print(matches_to_save.query('CMPNT_MATL_NUM==@debug_id and CHILD_NM==@debug_child'))
  
            df_counts.to_csv(f'{upload_dir}/tru_product_summary_{curr_time}.csv', index=False)

            ds = Datastore.get(ws, datastore_name)
            Dataset.File.upload_directory(src_dir=upload_dir,
                                          target=ds,
                                          overwrite=True)
    return matches


def verify_and_filter_valid_records(matches):
    matched = matches.query('is_matched').copy()
    matched['verification_matched'] = (matched[ERP_PART_ID].notna() & matched[TRU_SPEC_ID].notna())
    print('All matches verified:', all(matched['verification_matched']))
    unmatched = matches.query('not is_matched').copy()
    #  Add this debug print to check for downgraded records
    print(" Downgraded records in unmatched validation step")
    print(unmatched.query('CMPNT_MATL_NUM=="665880" and CHILD_NM=="PC-0000249"'))

    unmatched['verification_unmatched'] = (
    (unmatched[ERP_PART_ID].isna() & unmatched[TRU_SPEC_ID].notna()) |
    (unmatched[ERP_PART_ID].notna() & unmatched[TRU_SPEC_ID].isna()) |
    (unmatched['Matching Reason'] == 'Off Spec Non Match'))
    print('All unmatched verified:', all(unmatched['verification_unmatched']))
    print('% correct unmatched :', round(sum(unmatched['verification_unmatched']) / len(unmatched) * 100, 2))
    ids_wrong_results = unmatched.query('not verification_unmatched').index
    matches_to_save = matches[~matches.index.isin(ids_wrong_results)]
    print(f'Saving {(len(matches_to_save) / len(matches)) * 100}% of all results generated.')
    matches_to_save['mat_num'] = matches_to_save['mat_num'].astype(str)
    return matches_to_save
