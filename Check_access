import os
import uuid
import shutil
import subprocess
import time
import gradio as gr
import azure.cognitiveservices.speech as speechsdk
from pydub import AudioSegment
import whisperx

# Azure Speech settings
SPEECH_KEY = ""
CUSTOM_ENDPOINT = "https://cog-speech-dto-epil-dev.cognitiveservices.azure.com/"

# Path to ffmpeg.exe
FFMPEG_PATH = r"C:/Users/AChava05/Downloads/ffmpeg-2025-06-28-git-cfd1f81e7d-essentials_build/ffmpeg-2025-06-28-git-cfd1f81e7d-essentials_build/bin/ffmpeg.exe"

# Supported languages
SOURCE_LANGUAGES = {
    "English (US)": "en",
    "French (France)": "fr",
    "German": "de",
    "Italian": "it",
    "Spanish": "es",
    "Portuguese (Brazil)": "pt",
    "Chinese (Mandarin)": "zh",
    "Japanese": "ja",
    "Korean": "ko",
    "Russian": "ru",
    "Hindi": "hi",
    "Arabic": "ar",
    "Turkish": "tr",
    "Dutch": "nl",
    "Polish": "pl",
    "Thai": "th",
    "Swedish": "sv",
    "Czech": "cs",
    "Hungarian": "hu"
}

TARGET_LANGUAGES = {
    "French": "fr",
    "German": "de",
    "Spanish": "es",
    "Italian": "it",
    "Portuguese": "pt",
    "Chinese": "zh-Hans",
    "Japanese": "ja",
    "Korean": "ko",
    "Russian": "ru",
    "Hindi": "hi",
    "Arabic": "ar",
    "Turkish": "tr",
    "Dutch": "nl",
    "Polish": "pl",
    "Thai": "th",
    "Swedish": "sv",
    "Czech": "cs",
    "Hungarian": "hu"
}

def extract_audio(video_path, audio_path):
    command = [
        FFMPEG_PATH, "-y", "-i", video_path,
        "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path
    ]
    subprocess.run(command, check=True)

def translate_with_speaker_diarization(audio_path, translated_wav_path, from_lang, to_lang):
    model = whisperx.load_model("large-v3", device="cuda" if whisperx.utils.get_device() == "cuda" else "cpu", compute_type="float16")
    audio = whisperx.load_audio(audio_path)
    result = model.transcribe(audio, language=from_lang)

    diarize_model = whisperx.DiarizationPipeline(use_auth_token=True, device=model.device)
    diarize_segments = diarize_model(audio_path)

    segments = whisperx.merge_text_diarization(result["segments"], diarize_segments["segments"])

    speaker_voice_map = {
        "SPEAKER_00": "fr-FR-HenriNeural",
        "SPEAKER_01": "fr-FR-DeniseNeural",
        "SPEAKER_02": "fr-FR-AlainNeural"
    }

    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, endpoint=CUSTOM_ENDPOINT)
    all_audio = []

    for i, seg in enumerate(segments):
        speaker = seg.get("speaker", "SPEAKER_00")
        text = seg["text"]
        print(f"[{speaker}] {text}")

        translation_config = speechsdk.translation.SpeechTranslationConfig(
            subscription=SPEECH_KEY, endpoint=CUSTOM_ENDPOINT, speech_recognition_language=from_lang
        )
        translation_config.add_target_language(to_lang)

        # Translate
        translation_config.voice_name = speaker_voice_map.get(speaker, "fr-FR-DeniseNeural")
        speech_config.speech_synthesis_voice_name = translation_config.voice_name

        synthesizer = speechsdk.SpeechSynthesizer(
            speech_config=speech_config,
            audio_config=speechsdk.audio.AudioOutputConfig(filename=f"temp_tts_{i}.wav")
        )

        # Use Azure Translator Text API if better translation needed
        translator = speechsdk.translation.TranslationRecognizer(
            translation_config=translation_config,
            audio_config=speechsdk.audio.AudioConfig(filename=audio_path)
        )
        result = translator.recognize_once()

        if result.reason == speechsdk.ResultReason.TranslatedSpeech:
            translated_text = result.translations[to_lang]
        else:
            translated_text = text  # fallback

        synthesizer.speak_text_async(translated_text).get()
        audio_segment = AudioSegment.from_file(f"temp_tts_{i}.wav")
        all_audio.append(audio_segment)
        os.remove(f"temp_tts_{i}.wav")

    final_audio = sum(all_audio)
    final_audio.export(translated_wav_path, format="wav")

def stitch_audio_to_video(original_video, translated_audio, output_video):
    command = [
        FFMPEG_PATH, "-y", "-i", original_video,
        "-i", translated_audio, "-c:v", "copy",
        "-map", "0:v:0", "-map", "1:a:0",
        output_video
    ]
    subprocess.run(command, check=True)

def process_video(video_file_obj, from_lang_label, to_lang_label):
    from_lang = SOURCE_LANGUAGES[from_lang_label]
    to_lang = TARGET_LANGUAGES[to_lang_label]

    uid = str(uuid.uuid4())
    video_path = f"video_{uid}.mp4"
    audio_path = f"audio_{uid}.wav"
    translated_wav = f"translated_{uid}.wav"
    output_path = f"translated_video_{uid}.mp4"

    shutil.copy(video_file_obj, video_path)

    extract_audio(video_path, audio_path)
    translate_with_speaker_diarization(audio_path, translated_wav, from_lang, to_lang)
    stitch_audio_to_video(video_path, translated_wav, output_path)

    return output_path

gui = gr.Interface(
    fn=process_video,
    inputs=[
        gr.File(label="Upload Video", file_types=[".mp4"]),
        gr.Dropdown(choices=list(SOURCE_LANGUAGES.keys()), label="Source Language", value="English (US)"),
        gr.Dropdown(choices=list(TARGET_LANGUAGES.keys()), label="Target Language", value="French")
    ],
    outputs=gr.Video(label="Translated Video"),
    title="ðŸŽ¬ Video Translator with Speaker Diarization",
    description="Upload a video, detect speakers, translate, and assign different Azure voices."
)

gui.launch()
