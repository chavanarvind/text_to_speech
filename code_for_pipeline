from azureml.core import Workspace, Experiment, Environment
from azureml.pipeline.core import Pipeline, PipelineData, StepSequence
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.compute import ComputeTarget

# Setup workspace
ws = Workspace.from_config()  # Or use explicit params

compute_target = ComputeTarget(workspace=ws, name="your-compute-cluster")

# Environment setup
env = Environment.from_conda_specification(name="parquet-env", file_path="env.yml")

# Define intermediate output
intermediate_data = PipelineData(name="intermediate_data", datastore=ws.get_default_datastore())

# Define steps
steps = []

steps.append(PythonScriptStep(
    name="1 - Download Data",
    script_name="steps/1_download_data.py",
    compute_target=compute_target,
    arguments=["--output_path", intermediate_data],
    outputs=[intermediate_data],
    source_directory="azureml_pipeline",
    allow_reuse=True,
    runconfig_environment=env,
))

steps.append(PythonScriptStep(
    name="2 - Target Map Join",
    script_name="steps/2_target_map_join.py",
    compute_target=compute_target,
    arguments=["--input_path", intermediate_data],
    inputs=[intermediate_data],
    source_directory="azureml_pipeline",
    allow_reuse=True,
    runconfig_environment=env,
))

# Add remaining steps: 3, 4, 5 in same fashion
# ...
pipeline = Pipeline(workspace=ws, steps=StepSequence(steps))
pipeline.validate()
pipeline.submit("harmonized_bom_processing")
