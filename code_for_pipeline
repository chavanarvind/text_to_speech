import os
import glob
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import gc
import re


# azureml-core of version 1.0.72 or higher is required
from azureml.core import Workspace, Dataset

subscription_id = 
resource_group = 
workspace_name = 

workspace = Workspace(subscription_id, resource_group, workspace_name)

dataset = Dataset.get_by_name(workspace, name='harmonized_bom_data_asset')
# Download dataset locally
download_path = './data/parquet_files'
os.makedirs(download_path, exist_ok=True)
dataset.download(target_path=download_path, overwrite=True)

import os
import glob
import time
import pyarrow.parquet as pq
import polars as pl
from concurrent.futures import ThreadPoolExecutor

# --- Paths ---
input_path = './data/parquet_files'
output_path = './data/target_map_parquet_files'
os.makedirs(output_path, exist_ok=True)

# --- Load mapping ---
target_map_df = pl.read_csv('./data/target_map.csv', columns=['CMPNT_CAT_CD_DESC', 'Final Category']).unique()

# --- Required columns to extract ---
columns_to_read = [
    'MATL_SHRT_DESC',
    'CMPNT_MATL_DESC',
    'CMPNT_MATL_TYPE_CD',
    'CMPNT_CAT_CD_DESC',
    'CMPNT_UOM_CD'
]

# --- File processor ---
def process_file(file_idx, file):
    try:
        print(f" Start: {os.path.basename(file)}")
        start = time.time()

        table = pq.read_table(file, columns=columns_to_read)  # Read full file
        df = pl.from_arrow(table).unique()

        # Left join with target map
        joined = df.join(target_map_df, on='CMPNT_CAT_CD_DESC', how='left')

        # Save compressed output
        out_file = os.path.join(output_path, f'mapped_file{file_idx}.parquet')
        joined.write_parquet(out_file, compression='snappy')

        print(f" Done: {os.path.basename(file)} → {out_file} | ⏱️ {time.time() - start:.2f}s")
    except Exception as e:
        print(f" Error: {os.path.basename(file)} -> {e}")

# --- Run with thread pool ---
all_files = glob.glob(os.path.join(input_path, '*.parquet'))
with ThreadPoolExecutor(max_workers=8) as executor:
    for idx, file in enumerate(all_files):
        executor.submit(process_file, idx, file)

  import os
import re
import glob
import pandas as pd
from multiprocessing import Pool, cpu_count
from functools import partial

# Precompiled regex patterns
patterns = {
    'non_alphanumeric': re.compile(r'[^A-Za-z0-9&% ]+'),
    'percent_space': re.compile(r"\s*%\s*"),
    'canada_variants': re.compile(r'(canada|can|(ca\d+)$|ca)'),
    'remove_canada': re.compile(r'canada\s*(\d{2,})|(canada\d+)|canada|can\s*(\d{2,})|(can\d+)|can|(ca\d+)|ca\s(\d{2,})|ca$|(ca\s)'),
    'units': re.compile(r"(\D)(\d+)(\s*)(ml|l|gr|gm|g|ct)"),
    'spf_space': re.compile(r"(\s)(spf)\s*([\d+])"),
    'units_no_space': re.compile(r'([\d+])\s*(?:ml|l|gr|gm|g|ct)(?: |$)'),
    'spf_number': re.compile(r"(\D)(spf\d+)")
}

def clean_series(series):
    return (series.str.lower()
        .str.replace(patterns['non_alphanumeric'], '', regex=True)
        .str.replace(patterns['percent_space'], '% ', regex=True)
        .str.replace(patterns['canada_variants'], r' \1', regex=True)
        .str.replace(patterns['remove_canada'], '', regex=True)
        .str.replace(patterns['units'], r'\1 \2\3\4 ', regex=True)
        .str.replace(patterns['spf_space'], r'\1\2\3', regex=True)
        .str.replace(patterns['units_no_space'], lambda z: z.group().replace(" ", ""), regex=True)
        .str.replace(patterns['spf_number'], r'\1 \2 ', regex=True)
    )

def text_clean_step(file, input_path, output_path):
    try:
        df = pd.read_parquet(file)

        # Keep only rows with Final Category
        df = df[df['Final Category'].notna()]
        if df.empty:
            print(f"⚠️ Skipped (no valid rows): {os.path.basename(file)}")
            return

        # Clean both columns separately
        df['MATL_SHRT_DESC'] = clean_series(df['MATL_SHRT_DESC'].fillna(''))
        df['CMPNT_MATL_DESC'] = clean_series(df['CMPNT_MATL_DESC'].fillna(''))

        # Combine cleaned text
        df['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'] = (
            df['MATL_SHRT_DESC'] + ' ' + df['CMPNT_MATL_DESC']
        ).str.strip()

        # Save to new path
        rel_path = os.path.relpath(file, input_path)
        output_file = os.path.join(output_path, rel_path)
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df.to_parquet(output_file, index=False)

        print(f"✅ Done: {os.path.basename(file)}")
    except Exception as e:
        print(f"❌ Failed: {os.path.basename(file)} -> {e}")

# Run the cleaning process in parallel
if __name__ == '__main__':
    input_path = './data/target_map_parquet_files'
    output_path = './data/target_map_cleaned_non_null_target'
    all_files = glob.glob(os.path.join(input_path, '*.parquet'))

    worker_func = partial(text_clean_step, input_path=input_path, output_path=output_path)
    with Pool(processes=cpu_count()) as pool:
        pool.map(worker_func, all_files)

  import os
import glob
import pandas as pd
from multiprocessing import Pool, cpu_count

# UNIT_GROUP mapping
unit_group_map = {
    'KG': 'CHM', 'KGS': 'CHM', 'KGA': 'CHM', 'KGW': 'CHM', 'G': 'CHM', 'GR': 'CHM', 'GM': 'CHM', 'MG': 'CHM',
    'LB': 'CHM', 'LBS': 'CHM', 'OZ': 'CHM', 'OZA': 'CHM', 'GW': 'CHM', 'TON': 'CHM', 'DR': 'CHM',
    'L': 'Liquid', 'LT': 'Liquid', 'ML': 'Liquid', 'CC': 'Liquid', 'CL': 'Liquid', 'CCM': 'Liquid', 'GLL': 'Liquid',
    'EA': 'Discrete', 'PC': 'Discrete', 'PCS': 'Discrete', 'Pcs': 'Discrete', 'PKT': 'Discrete', 'PK': 'Discrete',
    'PAK': 'Discrete', 'PCK': 'Discrete', 'CS': 'Discrete', 'CSE': 'Discrete', 'CT': 'Discrete', 'CA': 'Discrete',
    'ST': 'Discrete', 'GRO': 'Discrete', 'BX': 'Discrete',
    'BOT': 'Containers', 'BOTTLE': 'Containers', 'ROLL': 'Containers', 'ROL': 'Containers', 'REEL': 'Containers', 'KAR': 'Containers',
    'FT': 'Dimensional', 'YD': 'Dimensional', 'KM': 'Dimensional', 'DM': 'Dimensional', 'M': 'Dimensional',
    'M1': 'Dimensional', 'M2': 'Dimensional', 'KM2': 'Dimensional', 'YD2': 'Dimensional', 'FT3': 'Dimensional',
    'SQM': 'Dimensional', 'sqm': 'Dimensional', 'MYD': 'Dimensional', 'MI': 'Dimensional', 'SM': 'Dimensional',
    'LM': 'Dimensional', 'LF': 'Dimensional', 'MH': 'Dimensional', 'KN': 'Dimensional', 'CH': 'Dimensional',
    'TH': 'Unclassified', 'THU': 'Unclassified', 'IM': 'Unclassified', 'NOS': 'Unclassified', 'NO': 'Unclassified',
    'TS': 'Unclassified', 'KA': 'Unclassified', 'ZPC': 'Unclassified', 'ZCT': 'Unclassified', '0%': 'Unclassified',
    'KP': 'Unclassified', 'GP': 'Unclassified', 'KAI': 'Unclassified', 'SY': 'Unclassified', 'UN': 'Unclassified',
    'MU': 'Unclassified', 'UM': 'Unclassified', 'HU': 'Unclassified'
}

# Function to process a single file
def process_file(file_path):
    try:
        df = pd.read_parquet(file_path)

        # Filter rows where 'Final Category' is not null
        df = df[df['Final Category'].notna()]
        if df.empty:
            print(f"⚠️ Skipped (no valid rows): {os.path.basename(file_path)}")
            return

        # Compute length of material descriptions
        df['MATL_SHRT_DESC_LEN'] = df['MATL_SHRT_DESC'].astype(str).str.len()
        df['CMPNT_MATL_DESC_LEN'] = df['CMPNT_MATL_DESC'].astype(str).str.len()

        # Map unit to group
        df['UNIT_GROUP'] = (
            df['CMPNT_UOM_CD']
            .fillna('')
            .str.upper()
            .map(unit_group_map)
            .fillna('Unclassified')
        )

        # Save to new folder (target_map_with_rules)
        output_file = file_path.replace("train_test_data", "target_map_with_rules")
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df.to_parquet(output_file, index=False)

        print(f"✅ Done: {os.path.basename(file_path)}")

    except Exception as e:
        print(f"❌ Failed: {os.path.basename(file_path)} -> {e}")

# Main parallel execution
if __name__ == '__main__':
    input_path = './data/target_map_cleaned_non_null_target'
    all_files = glob.glob(os.path.join(input_path, '*.parquet'))

    with Pool(cpu_count()) as pool:
        pool.map(process_file, all_files)


  import os
import glob
import pandas as pd
from multiprocessing import Pool, cpu_count

# --- Smart category mapping function ---
def map_cmpnt_type_category(val):
    if pd.isna(val):
        return 'OTHER'
    
    val_clean = str(val).strip()
    val_lower = val_clean.lower()
    val_upper = val_clean.upper()

    # ERP material type mapping (preserving ERP business meaning)
    erp_type_map = {
        'FERT': 'FINISHED_PRODUCT',
        'HALB': 'SEMI_FINISHED',
        'ROH': 'RAW_MATERIAL',
        'VERP': 'PACKAGING_MATERIAL',
        'TRAD': 'TRADED_GOOD',
        'ERSA': 'SUBCONTRACT_COMPONENT',
        'API': 'API',
        'TPF': 'TRADE_PRODUCT',
        'PACK': 'PACKAGING',
        'ZHBG': 'INTERMEDIATE',
        'EPC': 'EXCIPIENT',
        'EPF': 'FINISHED_PRODUCT',
        'HAWA': 'TRADING_GOOD',
        'ZEXI': 'EXCIPIENT',
        'ZROH': 'RAW_MATERIAL',
        'SAPR': 'PACKAGING',
        'IM': 'INTERMEDIATE',
        'UNBW': 'NON_VALUATED',
        'IG': 'INTERMEDIATE_GOOD'
    }

    # Priority 1: Known ERP codes
    if val_upper in erp_type_map:
        return erp_type_map[val_upper]

    # Priority 2: Descriptive pattern matching
    if any(x in val_lower for x in ['packaging', 'bottle', 'jar', 'cap', 'carton', 'tube', 'pouch', 'closure']):
        return 'PACKAGING'
    if any(x in val_lower for x in ['chemical', 'solvent', 'alcohol', 'acid', 'buffer', 'salt', 'preservative']):
        return 'CHEMICAL'
    if any(x in val_lower for x in ['actives', 'naturals', 'flavor', 'fragrance', 'api']):
        return 'ACTIVES_NATURALS'
    if any(x in val_lower for x in ['film', 'foil', 'label', 'sleeve']):
        return 'FILMS_LABELS'
    if any(x in val_lower for x in ['soap', 'conditioner', 'emulsifier', 'thickener', 'talc', 'sunscreen']):
        return 'COSMETIC_BASE'
    if any(x in val_lower for x in ['glass', 'pump', 'puff']):
        return 'CONTAINERS'

    # Priority 3: Unrecognized short ERP-like code
    if len(val_clean) <= 5 and val_clean.isalnum():
        return 'ERP_CODE'

    # Fallback
    return 'OTHER'

# --- Per-file processing ---
def process_file(file_path):
    try:
        df = pd.read_parquet(file_path)

        # Only process rows where Final Category is not null
        df = df[df['Final Category'].notna()]
        if df.empty:
            print(f"⚠️ Skipped (no valid rows): {os.path.basename(file_path)}")
            return

        # Apply mapping
        df['CMPNT_MATL_TYPE_CATEGORY'] = df['CMPNT_MATL_TYPE_CD'].apply(map_cmpnt_type_category)

        # Overwrite original file with new column
        df.to_parquet(file_path, index=False)
        print(f"✅ Updated: {os.path.basename(file_path)}")

    except Exception as e:
        print(f"❌ Failed: {os.path.basename(file_path)} -> {e}")

# --- Parallel execution ---
if __name__ == '__main__':
    input_path = './data/target_map_cleaned_non_null_target'
    all_files = glob.glob(os.path.join(input_path, '*.parquet'))

    with Pool(cpu_count()) as pool:
        pool.map(process_file, all_files)
