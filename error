import os
import argparse
import pandas as pd
import psutil
from datetime import datetime
from azureml.core import Run

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

REQUIRED_COLUMNS = [
    "CMPNT_MATL_NUM",
    "Final_Prediction",
    "Final Subcategory",
    "Score",
    "Subcategory_Score",
    "inferred_category_model",
    "Subcategory_Model",
    "prediction_flag",
    "prediction_flag_subcategory"
]

RENAMED_COLUMNS = {
    "Final_Prediction": "AI_FINAL_CATEGORY",
    "Final Subcategory": "AI_FINAL_SUBCATEGORY",
    "Score": "AI_FINAL_CATEGORY_CONFIDENCE",
    "Subcategory_Score": "AI_FINAL_SUBCATEGORY_CONFIDENCE",
    "inferred_category_model": "AI_MATCHING_REASON_FINAL_CATEGORY",
    "Subcategory_Model": "AI_MATCHING_REASON_FINAL_SUBCATEGORY"
}

def apply_priority_and_deduplicate(df):
    df['category_priority'] = df['prediction_flag'].map({
        'Mapped_PreExtracted': 2,
        'Mapped': 2,
        'High Confidence': 1,
        'Low Confidence': 0
    }).fillna(0)

    df['subcategory_priority'] = df['prediction_flag_subcategory'].map({
        'Mapped': 2,
        'High Confidence': 1,
        'Low Confidence': 0,
        'Fallback': -1
    }).fillna(0)

    df = df.sort_values(
        by=['CMPNT_MATL_NUM', 'category_priority', 'subcategory_priority'],
        ascending=[True, False, False]
    )

    return df.drop_duplicates(subset="CMPNT_MATL_NUM", keep="first")

def main(inference_output_dir, key_output_dir, final_output_dir):
    run = Run.get_context()
    os.makedirs(final_output_dir, exist_ok=True)
    log(" Starting to build global prediction table...")

    global_df = pd.DataFrame()

    for file in os.listdir(inference_output_dir):
        if not file.endswith(".parquet"):
            continue

        file_path = os.path.join(inference_output_dir, file)
        log(f" Reading inference file: {file}")

        try:
            df = pd.read_parquet(file_path)
            missing_cols = [col for col in REQUIRED_COLUMNS if col not in df.columns]
            if missing_cols:
                log(f" Skipping {file} â€” missing columns: {missing_cols}")
                continue

            df = df[[col for col in REQUIRED_COLUMNS if col in df.columns]]
            global_df = pd.concat([global_df, df], ignore_index=True)
            global_df = apply_priority_and_deduplicate(global_df)

            log(f" Global prediction table updated (rows: {len(global_df)})")

        except Exception as e:
            log(f" Failed to process file: {file}")
            log(f"   Reason: {str(e)}")

    if global_df.empty:
        raise Exception(" No valid prediction files found to build global prediction table!")

    # Save global_df after renaming
    global_df = global_df.rename(columns=RENAMED_COLUMNS)
    global_df_path = os.path.join(final_output_dir, "global_predictions.parquet")
    global_df.to_parquet(global_df_path, index=False)
    log(f" Saved global prediction table to: {global_df_path}")

    # Process each key file individually
    for file in os.listdir(key_output_dir):
        if not file.endswith(".parquet"):
            continue

        try:
            log(f" Processing key file: {file}")
            log(f" Memory usage before load: {psutil.virtual_memory().percent}%")

            key_path = os.path.join(key_output_dir, file)
            key_df = pd.read_parquet(key_path).drop_duplicates()

            # Load global predictions fresh each time to avoid memory bloat
            global_df = pd.read_parquet(global_df_path)

            merged_df = key_df.merge(global_df, how="left", on="CMPNT_MATL_NUM")

            out_path = os.path.join(final_output_dir, file)
            merged_df.to_parquet(out_path, index=False)

            log(f"Merged output saved: {out_path} (rows: {len(merged_df)})")
            log(f" Memory usage after save: {psutil.virtual_memory().percent}%")

        except Exception as e:
            log(f" Failed to process key file: {file}")
            log(f"   Reason: {str(e)}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--inference_output_dir", required=True)
    parser.add_argument("--key_output_dir", required=True)
    parser.add_argument("--final_output_dir", required=True)
    args = parser.parse_args()

    main(args.inference_output_dir, args.key_output_dir, args.final_output_dir)
