from azureml.core import Run, Datastore, Dataset, Workspace
import time

# Step 1: Get current date in DDMMYYYY format
curr_time = time.strftime("%d%m%Y", time.localtime())
output_folder_name = f"hbom_category_prediction_{curr_time}"

# Step 2: Setup upload directory
upload_dir = os.path.join("outputs", "upload_dir")
os.makedirs(upload_dir, exist_ok=True)

# Step 3: Copy all merged parquet files into upload_dir
for key_file in key_files:
    try:
        source_path = os.path.join("outputs", "mapped_keys", key_file)
        target_path = os.path.join(upload_dir, key_file)
        if os.path.exists(source_path):
            import shutil
            shutil.copy2(source_path, target_path)
    except Exception as e:
        log(f"❌ Failed to copy {key_file} to upload_dir: {e}")

# Step 4: Upload to ADLS2 container via registered datastore
try:
    run = Run.get_context()
    if run.identity.startswith('OfflineRun'):
        # If running locally (OPTIONAL: replace with your workspace config)
        ws = Workspace.from_config()
    else:
        ws = run.experiment.workspace

    ds = Datastore.get(ws, "bom-xref")

    Dataset.File.upload_directory(
        src_dir=upload_dir,
        target=ds.path(output_folder_name),
        overwrite=True
    )

    log(f"✅ Uploaded merged files to ADLS2 path: bom-xref/{output_folder_name}/")
except Exception as e:
    log(f"❌ ADLS2 upload failed: {e}")
