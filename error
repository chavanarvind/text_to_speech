# pipeline_runner.py

from azureml.core import Workspace, Environment, Experiment
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data.datapath import DataPath
from azureml.data import OutputFileDatasetConfig
from azureml.core import Dataset
import os

# === Load workspace ===
ws = Workspace.from_config(path=".azureml/dev_config.json")
compute_name = "llm-gpu-cluster-2"
env = Environment.get(workspace=ws, name="Bom_X_Evaluator")

run_config = RunConfiguration()
run_config.target = compute_name
run_config.environment = env

# === Output and input shared folders ===
data_step1_out = PipelineData(name="data_step1_out", is_directory=True)
data_step1a_mapped = PipelineData(name="data_step1a_mapped", is_directory=True)
data_step1a_unmapped = PipelineData(name="data_step1a_unmapped", is_directory=True)
data_step2_out = PipelineData(name="data_step2_out", is_directory=True)
data_step3_out = PipelineData(name="data_step3_out", is_directory=True)
data_step4_out = PipelineData(name="data_step4_out", is_directory=True)
data_step1a_key_output = PipelineData(name="data_step1a_key_output", is_directory=True)

# === Upload high_conf_mapping.csv from local folder to datastore ===
default_ds = ws.get_default_datastore()

# Upload file as before
default_ds.upload_files(
    files=['./local_data/high_conf_direct_mapping.csv'],
    target_path='high_conf_map',   # <== IMPORTANT: Folder name
    overwrite=True,
    show_progress=True
)

# Register or create file dataset
high_conf_dataset = Dataset.File.from_files((default_ds, 'high_conf_map'))
high_conf_mount = high_conf_dataset.as_named_input('high_conf_map').as_mount()

default_ds.upload_files(
    files=['./local_data/abbreviation_expension_updated.csv'],
    target_path='abbrev_map',
    overwrite=True,
    show_progress=True
)
abbrev_dataset = Dataset.File.from_files((default_ds, 'abbrev_map'))
abbrev_mount = abbrev_dataset.as_named_input('abbrev_map').as_mount()

# === Step 1: Data Pull ===
data_pull_step = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", data_step1_out],
    outputs=[data_step1_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 1a: High Confidence Merge ===
high_conf_merge_step = PythonScriptStep(
    name="Step 1a - High Confidence Merge",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    inputs=[data_step1_out, high_conf_mount],
    arguments=[
    "--input_path", data_step1_out,
    "--mapping_csv", high_conf_mount,
    "--mapped_output", data_step1a_mapped,
    "--needs_model_output", data_step1a_unmapped,
    "--key_output", data_step1a_key_output
],
    outputs=[data_step1a_mapped, data_step1a_unmapped, data_step1a_key_output],

    #outputs=[data_step1a_mapped, data_step1a_unmapped],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 2: Clean Text ===
clean_text_step = PythonScriptStep(
    name="Step 2 - Text Cleaning",
    script_name="step_2_clean_text.py",
    source_directory="scripts",
    inputs=[data_step1a_unmapped],
    arguments=["--input_path", data_step1a_unmapped, "--output_path", data_step2_out],
    outputs=[data_step2_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 3: Abbreviation Expansion ===
abbrev_expand_step = PythonScriptStep(
    name="Step 3 - Expand Abbreviations",
    script_name="step_3_expand_abbreviation.py",
    source_directory="scripts",
    inputs=[data_step2_out, abbrev_mount],
    arguments=[
        "--input_path", data_step2_out,
        "--abbrev_map", abbrev_mount,
        "--output_path", data_step3_out   # ✅ ADD THIS
    ],
    outputs=[data_step3_out],            # ✅ INCLUDE output
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 4: Feature Engineering ===
feature_eng_step = PythonScriptStep(
    name="Step 4 - Feature Engineering",
    script_name="step_4_feature_engineering.py",
    source_directory="scripts",
    inputs=[data_step3_out],
    arguments=["--input_path", data_step3_out, "--output_path", data_step4_out],
    outputs=[data_step4_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 5: Run Inference ===
inference_step = PythonScriptStep(
    name="Step 5 - Run Inference",
    script_name="step_5_run_inference.py",
    source_directory="scripts",
    inputs=[data_step4_out, data_step1a_mapped, data_step1a_key_output],
    arguments=[
        "--input_path", data_step4_out,
        "--additional_mapped_dir", data_step1a_mapped,
        "--key_output_dir", data_step1a_key_output
    ],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Build pipeline ===
pipeline = Pipeline(workspace=ws, steps=[
    data_pull_step,
    high_conf_merge_step,
    clean_text_step,
    abbrev_expand_step,
    feature_eng_step,
    inference_step
])

pipeline.validate()

# === Submit pipeline ===
experiment = Experiment(ws, "full_inference_pipeline")
pipeline_run = experiment.submit(pipeline)
pipeline_run.wait_for_completion(show_output=True)



___________________
#step_2_clean_text.py
import os
import re
import glob
import argparse
import pandas as pd
from multiprocessing import Pool, cpu_count

# --- Precompiled regex patterns ---
patterns = {
    'non_alphanumeric': re.compile(r'[^A-Za-z0-9&% ]+'),
    'percent_space': re.compile(r"\s*%\s*"),
    'remove_canada': re.compile(r'canada\s*\d*|can\s*\d*|ca\s*\d*|ca$|can$|ca\s'),
    'units': re.compile(r"(\D)(\d+)(\s*)(ml|l|gr|gm|g|ct)"),
    'spf_space': re.compile(r"(\s)(spf)\s*([\d+])"),
    'units_no_space': re.compile(r'(\d+)\s*(ml|l|gr|gm|g|ct)(?: |$)'),
    'spf_number': re.compile(r"(\D)(spf\d+)")
}

# --- Clean function ---
def clean_series(series):
    return (series.str.lower()
        .str.replace(patterns['non_alphanumeric'], '', regex=True)
        .str.replace(patterns['percent_space'], '% ', regex=True)
        .str.replace(patterns['remove_canada'], '', regex=True)
        .str.replace(patterns['units'], r'\1 \2\3\4 ', regex=True)
        .str.replace(patterns['spf_space'], r'\1\2\3', regex=True)
        .str.replace(patterns['units_no_space'], lambda z: z.group().replace(" ", ""), regex=True)
        .str.replace(patterns['spf_number'], r'\1 \2 ', regex=True))

# --- Per-file processor ---
def process_file(file_info):
    file_path, output_path = file_info
    try:
        df = pd.read_parquet(file_path)
        df['CMPNT_MATL_DESC'] = clean_series(df['CMPNT_MATL_DESC'].fillna(''))

        out_file = os.path.join(output_path, os.path.basename(file_path))
        df.to_parquet(out_file, index=False)
        print(f"✅ Cleaned and saved: {os.path.basename(file_path)}")
    except Exception as e:
        print(f"❌ Failed: {os.path.basename(file_path)} -> {e}")

# --- Main ---
def main(input_path, output_path):
    os.makedirs(output_path, exist_ok=True)
    files = glob.glob(os.path.join(input_path, '*.parquet'))
    file_info_list = [(f, output_path) for f in files]

    with Pool(cpu_count()) as pool:
        pool.map(process_file, file_info_list)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_path', required=True)
    parser.add_argument('--output_path', required=True)
    args = parser.parse_args()

    main(args.input_path, args.output_path)
#--------------------------
#step_3_expand_abbreviation.py
import os
import glob
import re
import argparse
import pandas as pd
from multiprocessing import Pool, cpu_count

# --- Replacement function ---
def replace_match(m):
    return abbrev_map.get(m.group(0).lower(), m.group(0))

# --- Per-file processing ---
def process_file(file):
    try:
        df = pd.read_parquet(file)

        if 'CMPNT_MATL_DESC' not in df.columns:
            print(f"⚠️ Skipped {file}: missing 'CMPNT_MATL_DESC'")
            return

        df['CMPNT_MATL_DESC'] = df['CMPNT_MATL_DESC'].str.replace(
            abbrev_pattern, replace_match, regex=True
        )

        df.to_parquet(file, index=False)
        print(f"✅ Expanded abbreviations in: {file}")

    except Exception as e:
        print(f"❌ Failed to process {file} -> {e}")

# --- Main ---
def main(input_path, abbrev_map_path, output_path):
    global abbrev_map, abbrev_pattern

    # Load abbreviation map
    csv_files = glob.glob(os.path.join(abbrev_map_path, "*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No abbreviation CSV found in: {abbrev_map_path}")
    abbrev_df = pd.read_csv(csv_files[0])
    abbrev_map = {k.lower(): v for k, v in zip(abbrev_df['Abbreviation_list'], abbrev_df['Abbreviation_Expension'])}
    abbrev_pattern = re.compile(r'\b(' + '|'.join(re.escape(k) for k in abbrev_map.keys()) + r')\b', flags=re.IGNORECASE)

    # Process files
    os.makedirs(output_path, exist_ok=True)
    files = glob.glob(os.path.join(input_path, '*.parquet'))
    for file in files:
        try:
            df = pd.read_parquet(file)
            df['CMPNT_MATL_DESC'] = df['CMPNT_MATL_DESC'].str.replace(abbrev_pattern, replace_match, regex=True)

            output_file = os.path.join(output_path, os.path.basename(file))
            df.to_parquet(output_file, index=False)
            print(f"✅ Expanded abbreviations in: {output_file}")
        except Exception as e:
            print(f"❌ Failed: {file} -> {e}")
# --- Entry point ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_path', required=True)
    parser.add_argument('--abbrev_map', required=True)
    parser.add_argument('--output_path',required=True)  # not used for now
    args = parser.parse_args()

    main(args.input_path, args.abbrev_map, args.output_path)
