import os
import argparse
import pandas as pd
from datetime import datetime
from azureml.core import Run

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

REQUIRED_COLUMNS = [
    "CMPNT_MATL_NUM",
    "Final_Prediction",
    "Final Subcategory",
    "Score",
    "Subcategory_Score",
    "inferred_category_model",
    "Subcategory_Model",
    "prediction_flag",
    "prediction_flag_subcategory"
]

RENAMED_COLUMNS = {
    "Final_Prediction": "AI_FINAL_CATEGORY",
    "Final Subcategory": "AI_FINAL_SUBCATEGORY",
    "Score": "AI_FINAL_CATEGORY_CONFIDENCE",
    "Subcategory_Score": "AI_FINAL_SUBCATEGORY_CONFIDENCE",
    "inferred_category_model": "AI_MATCHING_REASON_FINAL_CATEGORY",
    "Subcategory_Model": "AI_MATCHING_REASON_FINAL_SUBCATEGORY"
}

def apply_priority_and_deduplicate(df):
    df['category_priority'] = df['prediction_flag'].map({
        'Mapped_PreExtracted': 2,
        'Mapped': 2,
        'High Confidence': 1,
        'Low Confidence': 0
    }).fillna(0)

    df['subcategory_priority'] = df['prediction_flag_subcategory'].map({
        'Mapped': 2,
        'High Confidence': 1,
        'Low Confidence': 0,
        'Fallback': -1
    }).fillna(0)

    df = df.sort_values(
        by=['CMPNT_MATL_NUM', 'category_priority', 'subcategory_priority'],
        ascending=[True, False, False]
    )

    return df.drop_duplicates(subset="CMPNT_MATL_NUM", keep="first")

def main(inference_output_dir, key_output_dir, final_output_dir):
    run = Run.get_context()

    os.makedirs(final_output_dir, exist_ok=True)
    log("üìÅ Starting to build global prediction table...")

    global_df = pd.DataFrame()

    for file in os.listdir(inference_output_dir):
        if not file.endswith(".parquet"):
            continue

        file_path = os.path.join(inference_output_dir, file)
        log(f"üìÑ Reading: {file}")

        df = pd.read_parquet(file_path)

        # Retain only needed columns for deduplication
        df = df[[col for col in REQUIRED_COLUMNS if col in df.columns]]

        global_df = pd.concat([global_df, df], ignore_index=True)
        global_df = apply_priority_and_deduplicate(global_df)

        log(f"‚úÖ Updated global table (rows: {len(global_df)})")

    # Read and merge with all key files
    all_keys = []
    for file in os.listdir(key_output_dir):
        if file.endswith(".parquet"):
            key_df = pd.read_parquet(os.path.join(key_output_dir, file))
            all_keys.append(key_df.drop_duplicates())

    if not all_keys:
        raise Exception("‚ùå No key files found!")

    key_df = pd.concat(all_keys, ignore_index=True).drop_duplicates()
    final_df = key_df.merge(
        global_df.rename(columns=RENAMED_COLUMNS),
        how="left",
        on="CMPNT_MATL_NUM"
    )

    out_path = os.path.join(final_output_dir, "final_prediction_output.parquet")
    final_df.to_parquet(out_path, index=False)

    log(f"üéâ Final merged output saved: {out_path} (rows: {len(final_df)})")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--inference_output_dir", required=True)
    parser.add_argument("--key_output_dir", required=True)
    parser.add_argument("--final_output_dir", required=True)
    args = parser.parse_args()

    main(args.inference_output_dir, args.key_output_dir, args.final_output_dir)
