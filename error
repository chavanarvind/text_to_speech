import os
import glob
import joblib
import torch
import pandas as pd
import numpy as np
from concurrent.futures import ProcessPoolExecutor, as_completed
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler

# --- DEVICE CHECK (Global GPU Shared) ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"üñ•Ô∏è Using device: {device}")
if device == 'cuda':
    print(f"üöÄ GPU available: {torch.cuda.get_device_name(0)}")

# --- GLOBAL PATHS ---
input_path = './data/target_map_cleaned_non_null_target'
embedding_cache_dir = './data/validation_step/model_artifects/bert_embedding'
sample_cache_dir = './data/validation_step/model_artifects/sampled_rows'
output_dir = './data/validation_step/model_artifects'
os.makedirs(embedding_cache_dir, exist_ok=True)
os.makedirs(sample_cache_dir, exist_ok=True)
os.makedirs(output_dir, exist_ok=True)

required_cols = [
    'CMPNT_MATL_DESC', 'CMPNT_MATL_DESC_LEN',
    'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY',
    'Final Category'
]

# --- CATEGORY-WISE FRAC MAP ---
category_frac_map = {
    'CHM': 0.05,
    'PKG': 0.05,
    'FNW': 0.1,
    'FNW_CHM': 0.3,
    'Liquids and Creams': 0.5,
    'API': 1.0
}

# --- EMBEDDING FUNCTION ---
def process_file(file):
    try:
        encoder = SentenceTransformer(
            'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb',
            device=device
        )

        file_name = os.path.splitext(os.path.basename(file))[0]
        bert_cache_file = os.path.join(embedding_cache_dir, f"{file_name}_bert.npy")
        sample_cache_file = os.path.join(sample_cache_dir, f"{file_name}_sampled.parquet")

        # Load and clean
        df = pd.read_parquet(file, columns=required_cols)
        df.drop_duplicates(subset=required_cols, inplace=True)
        df = df[df['CMPNT_MATL_DESC'].notna()].copy()
        df['Final Category'] = df['Final Category'].replace({'FNW or CHM': 'FNW_CHM'})
        df.drop_duplicates(inplace=True)

        # Stratified sampling per category
        df_sampled = (
            df.groupby('Final Category', group_keys=False)
            .apply(lambda g: g.sample(
                frac=category_frac_map.get(g.name, 0.1),
                random_state=42
            ))
            .reset_index(drop=True)
        )

        if df_sampled.empty:
            return None

        df_sampled.to_parquet(sample_cache_file, index=False)

        if os.path.exists(bert_cache_file):
            desc_emb = np.load(bert_cache_file)
        else:
            desc_texts = df_sampled['CMPNT_MATL_DESC'].astype(str).tolist()
            desc_emb = encoder.encode(
                desc_texts,
                batch_size=128,
                show_progress_bar=False,
                convert_to_numpy=True,
                num_workers=4
            )
            np.save(bert_cache_file, desc_emb)

        if len(desc_emb) != len(df_sampled):
            return None

        df_sampled['__bert_emb'] = list(desc_emb)
        return df_sampled

    except Exception as e:
        print(f"‚ùå Error processing {file}: {e}")
        return None

# --- MAIN PARALLEL EXECUTION ---
all_files = glob.glob(os.path.join(input_path, '*.parquet'))
print(f"üìÅ Found {len(all_files)} parquet files to process.")

output_X = []
with ProcessPoolExecutor(max_workers=6) as executor:
    futures = [executor.submit(process_file, file) for file in all_files]
    for f in as_completed(futures):
        result = f.result()
        if result is not None:
            output_X.append(result)

# --- CONCAT & FINALIZE ---
if not output_X:
    raise RuntimeError("‚ùå No valid data processed.")

df_all = pd.concat(output_X, ignore_index=True)
print(f"\n‚úÖ Total records after merge: {len(df_all)}")

# --- FIT AND SAVE ENCODERS ---
ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
scaler = StandardScaler()
ordinal.fit(df_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
scaler.fit(df_all[['CMPNT_MATL_DESC_LEN']])

joblib.dump(ordinal, os.path.join(output_dir, 'ordinal_encoder.pkl'))
joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))
print("‚úÖ OrdinalEncoder and Scaler saved.")

# --- FINAL FEATURE MATRIX ---
meta_scaled = scaler.transform(df_all[['CMPNT_MATL_DESC_LEN']])
cat_encoded = ordinal.transform(df_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
bert_array = np.vstack(df_all['__bert_emb'].values)

X_full = np.hstack([bert_array, meta_scaled, cat_encoded])
y_full = df_all['Final Category'].values

np.save(os.path.join(output_dir, 'X_full.npy'), X_full)
np.save(os.path.join(output_dir, 'y_full.npy'), y_full)
print("‚úÖ Final BERT+Meta feature matrix saved.")
