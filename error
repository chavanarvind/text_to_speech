import os
import pandas as pd
import pyarrow.parquet as pq
import glob

# Paths
input_path = './data/parquet_files'
output_path = './data/target_map_parquet'
os.makedirs(output_path, exist_ok=True)

# Load mapping once
target_map_df = pd.read_csv('./data/target_map.csv')

# Required columns
columns_to_read = [
    'MATL_SHRT_DESC',
    'CMPNT_MATL_DESC',
    'CMPNT_MATL_TYPE_CD',
    'CMPNT_CAT_CD_DESC',
    'CMPNT_UOM_CD'
]

# Process each Parquet file
for file_idx, file in enumerate(glob.glob(os.path.join(input_path, '*.parquet'))):
    try:
        parquet_file = pq.ParquetFile(file)
        for row_group_idx in range(parquet_file.num_row_groups):
            # Read selected columns from current row group
            table = parquet_file.read_row_group(row_group_idx, columns=columns_to_read)
            df_chunk = table.to_pandas()

            # Drop duplicates
            df_chunk = df_chunk.drop_duplicates()

            # Left join with mapping
            joined_chunk = df_chunk.merge(target_map_df, how='left', on='CMPNT_CAT_CD_DESC')

            # Save to output
            out_file = os.path.join(output_path, f'mapped_file{file_idx}_chunk{row_group_idx}.parquet')
            joined_chunk.to_parquet(out_file, index=False)
            print(f"Saved: {out_file}")

    except Exception as e:
        print(f"Error processing {file}: {e}")
