from azure.identity import DefaultAzureCredential
from azure.storage.filedatalake import DataLakeServiceClient
import os
from datetime import datetime

# === Storage account and path config ===
account_name = "xbomrefadlsg2"
file_system = "<your-container-name>"  # e.g., 'raw', 'ml-data', etc.
target_dir = f"hbom_category_prediction/hbom_category_prediction_{datetime.today().strftime('%d%m%Y')}_updated"
local_folder = "./data/final_merged_predictions_updated"

# === Authenticate using Azure Identity (Default context) ===
credential = DefaultAzureCredential()

# === Connect to ADLS Gen2 ===
service_client = DataLakeServiceClient(
    account_url=f"https://{account_name}.dfs.core.windows.net",
    credential=credential
)

file_system_client = service_client.get_file_system_client(file_system)
directory_client = file_system_client.get_directory_client(target_dir)

# === Upload all .parquet files ===
for filename in os.listdir(local_folder):
    if filename.endswith(".parquet"):
        file_path = os.path.join(local_folder, filename)
        file_client = directory_client.create_file(filename)

        with open(file_path, "rb") as f:
            data = f.read()
            file_client.append_data(data=data, offset=0, length=len(data))
            file_client.flush_data(len(data))

        print(f"âœ… Uploaded: {filename}")

print(f"ðŸŽ‰ All files uploaded to: {target_dir}")
