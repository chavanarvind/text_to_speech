import glob
import os
import pandas as pd
import numpy as np
from sklearn.utils.class_weight import compute_class_weight
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import (
    log_loss, accuracy_score, classification_report,
    top_k_accuracy_score, f1_score
)
from sklearn.model_selection import train_test_split
from scipy.sparse import hstack
import joblib

# Paths
input_path = './data/target_map_cleaned_non_null_target'
all_files = glob.glob(os.path.join(input_path, '*.parquet'))

# Step 1: Dynamic stratified sampling across all files
required_cols = [
    'CMPNT_MATL_DESC', 'CMPNT_MATL_DESC_LEN',
    'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY', 'Final Category'
]

sample_list = []

for f in all_files:
    df = pd.read_parquet(f, columns=required_cols)
    df = df[df['Final Category'].notna() & df['CMPNT_MATL_DESC'].notna()]
    df['Final Category'] = df['Final Category'].replace({'FNW or CHM': 'FNW_CHM'})
    df.drop_duplicates(inplace=True)

    if df.empty:
        continue

    total_rows = len(df)
    num_classes = df['Final Category'].nunique()
    sample_size = max(1000, int(0.05 * total_rows))
    per_class_limit = max(10, sample_size // num_classes)

    df_sampled = df.groupby('Final Category', group_keys=False).apply(
        lambda x: x.sample(min(len(x), per_class_limit), random_state=42)
    )
    sample_list.append(df_sampled)

sample_df = pd.concat(sample_list, ignore_index=True)
print(f"\n‚úÖ Sampled {len(sample_df)} rows from {len(all_files)} files.")

# Train/Val/Test split
train_val_df, test_df = train_test_split(
    sample_df, test_size=0.2, stratify=sample_df['Final Category'], random_state=42
)
train_df, val_df = train_test_split(
    train_val_df, test_size=0.25, stratify=train_val_df['Final Category'], random_state=42
)
classes = np.unique(train_df['Final Category'])

# Fit transformers
tfidf_vec = TfidfVectorizer(ngram_range=(1, 2), max_features=2**14)
tfidf_vec.fit(train_df['CMPNT_MATL_DESC'].astype(str))

ordinal_enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
ordinal_enc.fit(train_df[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])

scaler = StandardScaler()
scaler.fit(train_df[['CMPNT_MATL_DESC_LEN']])

# Class weights
class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df['Final Category'])
class_weight_dict = dict(zip(classes, class_weights))

# Transform function
def transform(df):
    return hstack([
        tfidf_vec.transform(df['CMPNT_MATL_DESC'].astype(str)),
        scaler.transform(df[['CMPNT_MATL_DESC_LEN']]),
        ordinal_enc.transform(df[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
    ])

X_val_trans = transform(val_df)
y_val = val_df['Final Category']

X_test_trans = transform(test_df)
y_test = test_df['Final Category']

# Initialize model
clf = SGDClassifier(
    loss='log_loss',
    max_iter=1,
    warm_start=True,
    random_state=42,
    class_weight=class_weight_dict,
    tol=None
)

# Training loop
best_loss = float('inf')
no_improve_count = 0
patience = 2
n_epochs = 10

for epoch in range(1, n_epochs + 1):
    print(f"\nüîÅ Epoch {epoch}")
    for file in all_files:
        df = pd.read_parquet(file, columns=required_cols)
        df = df[df['Final Category'].notna() & df['CMPNT_MATL_DESC'].notna()]
        df['Final Category'] = df['Final Category'].replace({'FNW or CHM': 'FNW_CHM'})
        df.drop_duplicates(inplace=True)

        if df.empty:
            continue

        X_batch = df[['CMPNT_MATL_DESC', 'CMPNT_MATL_DESC_LEN', 'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']]
        y_batch = df['Final Category']

        X_batch_trans = transform(X_batch)
        clf.partial_fit(X_batch_trans, y_batch, classes=classes)

    # Validation
    y_val_proba = clf.predict_proba(X_val_trans)
    val_loss = log_loss(y_val, y_val_proba, labels=classes)
    val_acc = accuracy_score(y_val, clf.predict(X_val_trans))
    val_top3 = top_k_accuracy_score(y_val, y_val_proba, k=3, labels=classes)

    print(f"üìò Val Loss = {val_loss:.4f} | Top-1 Acc = {val_acc:.4f} | Top-3 Acc = {val_top3:.4f}")

    if val_loss < best_loss - 1e-4:
        best_loss = val_loss
        best_model = clf
        no_improve_count = 0
    else:
        no_improve_count += 1
        if no_improve_count >= patience:
            print(f"üõë Early stopping at epoch {epoch}")
            break

# Final test evaluation
y_test_pred = best_model.predict(X_test_trans)
y_test_proba = best_model.predict_proba(X_test_trans)

print("\n‚úÖ Final Test Set Evaluation:")
print(f"Test Loss        = {log_loss(y_test, y_test_proba, labels=classes):.4f}")
print(f"Top-1 Accuracy   = {accuracy_score(y_test, y_test_pred):.4f}")
print(f"Top-3 Accuracy   = {top_k_accuracy_score(y_test, y_test_proba, k=3, labels=classes):.4f}")
print(f"F1 Macro         = {f1_score(y_test, y_test_pred, average='macro'):.4f}")
print(f"F1 Micro         = {f1_score(y_test, y_test_pred, average='micro'):.4f}")
print(f"F1 Weighted      = {f1_score(y_test, y_test_pred, average='weighted'):.4f}")

print("\nüìä Classification Report (Test Set):")
print(classification_report(y_test, y_test_pred, digits=3))

# Save model and transformers
os.makedirs("./saved_model", exist_ok=True)
joblib.dump(best_model, "./saved_model/best_sgd_model.joblib")
joblib.dump(tfidf_vec, "./saved_model/tfidf_vectorizer.joblib")
joblib.dump(ordinal_enc, "./saved_model/ordinal_encoder.joblib")
joblib.dump(scaler, "./saved_model/standard_scaler.joblib")

print("\nüíæ Saved best model and transformers to ./saved_model/")
