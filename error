# pipeline_runner_updated.py

from azureml.core import Workspace, Environment, Experiment
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data.datapath import DataPath
from azureml.data import OutputFileDatasetConfig
from azureml.core import Dataset
from azureml.core.authentication import ServicePrincipalAuthentication
import os
import sys
from datetime import datetime

# === Load workspace ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"

def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        tenant_id = args[1]
        client_id = args[2]
        client_secret = args[3]
        sp_auth = ServicePrincipalAuthentication(
            tenant_id=tenant_id,
            service_principal_id=client_id,
            service_principal_password=client_secret
        )
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME,
            auth=sp_auth
        )
    else:
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME
        )

# Initialize workspace
ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
compute_name = "llm-gpu-cluster-2"
env = Environment.get(workspace=ws, name="Bom_X_Evaluator")
default_ds = ws.datastores["xbomrefadlsg2"]

run_config = RunConfiguration()
run_config.target = compute_name
run_config.environment = env

today = datetime.today().strftime("%d%m%Y")

# Output folders

# Intermediate outputs
step1a_key_output_temp = OutputFileDatasetConfig(name='step1a_key_output_temp')

# === Step 5 Output: Raw inference per file ===
final_output_to_adls = OutputFileDatasetConfig(  # <- used in step 5
    name='final_predictions_to_adls',
    destination=(default_ds, f"hbom_category_prediction_inference_per_file/hbom_category_prediction_{today}/")
)

# === Step 6 Output: Final merged and deduplicated predictions ===
final_merged_output = OutputFileDatasetConfig(  # <- used in step 6
    name='final_merged_predictions',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)
# === Upload high_conf_mapping.csv from local folder to datastore ===
default_ds1 = ws.get_default_datastore()

# Upload file as before
default_ds1.upload_files(
    files=['./local_data/high_conf_direct_mapping.csv'],
    target_path='high_conf_map',   # <== IMPORTANT: Folder name
    overwrite=True,
    show_progress=True
)

# Register or create file dataset
high_conf_dataset = Dataset.File.from_files((default_ds1, 'high_conf_map'))
high_conf_mount = high_conf_dataset.as_named_input('high_conf_map').as_mount()

default_ds1.upload_files(
    files=['./local_data/abbreviation_expension_updated.csv'],
    target_path='abbrev_map',
    overwrite=True,
    show_progress=True
)
abbrev_dataset = Dataset.File.from_files((default_ds1, 'abbrev_map'))
abbrev_mount = abbrev_dataset.as_named_input('abbrev_map').as_mount()


# === Step 1 - Pull raw BOM dataset ===
step1 = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", step1_out],
    outputs=[step1_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 1a - Apply direct mapping & generate final output ===
step1a = PythonScriptStep(
    name="Step 1a - High Confidence Merge (Single Output)",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    inputs=[step1_out, high_conf_mount],
    arguments=[
        "--input_path", step1_out,
        "--mapping_csv", high_conf_mount,
        "--final_output", step1a_final_output,
        "--key_output", step1a_key_output_temp
    ],
    outputs=[step1a_final_output, step1a_key_output_temp],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

# === Step 2 - Preprocessing: abbreviation, cleaning, features ===
step2 = PythonScriptStep(
    name="Step 2 - Preprocessing",
    script_name="step2_pre_process.py",
    source_directory="scripts",
    inputs=[step1a_final_output, abbreviation_mount],
    arguments=[
        "--input_path", step1a_final_output,
        "--abbrev_map", abbreviation_mount,
        "--output_path", step2_final_output
    ],
    outputs=[step2_final_output],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

# === Build and Run Pipeline ===
pipeline = Pipeline(workspace=ws, steps=[step1, step1a, step2])
pipeline.validate()

experiment = Experiment(ws, "bom_pipeline_pull_merge_preprocess")
run = experiment.submit(pipeline)
run.wait_for_completion(show_output=True)
