# scripts/step_1a_extract_and_merge.py

import os
import argparse
import pandas as pd
import glob
from azureml.core import Run

# === Parse arguments ===
parser = argparse.ArgumentParser()
parser.add_argument("--input_path", type=str, required=True)
parser.add_argument("--mapping_csv", type=str, required=True)
parser.add_argument("--mapped_output", type=str, required=True)
parser.add_argument("--unmapped_output", type=str, required=True)
args = parser.parse_args()

# === Setup ===
run = Run.get_context()
input_path = args.input_path
mapping_csv = args.mapping_csv
mapped_output = args.mapped_output
unmapped_output = args.unmapped_output

os.makedirs(mapped_output, exist_ok=True)
os.makedirs(unmapped_output, exist_ok=True)

# === Load Mapping CSV ===
print("üîπ Loading high confidence mapping...")
mapping_df = pd.read_csv(mapping_csv)
print(f"‚úÖ Loaded {len(mapping_df)} mapping rows.")

# === Process Each File Individually ===
all_files = glob.glob(os.path.join(input_path, "*.parquet"))
print(f"üîπ Found {len(all_files)} parquet files to process.")

mapped_total, unmapped_total = 0, 0

for file_path in all_files:
    fname = os.path.basename(file_path)
    print(f"\n‚û°Ô∏è Processing {fname}...")

    try:
        df = pd.read_parquet(file_path)
        original_count = len(df)

        # Filter out rows where CMPNT_CAT_CD_DESC is null
        df = df[df['CMPNT_CAT_CD_DESC'].notna()]
        filtered_count = len(df)

        # Deduplicate based on component
        df = df.drop_duplicates(subset=['CMPNT_MATL_NUM'])
        deduped_count = len(df)

        # Merge with high confidence mapping
        merged = df.merge(
            mapping_df,
            left_on='CMPNT_CAT_CD_DESC',
            right_on='Direct Mapping',
            how='left'
        )

        # Add matching metadata
        merged['Matching Reason'] = merged['Final Category'].notna().map(lambda x: 'Direct Map' if x else None)
        merged['Confidence Score'] = merged['Final Category'].notna().map(lambda x: 1.0 if x else None)

        mapped = merged[merged['Final Category'].notna()].copy()
        unmapped = merged[merged['Final Category'].isna()].copy()

        # Save mapped/unmapped separately
        if not mapped.empty:
            mapped.to_parquet(os.path.join(mapped_output, f"mapped_{fname}"), index=False)
            mapped_total += len(mapped)

        if not unmapped.empty:
            unmapped.to_parquet(os.path.join(unmapped_output, f"unmapped_{fname}"), index=False)
            unmapped_total += len(unmapped)

        print(f"  üßπ Original: {original_count}, Filtered: {filtered_count}, Deduped: {deduped_count}")
        print(f"  ‚úÖ Mapped: {len(mapped)}, ‚ùå Unmapped: {len(unmapped)}")

    except Exception as e:
        print(f"  ‚ö†Ô∏è Failed to process {fname}: {e}")

print("\n‚úÖ Done processing all files.")
print(f"Total mapped records: {mapped_total}")
print(f"Total unmapped records: {unmapped_total}")
