c1
from multiprocessing import Pool, cpu_count
abbrev_df = pd.read_csv('./data/abbreviation_expension_updated.csv')
abbrev_map = {k.lower(): v for k, v in zip(abbrev_df['Abbreviation_list'], abbrev_df['Abbreviation_Expension'])}
abbrev_pattern = re.compile(r'\b(' + '|'.join(re.escape(k) for k in abbrev_map.keys()) + r')\b', flags=re.IGNORECASE)


c2

def expand_abbreviations(text):
    if text is None: return text
    return abbrev_pattern.sub(lambda m: abbrev_map[m.group(0).lower()], text)

def clean_text(text):
    if text is None: return text
    text = re.sub(r'[^a-z0-9\s]', ' ', text.lower())
    return re.sub(r'\s+', ' ', text).strip()

def get_unit_group(unit):
    if unit is None: return None
    unit = str(unit).strip().upper()
    chem = {'KG', 'KGS', 'KGA', 'KGW', 'G', 'GR', 'GM', 'MG', 'LB', 'LBS', 'OZ', 'OZA', 'GW', 'TON', 'DR'}
    liquid = {'L', 'LT', 'ML', 'CC', 'CL', 'CCM', 'GLL'}
    discrete = {'EA', 'PC', 'PCS', 'PKT', 'PK', 'PAK', 'PCK', 'CS', 'CSE', 'CT', 'CA', 'ST', 'GRO', 'BX'}
    containers = {'BOT', 'BOTTLE', 'ROLL', 'ROL', 'REEL', 'KAR'}
    dimensional = {'FT', 'YD', 'KM', 'DM', 'M', 'M1', 'M2', 'KM2', 'YD2', 'FT3', 'SQM', 'MYD', 'MI', 'SM', 'LM', 'LF', 'MH', 'KN', 'CH'}
    return (
        'CHM' if unit in chem else
        'Liquid' if unit in liquid else
        'Discrete' if unit in discrete else
        'Containers' if unit in containers else
        'Dimensional' if unit in dimensional else
        'Unclassified'
    )

c3

required_cols = [
    'CMPNT_CAT_CD_DESC',
    'CMPNT_MATL_DESC',
    'CMPNT_MATL_TYPE_CD',
    'CMPNT_UOM_CD',
    'Final Category',
    'MATL_SHRT_DESC'
]

def process_one_file(file):
    try:
        print(f"ðŸ”„ Processing: {os.path.basename(file)}")
        start = time.time()

        all_cols = pl.read_parquet(file).columns
        df = pl.read_parquet(file, columns=[col for col in required_cols if col in all_cols])

        for col in required_cols:
            if col not in df.columns:
                df = df.with_columns(pl.lit(None).alias(col))

        # Phase 1: Clean MATL_SHRT_DESC
        df = df.with_columns([
            pl.col("MATL_SHRT_DESC").map_elements(lambda x: clean_text(expand_abbreviations(x)) if x else None, return_dtype=pl.Utf8)
        ])

        # Phase 2: Clean CMPNT_MATL_DESC and combine
        df = df.with_columns([
            pl.col("CMPNT_MATL_DESC").map_elements(lambda x: clean_text(expand_abbreviations(x)) if x else None, return_dtype=pl.Utf8),
            (pl.col("MATL_SHRT_DESC").fill_null('') + pl.lit(" ") + pl.col("CMPNT_MATL_DESC").fill_null(''))
                .str.strip_chars()
                .alias("MATL_SHRT_DESC_AND_CMPNT_MATL_DESC")
        ])

        # Phase 3: Clean category and group
        df = df.with_columns([
            pl.col("CMPNT_CAT_CD_DESC").map_elements(lambda x: clean_text(x.lower()) if x else None, return_dtype=pl.Utf8),
            pl.col("CMPNT_UOM_CD").map_elements(lambda x: get_unit_group(x), return_dtype=pl.Utf8)
        ])

        df.write_parquet(file)
        print(f"âœ… Done: {os.path.basename(file)} | â±ï¸ {time.time() - start:.2f}s")

    except Exception as e:
        print(f"âŒ Failed: {os.path.basename(file)} -> {e}")

c5

input_path = './data/target_map_parquet_files'
files = glob.glob(os.path.join(input_path, '*.parquet'))

# Use all available cores or limit manually (e.g., processes=4)
with Pool(processes=cpu_count()) as pool:
    pool.map(process_one_file, files)
