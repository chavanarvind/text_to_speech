import os
import argparse
import glob
import traceback
import pandas as pd
import time
import sqlite3
from concurrent.futures import ProcessPoolExecutor

# === Constants ===
DIRECT_MAP_KEY_1 = "Direct Mapping"
DIRECT_MAP_KEY_2 = "Direct Mapping"
JOIN_COL_1 = "CMPNT_CAT_CD_DESC"
JOIN_COL_2 = "CMPNT_MATL_TYPE_CD"
COMP_DB = "seen_components.db"
SIZE_THRESHOLD_MB = 100  # Increased for higher parallel throughput
MAX_WORKERS = 8           # Scales with 16-core machine
BATCH_SIZE = 500          # Increased for faster dedup insertion

# === SQLite dedup helpers ===
def setup_component_db():
    conn = sqlite3.connect(COMP_DB, timeout=30)
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute("CREATE TABLE IF NOT EXISTS seen (component_id TEXT PRIMARY KEY)")
    return conn

def insert_new_components(conn, component_ids, batch_size=BATCH_SIZE):
    unique_ids = list(set(component_ids))
    for i in range(0, len(unique_ids), batch_size):
        batch = [(cid,) for cid in unique_ids[i:i+batch_size]]
        with conn:
            conn.executemany("INSERT OR IGNORE INTO seen (component_id) VALUES (?)", batch)

def filter_new_components(conn, comp_ids):
    return [cid for cid in comp_ids if not is_component_seen(conn, cid)]

def is_component_seen(conn, cid):
    cur = conn.execute("SELECT 1 FROM seen WHERE component_id = ?", (cid,))
    return cur.fetchone() is not None

# === Core processing ===
def process_single_file(file_path, mapping_df, final_output_dir, key_output_dir):
    start_time = time.time()
    try:
        print(f"\n🔄 Processing: {file_path}")
        conn = setup_component_db()
        df = pd.read_parquet(file_path)
        basename = os.path.basename(file_path)

        mapping_cols = ["CMPNT_MATL_NUM", "CMPNT_MATL_DESC", "CMPNT_MATL_TYPE_CD", "CMPNT_CAT_CD_DESC", "CMPNT_UOM_CD"]
        key_cols = ["SRC_SYS_CD", "CMPNT_MATL_NUM"]

        df_key = df[key_cols].dropna().drop_duplicates()
        df_key["SRC_SYS_CD"] = df_key["SRC_SYS_CD"].astype(str).str.strip()
        df_key["composite_key"] = df_key["SRC_SYS_CD"] + "_" + df_key["CMPNT_MATL_NUM"].astype(str).str.strip()

        if not df_key.empty:
            key_output_file = os.path.join(key_output_dir, f"key_{basename}")
            df_key.drop_duplicates(subset=["composite_key"]).to_parquet(key_output_file, index=False)

        df_mapping = df[mapping_cols].dropna(subset=["CMPNT_MATL_DESC"]).drop_duplicates(subset=["CMPNT_MATL_NUM"])
        new_ids = filter_new_components(conn, df_mapping["CMPNT_MATL_NUM"].astype(str).tolist())
        df_mapping = df_mapping[df_mapping["CMPNT_MATL_NUM"].astype(str).isin(new_ids)]
        insert_new_components(conn, new_ids)

        if not df_mapping.empty:
            df_mapping = ensure_ai_columns(df_mapping)
            df_mapping = apply_existing_ai_overrides(df_mapping)

            cat_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Category')
            cat_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Category')
            df_mapping = map_values(df_mapping, cat_map_1, cat_map_2, JOIN_COL_1, JOIN_COL_2,
                                    DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Category', 'Mapped_File_Category')

            sub_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Subcategory')
            sub_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory')
            df_mapping = map_values(df_mapping, sub_map_1, sub_map_2, JOIN_COL_1, JOIN_COL_2,
                                    DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory', 'Mapped_File_Subcategory')

            df_mapping = add_flags(df_mapping)
            finalize_output(df_mapping, os.path.join(final_output_dir, f"mapped_{basename}"))

        print(f"[✅] Done: {basename} in {time.time() - start_time:.2f}s ➤ New components: {len(new_ids)}")
        conn.close()

    except Exception as e:
        print(f"[❌ ERROR] {file_path}: {e}")
        traceback.print_exc()

# === Main ===
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", required=True)
    parser.add_argument("--mapping_csv", required=True)
    parser.add_argument("--key_output", required=True)
    parser.add_argument("--final_output", required=True)
    args = parser.parse_args()

    parquet_files = glob.glob(os.path.join(args.input_path, "*.parquet"))
    csv_files = glob.glob(os.path.join(args.mapping_csv, "*.csv"))
    if not parquet_files:
        raise FileNotFoundError("No parquet files found")
    if not csv_files:
        raise FileNotFoundError("No mapping CSV file found")

    mapping_df = pd.read_csv(csv_files[0])

    os.makedirs(args.final_output, exist_ok=True)
    os.makedirs(args.key_output, exist_ok=True)

    def is_large(file):
        return os.path.getsize(file) / (1024 * 1024) > SIZE_THRESHOLD_MB

    small_files = [f for f in parquet_files if not is_large(f)]
    large_files = [f for f in parquet_files if is_large(f)]

    print(f"🔹 {len(small_files)} small files → multiprocessing")
    print(f"🔸 {len(large_files)} large files → sequential")

    for file in large_files:
        process_single_file(file, mapping_df, args.final_output, args.key_output)

    if small_files:
        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for file in small_files:
                executor.submit(process_single_file, file, mapping_df, args.final_output, args.key_output)

    print("\n✅ All files processed with SQLite deduplication and hybrid multiprocessing.")
