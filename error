# === Updated process_single_file with manager.dict() and locking ===
def process_single_file(args):
    file_path, mapping_df, final_output_dir, key_output_dir, seen_keys, seen_components, lock = args
    start_time = time.time()
    try:
        df = pd.read_parquet(file_path)
        basename = os.path.basename(file_path)
        print(f"[üîÑ] Started processing: {basename}")

        mapping_cols = ["CMPNT_MATL_NUM", "CMPNT_MATL_DESC", "CMPNT_MATL_TYPE_CD", "CMPNT_CAT_CD_DESC", "CMPNT_UOM_CD"]
        key_cols = mapping_cols + ["LOGL_KEY_COMB_COL_VAL"]

        df_key = df[key_cols].dropna(subset=["LOGL_KEY_COMB_COL_VAL"]).drop_duplicates(subset=["LOGL_KEY_COMB_COL_VAL"])
        new_key_rows = []
        with lock:
            for key in df_key["LOGL_KEY_COMB_COL_VAL"]:
                if key not in seen_keys:
                    seen_keys[key] = True
                    new_key_rows.append(key)
        df_key = df_key[df_key["LOGL_KEY_COMB_COL_VAL"].isin(new_key_rows)]

        if not df_key.empty:
            key_output_file = os.path.join(key_output_dir, f"key_{basename}")
            df_key.to_parquet(key_output_file, index=False)

        df_mapping = df[mapping_cols].dropna(subset=["CMPNT_MATL_DESC"]).drop_duplicates(subset=["CMPNT_MATL_NUM"])
        new_ids = []
        with lock:
            for comp_id in df_mapping["CMPNT_MATL_NUM"]:
                if comp_id not in seen_components:
                    seen_components[comp_id] = True
                    new_ids.append(comp_id)
        df_mapping = df_mapping[df_mapping["CMPNT_MATL_NUM"].isin(new_ids)]

        if not df_mapping.empty:
            df_mapping = ensure_ai_columns(df_mapping)
            df_mapping = apply_existing_ai_overrides(df_mapping)

            cat_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Category')
            cat_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Category')
            df_mapping = map_values(df_mapping, cat_map_1, cat_map_2, JOIN_COL_1, JOIN_COL_2,
                                    DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Category', 'Mapped_File_Category')

            sub_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Subcategory')
            sub_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory')
            df_mapping = map_values(df_mapping, sub_map_1, sub_map_2, JOIN_COL_1, JOIN_COL_2,
                                    DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory', 'Mapped_File_Subcategory')

            df_mapping = add_flags(df_mapping)
            mapped_output_file = os.path.join(final_output_dir, f"mapped_{basename}")
            finalize_output(df_mapping, mapped_output_file)

                duration = time.time() - start_time
        print(f"[‚úÖ] Finished processing: {basename} in {duration:.2f} seconds")
        print(f"     ‚û§ New unique keys added: {len(new_key_rows)}")
        print(f"     ‚û§ New unique components added: {len(new_ids)}")

    except Exception as e:
        print(f"[‚ùå ERROR] File: {file_path}\n{e}")
        traceback.print_exc()

# === Updated main function to use manager.dict() and Lock ===
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True)
    parser.add_argument("--mapping_csv", type=str, required=True)
    parser.add_argument("--key_output", type=str, required=True)
    parser.add_argument("--final_output", type=str, required=True)
    args = parser.parse_args()

    parquet_files = glob.glob(os.path.join(args.input_path, "*.parquet"))
    if not parquet_files:
        raise FileNotFoundError(f"No parquet files found in {args.input_path}")

    csv_files = glob.glob(os.path.join(args.mapping_csv, "*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No CSV mapping file found in {args.mapping_csv}")
    mapping_df = pd.read_csv(csv_files[0])

    os.makedirs(args.final_output, exist_ok=True)
    os.makedirs(args.key_output, exist_ok=True)

    with Manager() as manager:
        seen_keys = manager.dict()
        seen_components = manager.dict()
        lock = manager.Lock()

        args_list = [
            (f, mapping_df, args.final_output, args.key_output, seen_keys, seen_components, lock)
            for f in parquet_files
        ]

        with Pool(cpu_count() - 1) as pool:
            pool.map(process_single_file, args_list)

    print("\n‚úÖ All files processed with strict deduplication.")

if __name__ == "__main__":
    main()
