import os
import glob
import numpy as np
import pandas as pd
import torch
import joblib
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler

# --- Paths ---
input_path = './data/target_map_cleaned_non_null_target'
embedding_dir = './data/validation_step/model_artifects/bert_embedding_pkg_only'
sample_dir = './data/validation_step/model_artifects/sampled_rows_pkg_only'
output_dir = './data/validation_step/model_artifects/model_artifects_pkg_only'
os.makedirs(embedding_dir, exist_ok=True)
os.makedirs(sample_dir, exist_ok=True)
os.makedirs(output_dir, exist_ok=True)

# --- BERT Encoder ---
encoder = SentenceTransformer(
    'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb',
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# --- Sampling Fraction Map (Fixed per PKG Final Category) ---
category_frac_map = {
    "PKG - Labels": 0.03,
    "PKG - Corrugated": 0.03,
    "PKG - Cartons": 0.1,
    "PKG - Labels or PKG - Leaflets": 0.1,
    "PKG - Other Packaging": 0.1,
    "PKG - Films (Polybags)": 0.1,
    "PKG - Tubes": 0.1,
    "PKG - Caps/Closures/Lids": 0.1,
    "PKG - Bottles (PET)": 0.1,
    "PKG - Films (Other Plastic)": 0.1,
    "PKG - Bottles (HDPE)": 0.1,
    "PKG - Other Rigid Packaging (Glass packaging)": 0.1,
    "PKG - Bottles (PP)": 0.1,
    "PKG - Promotional Materials": 0.3,
    "PKG - Cosmetics": 0.3,
    "PKG - Pumps": 0.3,
    "PKG - Leaflets": 0.3,
    "PKG - Foils": 0.3,
    "PKG - Thermoformed (All other)": 0.3,
    "PKG - Jars": 0.3,
    "PKG - Injection Molded Components": 0.3,
    "PKG - Sleeves": 0.3,
    "PKG - Films (Paper)": 0.3,
    "PKG - Inserts": 1.0,
    "PKG - Metals (Tins and Cans)": 1.0,
    "PKG - Films (PVC)": 1.0,
    "PKG - Bottles (PET) or PKG - Bottles (HDPE) or PKG - Bottles (PP)": 1.0,
    "PKG - Films (Laminates)": 1.0,
    "PKG - Metals (Tins and Cans) or PKG - Pumps": 1.0,
    "PKG - Films (Laminates) or PKG - Tubes": 1.0,
}

# --- Required Columns ---
required_cols = [
    'CMPNT_MATL_DESC', 'CMPNT_MATL_DESC_LEN',
    'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY',
    'Final Category'
]

# --- Initialize Output Lists ---
files = sorted(glob.glob(os.path.join(input_path, '*.parquet')))
output_X = []
output_y = []
meta_features = []

# --- File Loop ---
for file in files:
    file_name = os.path.splitext(os.path.basename(file))[0]
    print(f"Processing {file_name}")

    try:
        df = pd.read_parquet(file, columns=required_cols)
        df = df[df['CMPNT_MATL_DESC'].notna()]
        df = df.drop_duplicates(subset=required_cols)

        # Keep only PKG Final Categories
        df = df[
            df['Final Category'].notna() &
            df['Final Category'].str.startswith('PKG')
        ]
        print("PKG Categories Found:", df['Final Category'].unique())

        if df.empty:
            print(f" Skipping {file_name}: no PKG data")
            continue

        # Sampling
        sampled = (
            df.groupby('Final Category', group_keys=False)
              .apply(lambda g: g.sample(frac=category_frac_map.get(g.name, 0.1), random_state=42))
              .reset_index(drop=True)
        )

        if sampled.empty:
            print(f" Skipping {file_name}: no valid sampled rows")
            continue

        # BERT Encoding
        desc_emb = encoder.encode(
            sampled['CMPNT_MATL_DESC'].astype(str).tolist(),
            batch_size=256,
            show_progress_bar=True,
            convert_to_numpy=True,
            num_workers=4
        )

        if len(desc_emb) != len(sampled):
            print(f" Skipped {file_name}: embedding/sample size mismatch")
            continue

        # Save sampled + embeddings
        sampled.to_parquet(os.path.join(sample_dir, f'{file_name}_sampled.parquet'), index=False)
        np.save(os.path.join(embedding_dir, f'{file_name}_bert.npy'), desc_emb)

        # Accumulate
        output_X.append(desc_emb)
        output_y.append(sampled['Final Category'].astype(str).values)
        meta_features.append(sampled[['CMPNT_MATL_DESC_LEN', 'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])

        print(f"Processed: {file_name} ({len(sampled)} rows)")

    except Exception as e:
        print(f"Error processing {file_name}: {e}")

# --- Final Check ---
if not output_X:
    raise RuntimeError("No valid embeddings or samples generated.")

# --- Stack Arrays ---
bert_array = np.vstack(output_X)
y_full = np.concatenate(output_y)
meta_all = pd.concat(meta_features, ignore_index=True)

# --- Encode Meta Features ---
ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
scaler = StandardScaler()

ordinal.fit(meta_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
scaler.fit(meta_all[['CMPNT_MATL_DESC_LEN']])

meta_scaled = scaler.transform(meta_all[['CMPNT_MATL_DESC_LEN']])
cat_encoded = ordinal.transform(meta_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])

# --- Final X Matrix ---
X_full = np.hstack([bert_array, meta_scaled, cat_encoded])

# --- Save Outputs ---
np.save(os.path.join(output_dir, 'X_full.npy'), X_full)
np.save(os.path.join(output_dir, 'y_full.npy'), y_full)
joblib.dump(ordinal, os.path.join(output_dir, 'ordinal_encoder.pkl'))
joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))

print(f"\n All files saved:")
print(f"   X_full.npy shape = {X_full.shape}")
print(f"   y_full.npy shape = {y_full.shape}, dtype = {y_full.dtype}")
