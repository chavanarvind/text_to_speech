import argparse
import os
import glob
import uuid
import pandas as pd
from azureml.core import Run, Dataset

# === Define constants ===
DIRECT_MAP_KEY_1 = "Direct Mapping"
DIRECT_MAP_KEY_2 = "Direct Mapping"
JOIN_COL_1 = "CMPNT_CAT_CD_DESC"
JOIN_COL_2 = "CMPNT_MATL_TYPE_CD"
KEY_COLS = ["SRC_SYS_CD", "MATL_NUM", "PLNT_CD", "CMPNT_MATL_NUM", "LOGL_KEY_COMB_COL_VAL"]

# [keep all function definitions as-is above this line ‚Äî unchanged]

def save_key_file(df, file_path, key_output_dir, seen_keys):
    missing = [col for col in KEY_COLS if col not in df.columns]
    if missing:
        print(f"[SKIPPED] {os.path.basename(file_path)} ‚Äî missing columns: {missing}")
        return seen_keys, 0

    key_df = df[KEY_COLS].drop_duplicates()
    key_df = key_df[~key_df["LOGL_KEY_COMB_COL_VAL"].isin(seen_keys)]
    if key_df.empty:
        print(f"[SKIPPED] All LOGL_KEY_COMB_COL_VAL already seen in key file processing: {os.path.basename(file_path)}")
        return seen_keys, 0

    seen_keys.update(key_df["LOGL_KEY_COMB_COL_VAL"].unique())

    output_file = os.path.join(key_output_dir, f"key_file_{os.path.basename(file_path)}")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    key_df.to_parquet(output_file, index=False)
    print(f"[‚úÖ] Saved key file: {output_file}")
    return seen_keys, len(key_df)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True)
    parser.add_argument("--mapping_csv", type=str, required=True)
    parser.add_argument("--final_output", type=str, required=True)
    parser.add_argument("--key_output", type=str, required=True)
    parser.add_argument("--skip_register", action="store_true")
    parser.add_argument("--process_only_one", action="store_true")
    args = parser.parse_args()

    run = Run.get_context()

    parquet_files = glob.glob(os.path.join(args.input_path, "*.parquet"))
    if not parquet_files:
        raise FileNotFoundError(f"No parquet files found in {args.input_path}")

    csv_files = glob.glob(os.path.join(args.mapping_csv, "*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No CSV files found in {args.mapping_csv}")

    mapping_df = pd.read_csv(csv_files[0])

    seen_components = set()
    seen_keys = set()
    summary = []

    for idx, file_path in enumerate(parquet_files):
        print(f"\nüîÑ [Processing file]: {os.path.basename(file_path)}")
        try:
            full_df = pd.read_parquet(file_path)

            # Split views for mapping and key logic
            df = full_df.copy()
            df = df[df['CMPNT_MATL_DESC'].notna()].drop_duplicates(subset=["CMPNT_MATL_NUM"])
            df = df[~df["CMPNT_MATL_NUM"].isin(seen_components)]
            if df.empty:
                print("[SKIPPED] All CMPNT_MATL_NUM already processed in previous files.")
                continue
            seen_components.update(df["CMPNT_MATL_NUM"].unique())

            df = ensure_ai_columns(df)
            df = apply_existing_ai_overrides(df)

            cat_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Category')
            cat_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Category')
            df = map_values(df, cat_map_1, cat_map_2, JOIN_COL_1, JOIN_COL_2,
                            DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Category', 'Mapped_File_Category')

            sub_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Subcategory')
            sub_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory')
            df = map_values(df, sub_map_1, sub_map_2, JOIN_COL_1, JOIN_COL_2,
                            DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory', 'Mapped_File_Subcategory')

            df = add_flags(df)

            output_file = os.path.join(args.final_output, f"mapped_{os.path.basename(file_path)}")
            finalize_output(df, output_file)

            seen_keys, num_keys_saved = save_key_file(full_df, file_path, args.key_output, seen_keys)

            summary.append({
                "file": os.path.basename(file_path),
                "mapped_rows": len(df),
                "new_keys_saved": num_keys_saved
            })

            if args.process_only_one:
                break

        except Exception as e:
            print(f"[‚ùå ERROR] Failed to process {file_path}: {e}")
            summary.append({
                "file": os.path.basename(file_path),
                "mapped_rows": 0,
                "new_keys_saved": 0,
                "error": str(e)
            })

    # Save summary log to Azure ML outputs folder
    summary_df = pd.DataFrame(summary)
    os.makedirs("outputs", exist_ok=True)
    summary_df.to_csv("outputs/summary_log.csv", index=False)
    print("\nüìÑ Summary written to outputs/summary_log.csv")
    print("\n‚úÖ All files processed.")


if __name__ == "__main__":
    main()
