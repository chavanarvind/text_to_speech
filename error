from azureml.core import Workspace, Dataset
from azureml.core.datastore import Datastore
import pandas as pd
import os

# === Azure ML Workspace Config ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"
DATASTORE_NAME = "xbomrefadlsg2"

from datetime import datetime
today = datetime.today().strftime("%d%m%Y")

# === Load Workspace ===
ws = Workspace(subscription_id=SUBSCRIPTION_ID,
               resource_group=RESOURCE_GROUP,
               workspace_name=WORKSPACE_NAME)

# === Get Datastore & Dataset ===
ds = Datastore.get(ws, DATASTORE_NAME)
dataset = Dataset.get_by_name(ws, "final_merged_predictions")

# === Local temp paths ===
input_path = "./data/final_merged_predictions"
output_path = "./data/final_merged_predictions_updated"
os.makedirs(input_path, exist_ok=True)
os.makedirs(output_path, exist_ok=True)

# === Step 1: Download original files ===
dataset.download(target_path=input_path, overwrite=True)

# === Step 2: Update and save to output_path ===
for file in os.listdir(input_path):
    if file.endswith(".parquet"):
        input_file = os.path.join(input_path, file)
        output_file = os.path.join(output_path, file)

        df = pd.read_parquet(input_file)

        mask = (
            df["COMP_MAT_NUM"].notna() &
            (df["FINAL_CATEGORY"].str.upper() != "OTHER") &
            df["MATCHING_REASON"].isna()
        )
        df.loc[mask, "MATCHING_REASON"] = "direct_mapping"
        df.loc[mask, "SCORE"] = 1

        df.to_parquet(output_file, index=False)
        print(f"âœ… Updated: {file}")

# === Step 3: Upload to different folder on same datastore ===
target_blob_path = f"hbom_category_prediction/hbom_category_prediction_{today}_updated/"
ds.upload(
    src_dir=output_path,
    target_path=target_blob_path,
    overwrite=True,
    show_progress=True
)

print(f"ðŸŽ‰ All updated files uploaded to: {target_blob_path}")
