from azure.storage.filedatalake import DataLakeServiceClient
import os

# === Configs ===
account_name = "<your_account_name>"                 # e.g., 'xbomrefadlsg2'
account_key = "<your_account_key>"                   # get from Azure portal
file_system = "<your_container_name>"                # e.g., 'raw' or 'processed'
target_dir = f"hbom_category_prediction/hbom_category_prediction_{datetime.today().strftime('%d%m%Y')}_updated"
local_folder = "./data/final_merged_predictions_updated"

# === Connect to Gen2 DataLake ===
service_client = DataLakeServiceClient(account_url=f"https://{account_name}.dfs.core.windows.net", credential=account_key)
file_system_client = service_client.get_file_system_client(file_system)
directory_client = file_system_client.get_directory_client(target_dir)

# === Upload all parquet files ===
for filename in os.listdir(local_folder):
    if filename.endswith(".parquet"):
        file_path = os.path.join(local_folder, filename)
        file_client = directory_client.create_file(filename)

        with open(file_path, "rb") as f:
            file_client.append_data(data=f, offset=0, length=os.path.getsize(file_path))
            file_client.flush_data(os.path.getsize(file_path))

        print(f"âœ… Uploaded: {filename}")
