# pipeline_bom_prediction.py
# ---------------------------
# Azure ML pipeline to run: data pull → key file generation → final merge

from azureml.core import Workspace, Environment, Experiment, Dataset, Datastore
from azureml.pipeline.core import Pipeline
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data import OutputFileDatasetConfig
from datetime import datetime
import sys

# === Config ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"
COMPUTE_NAME = "rpmdataprocess"
ENV_NAME = "Bom_X_Evaluator"

# === Dates ===
today = datetime.today().strftime("%d%m%Y")

# === Workspace Auth ===
def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        from azureml.core.authentication import ServicePrincipalAuthentication
        sp_auth = ServicePrincipalAuthentication(
            tenant_id=args[1],
            service_principal_id=args[2],
            service_principal_password=args[3]
        )
        return Workspace(subscription_id=SUBSCRIPTION_ID,
                         resource_group=RESOURCE_GROUP,
                         workspace_name=WORKSPACE_NAME,
                         auth=sp_auth)
    else:
        return Workspace(subscription_id=SUBSCRIPTION_ID,
                         resource_group=RESOURCE_GROUP,
                         workspace_name=WORKSPACE_NAME)

ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
compute_target = COMPUTE_NAME
env = Environment.get(workspace=ws, name=ENV_NAME)
datastore = ws.datastores["xbomrefadlsg2"]

# === Run Config ===
run_config = RunConfiguration()
run_config.target = compute_target
run_config.environment = env

# === Step 1 Output: Harmonized BOM pull ===
hbom_raw_output = OutputFileDatasetConfig(
    name="hbom_raw_data",
    destination=(datastore, f"bom_pipeline_output/harmonized_bom_data_{today}/")
)

step1 = PythonScriptStep(
    name="Step 1 - Pull Harmonized BOM Data",
    script_name="script_1_download_hbom_data.py",
    source_directory="scripts",
    arguments=["--output_path", hbom_raw_output],
    outputs=[hbom_raw_output],
    compute_target=compute_target,
    runconfig=run_config,
    allow_reuse=False
)

# === Step 2 Output: Key File Generation ===
key_output = OutputFileDatasetConfig(
    name="hbom_key_files",
    destination=(datastore, f"bom_pipeline_output/hbom_key_files_{today}/")
)

step2 = PythonScriptStep(
    name="Step 2 - Generate Key Files",
    script_name="script_2_generate_key_files_only.py",
    source_directory="scripts",
    arguments=[
        "--input_path", hbom_raw_output,
        "--key_output", key_output
    ],
    inputs=[hbom_raw_output],
    outputs=[key_output],
    compute_target=compute_target,
    runconfig=run_config,
    allow_reuse=False
)

# === Step 3 Input: raw inference (external) ===
inference_output = Dataset.get_by_name(ws, "raw_predictions").as_named_input("inference_output_dir").as_mount()

# === Step 3 Output: Final Merged Output ===
final_output = OutputFileDatasetConfig(
    name="final_merged_predictions",
    destination=(datastore, f"bom_pipeline_output/final_predictions_{today}/")
)

step3 = PythonScriptStep(
    name="Step 3 - Finalize Output (Priority + Dedup)",
    script_name="step_6_finalize_output_global_debug.py",
    source_directory="scripts",
    arguments=[
        "--inference_output_dir", inference_output,
        "--key_output_dir", key_output,
        "--final_output_dir", final_output
    ],
    inputs=[inference_output, key_output],
    outputs=[final_output],
    compute_target=compute_target,
    runconfig=run_config,
    allow_reuse=False
)

# === Assemble Pipeline ===
pipeline = Pipeline(workspace=ws, steps=[step1, step2, step3])
pipeline.validate()

# === Submit Pipeline Run ===
experiment = Experiment(ws, "bom_category_pipeline")
run = experiment.submit(pipeline)
run.wait_for_completion(show_output=True)
