import os
import re
import time
import polars as pl
from concurrent.futures import ThreadPoolExecutor
import glob

# --- Your cleaning logic ---
def clean_text_pipeline(text):
    if not isinstance(text, str):
        return text
    try:
        text = text.lower()
        text = re.sub('[^A-Za-z0-9&% ]+', '', text).strip()
        text = re.sub(r"\s*%\s*", "% ", text)
        text = re.sub(r'(canada|can|(ca\d+)$|ca)', r' \1', text)
        text = re.sub('canada\s*(\d{2,})|(canada\d+)|canada|can\s*(\d{2,})|(can\d+)|can|(ca\d+)|ca\s(\d{2,})|ca$|(ca\s)', '', text).strip()
        text = re.sub(r"(\D)(\d+)(\s*)(ml|l|gr|gm|g|ct)", r"\1 \2\3\4 ", text).strip()
        text = re.sub("(\s)(spf)\s*([\d+])", r"\1\2\3", text).strip()
        text = re.sub('([\d+])\s*(?:ml|l|gr|gm|g|ct)(?: |$)', lambda z: z.group().replace(" ", ""), text).strip()
        text = re.sub(r"(\D)(spf\d+)", r'\1 \2 ', text).strip()
        return text
    except:
        return text

# --- Per-file processing with map_elements ---
def process_file(file):
    try:
        print(f" Processing: {os.path.basename(file)}")
        start = time.time()

        df = pl.read_parquet(file)

        df = df.with_columns([
            pl.col("MATL_SHRT_DESC").map_elements(clean_text_pipeline, return_dtype=pl.Utf8),
            pl.col("CMPNT_MATL_DESC").map_elements(clean_text_pipeline, return_dtype=pl.Utf8)
        ])

        df = df.with_columns([
            (pl.col("MATL_SHRT_DESC").fill_null('') + pl.lit(" ") + pl.col("CMPNT_MATL_DESC").fill_null(''))
            .str.strip_chars()
            .alias("MATL_SHRT_DESC_AND_CMPNT_MATL_DESC")
        ])

        df.write_parquet(file)

        print(f"Done: {os.path.basename(file)} |  {time.time() - start:.2f}s")
    except Exception as e:
        print(f"Failed: {os.path.basename(file)} -> {e}")

# --- Threaded execution ---
def run_multithreaded_parquet_cleaning(folder_path, max_workers=6):
    files = glob.glob(os.path.join(folder_path, "*.parquet"))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        executor.map(process_file, files)

# --- Run it ---
run_multithreaded_parquet_cleaning('./data/target_map_parquet_files', max_workers=4)
