import os
import argparse
import pandas as pd
import numpy as np
import torch
import joblib
import gc
from datetime import datetime
from sentence_transformers import SentenceTransformer
from azureml.core import Run, Model

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def encode_descriptions(encoder, texts, batch_size=64):
    return encoder.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)

def round_datetime_columns(df):
    for col in df.select_dtypes(include=["datetime64[ns]"]).columns:
        df[col] = df[col].dt.round("us")
    return df

def predict_category(df, encoder, model, scaler, ordinal):
    category_idx = df['needs_category_model'] == True
    df_cat = df[category_idx].copy()

    if df_cat.empty:
        return df

    descs = df_cat['CMPNT_MATL_DESC_CLEAN'].astype(str).tolist()
    embeddings = encode_descriptions(encoder, descs)

    X = np.hstack([
        embeddings,
        scaler.transform(df_cat[['CMPNT_MATL_DESC_LEN']]),
        ordinal.transform(df_cat[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
    ])

    probs = model.predict_proba(X)
    preds = model.predict(X)
    scores = np.max(probs, axis=1)

    df.loc[category_idx, 'AI_FINAL_CATEGORY'] = np.where(scores < 0.6, 'Other', preds)
    df.loc[category_idx, 'AI_FINAL_CATEGORY_CONFIDENCE'] = scores
    df.loc[category_idx, 'AI_MATCHING_REASON_FINAL_CATEGORY'] = np.where(
        scores < 0.6, 'Low Confidence', 'lightgbm_Bert_RPM_Category_model'
    )
    log(f"ðŸ“Š Category predictions done: {len(df_cat)}")

    return df

def predict_subcategory(df, encoder, subcategory_models):
    for cat, model_bundle in subcategory_models.items():
        subcat_idx = (df['AI_FINAL_CATEGORY'] == cat) & (df['needs_subcategory_model'] == True)
        df_sub = df[subcat_idx].copy()

        if df_sub.empty:
            continue

        descs = df_sub['CMPNT_MATL_DESC_CLEAN'].astype(str).tolist()
        embeddings = encode_descriptions(encoder, descs)

        X = np.hstack([
            embeddings,
            model_bundle['scaler'].transform(df_sub[['CMPNT_MATL_DESC_LEN']]),
            model_bundle['encoder'].transform(df_sub[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
        ])

        probs = model_bundle['model'].predict_proba(X)
        preds = model_bundle['model'].predict(X)
        scores = np.max(probs, axis=1)

        df.loc[subcat_idx, 'AI_FINAL_SUBCATEGORY'] = np.where(scores < 0.6, 'Other', preds)
        df.loc[subcat_idx, 'AI_FINAL_SUBCATEGORY_CONFIDENCE'] = scores
        df.loc[subcat_idx, 'AI_MATCHING_REASON_FINAL_SUBCATEGORY'] = np.where(
            scores < 0.6, 'Low Confidence', f"RPM_Category_model_full_{cat}_V1"
        )
        log(f"ðŸ“Š Subcategory predictions for {cat}: {len(df_sub)}")

    return df

def apply_fallback(df):
    fallback_idx = (df['needs_subcategory_model'] == True) & (~df['AI_FINAL_CATEGORY'].isin(['CHM', 'PKG', 'FNW']))
    df.loc[fallback_idx, 'AI_FINAL_SUBCATEGORY'] = df.loc[fallback_idx, 'AI_FINAL_CATEGORY']
    df.loc[fallback_idx, 'AI_FINAL_SUBCATEGORY_CONFIDENCE'] = 0
    df.loc[fallback_idx, 'AI_MATCHING_REASON_FINAL_SUBCATEGORY'] = 'Fallback'
    log(f"ðŸ›‘ Fallbacks applied to {fallback_idx.sum()} rows")
    return df

def main(input_path, final_output_dir, skip_if_exists=False, save_csv_summary=False):
    run = Run.get_context()
    ws = run.experiment.workspace

    os.makedirs(final_output_dir, exist_ok=True)
    log_dir = os.path.join(final_output_dir, "log")
    os.makedirs(log_dir, exist_ok=True)

    log("ðŸ” Loading category model and encoders...")
    model_dir = Model.get_model_path("RPM_Category_model_full_cat", _workspace=ws)
    model = joblib.load(os.path.join(model_dir, "final_model.joblib"))
    ordinal = joblib.load(os.path.join(model_dir, "ordinal_encoder.pkl"))
    scaler = joblib.load(os.path.join(model_dir, "scaler.pkl"))

    encoder = SentenceTransformer(
        'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb',
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    encoder.max_seq_length = 128

    log("ðŸ“¦ Loading subcategory models...")
    subcategory_models = {}
    for cat in ['CHM', 'PKG', 'FNW']:
        model_path = Model.get_model_path(f"RPM_Category_model_full_{cat}_V1", _workspace=ws)
        subcategory_models[cat] = {
            "model": joblib.load(os.path.join(model_path, "final_model.joblib")),
            "encoder": joblib.load(os.path.join(model_path, "ordinal_encoder.pkl")),
            "scaler": joblib.load(os.path.join(model_path, "scaler.pkl")),
        }

    for fname in os.listdir(input_path):
        if not fname.endswith(".parquet"):
            continue

        output_path = os.path.join(final_output_dir, f"predicted_{fname}")
        if skip_if_exists and os.path.exists(output_path):
            log(f"â­ï¸ Skipping already processed file: {fname}")
            continue

        try:
            log(f"ðŸ”Ž Processing: {fname}")
            df = pd.read_parquet(os.path.join(input_path, fname))
            df = round_datetime_columns(df)

            df = predict_category(df, encoder, model, scaler, ordinal)
            df = predict_subcategory(df, encoder, subcategory_models)
            df = apply_fallback(df)

            df.to_parquet(output_path, index=False)
            log(f"ðŸ’¾ Saved: {output_path} (rows: {len(df)})")

            if save_csv_summary:
                csv_path = os.path.join(log_dir, f"summary_{fname.replace('.parquet', '.csv')}")
                df.to_csv(csv_path, index=False)
                log(f"ðŸ“ Summary CSV saved: {csv_path}")

        except Exception as e:
            log(f"âŒ Failed on {fname}: {e}")
        finally:
            del df
            gc.collect()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", required=True)
    parser.add_argument("--final_output_dir", required=True)
    parser.add_argument("--skip_if_exists", action="store_true", help="Skip files already predicted")
    parser.add_argument("--save_csv_summary", action="store_true", help="Also write CSV logs per file")
    args = parser.parse_args()

    main(
        input_path=args.input_path,
        final_output_dir=args.final_output_dir,
        skip_if_exists=args.skip_if_exists,
        save_csv_summary=args.save_csv_summary
    )
