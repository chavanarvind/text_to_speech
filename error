from azureml.core import Workspace, Experiment, Environment, Dataset, Datastore
from azureml.core.runconfig import RunConfiguration
from azureml.data import OutputFileDatasetConfig
from azureml.pipeline.core import Pipeline
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.authentication import ServicePrincipalAuthentication
from datetime import datetime
import sys

# === Workspace configuration ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"
COMPUTE_NAME = "llm-gpu-cluster-2"
ENV_NAME = "Bom_X_Evaluator"

# === Get workspace ===
def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        tenant_id = args[1]
        client_id = args[2]
        client_secret = args[3]
        sp_auth = ServicePrincipalAuthentication(
            tenant_id=tenant_id,
            service_principal_id=client_id,
            service_principal_password=client_secret
        )
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME,
            auth=sp_auth
        )
    else:
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME
        )

# === Initialize workspace and configs ===
ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
env = Environment.get(workspace=ws, name=ENV_NAME)

run_config = RunConfiguration()
run_config.target = COMPUTE_NAME
run_config.environment = env

default_ds = ws.get_default_datastore()

# === Pipeline Outputs ===
step1_out = OutputFileDatasetConfig(name="data_step1_out")
step1a_final_output = OutputFileDatasetConfig(name="step1a_final_output")
step1a_key_output_temp = OutputFileDatasetConfig(name="step1a_key_output_temp")
step2_final_output = OutputFileDatasetConfig(name="step2_final_output")

# === Upload Mapping CSV ===
default_ds.upload_files(
    files=['./local_data/high_conf_direct_mapping.csv'],
    target_path='high_conf_map',
    overwrite=True,
    show_progress=True
)
high_conf_dataset = Dataset.File.from_files((default_ds, 'high_conf_map'))
high_conf_mount = high_conf_dataset.as_named_input('high_conf_map').as_mount()

# === Step 1 - Pull raw BOM dataset ===
step1 = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", step1_out],
    outputs=[step1_out],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

# === Step 1a - Apply direct mapping & generate final output ===
step1a = PythonScriptStep(
    name="Step 1a - High Confidence Merge (Single Output)",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    inputs=[step1_out, high_conf_mount],
    arguments=[
        "--input_path", step1_out,
        "--mapping_csv", high_conf_mount,
        "--final_output", step1a_final_output,
        "--key_output", step1a_key_output_temp
    ],
    outputs=[step1a_final_output, step1a_key_output_temp],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

# === Step 2 - Preprocessing: abbreviation, cleaning, features ===
step2 = PythonScriptStep(
    name="Step 2 - Preprocessing",
    script_name="step2_pre_process.py",
    source_directory="scripts",
    inputs=[step1a_final_output, high_conf_mount],
    arguments=[
        "--input_path", step1a_final_output,
        "--abbrev_map", high_conf_mount,
        "--output_path", step2_final_output
    ],
    outputs=[step2_final_output],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

# === Build and Run Pipeline ===
pipeline = Pipeline(workspace=ws, steps=[step1, step1a, step2])
pipeline.validate()

experiment = Experiment(ws, "bom_pipeline_pull_merge_preprocess")
run = experiment.submit(pipeline)
run.wait_for_completion(show_output=True)
