import argparse
import os
import glob
import traceback
import pandas as pd
from azureml.core import Run, Dataset

# === Constants ===
DIRECT_MAP_KEY_1 = "Direct Mapping"
DIRECT_MAP_KEY_2 = "Direct Mapping"
JOIN_COL_1 = "CMPNT_CAT_CD_DESC"
JOIN_COL_2 = "CMPNT_MATL_TYPE_CD"
KEY_COLS = ["CMPNT_MATL_NUM", "LOGL_KEY_COMB_COL_VAL"]

def ensure_ai_columns(df):
    expected_cols = [
        'AI_FINAL_CATEGORY', 'AI_FINAL_CATEGORY_CONFIDENCE', 'AI_MATCHING_REASON_FINAL_CATEGORY',
        'AI_FINAL_SUBCATEGORY', 'AI_FINAL_SUBCATEGORY_CONFIDENCE', 'AI_MATCHING_REASON_FINAL_SUBCATEG'
    ]
    for col in expected_cols:
        if col not in df.columns:
            df[col] = None
    return df

def apply_existing_ai_overrides(df):
    df['skip_category_mapping'] = (
        df['AI_FINAL_CATEGORY'].notna() & df['AI_FINAL_CATEGORY'].astype(str).str.strip().ne('') &
        df['AI_FINAL_CATEGORY_CONFIDENCE'].notna() & df['AI_MATCHING_REASON_FINAL_CATEGORY'].notna()
    )
    df['skip_subcategory_mapping'] = (
        df['AI_FINAL_SUBCATEGORY'].notna() & df['AI_FINAL_SUBCATEGORY'].astype(str).str.strip().ne('') &
        df['AI_FINAL_SUBCATEGORY_CONFIDENCE'].notna() & df['AI_MATCHING_REASON_FINAL_SUBCATEG'].notna()
    )
    df.loc[df['skip_category_mapping'], 'Mapped_File_Category'] = df['AI_FINAL_CATEGORY']
    df.loc[df['skip_subcategory_mapping'], 'Mapped_File_Subcategory'] = df['AI_FINAL_SUBCATEGORY']
    return df

def clean_mapping_df(mapping_df, key_col, value_col):
    mapping_df.columns = mapping_df.columns.str.strip()
    mapping_df[key_col] = mapping_df[key_col].astype(str).str.upper().str.strip()
    mapping_df[value_col] = mapping_df[value_col].astype(str).str.strip()
    df_map = mapping_df[[key_col, value_col]].drop_duplicates()
    return df_map[df_map[value_col].notna() & df_map[value_col].ne("")]

def map_values(df, map_df1, map_df2, join_col1, join_col2, key1, key2, value_col, output_col):
    df[join_col1] = df[join_col1].astype(str).str.upper().str.strip()
    df[join_col2] = df[join_col2].astype(str).str.upper().str.strip()
    map_dict1 = dict(zip(map_df1[key1], map_df1[value_col]))
    map_dict2 = dict(zip(map_df2[key2], map_df2[value_col]))
    df[output_col] = df[join_col1].map(map_dict1)
    missing = df[output_col].isna() | df[output_col].astype(str).str.strip().eq("")
    df.loc[missing, output_col] = df.loc[missing, join_col2].map(map_dict2)
    df[output_col] = df[output_col].replace(["nan", "NaN"], pd.NA)
    return df

def add_flags(df):
    df['Mapped_File_Category Filled'] = df['Mapped_File_Category'].notna() & df['Mapped_File_Category'].astype(str).str.strip().ne("")
    df['Mapped_File_Subcategory Filled'] = df['Mapped_File_Subcategory'].notna() & df['Mapped_File_Subcategory'].astype(str).str.strip().ne("")

    df['category_matching_reason'] = df.apply(
        lambda row: row['AI_MATCHING_REASON_FINAL_CATEGORY'] if pd.notna(row['AI_MATCHING_REASON_FINAL_CATEGORY']) else (
            'direct_mapping' if row['Mapped_File_Category Filled'] else 'unmapped'
        ), axis=1
    )
    df['Final Category Confidence Score'] = df.apply(
        lambda row: row['AI_FINAL_CATEGORY_CONFIDENCE'] if pd.notna(row['AI_FINAL_CATEGORY_CONFIDENCE']) else (
            1.0 if row['Mapped_File_Category Filled'] else None
        ), axis=1
    )
    df['subcategory_matching_reason'] = df.apply(
        lambda row: row['AI_MATCHING_REASON_FINAL_SUBCATEG'] if pd.notna(row['AI_MATCHING_REASON_FINAL_SUBCATEG']) else (
            'direct_mapping' if row['Mapped_File_Subcategory Filled'] else 'unmapped'
        ), axis=1
    )
    df['Final Subcategory Confidence Score'] = df.apply(
        lambda row: row['AI_FINAL_SUBCATEGORY_CONFIDENCE'] if pd.notna(row['AI_FINAL_SUBCATEGORY_CONFIDENCE']) else (
            1.0 if row['Mapped_File_Subcategory Filled'] else None
        ), axis=1
    )

    df['needs_model'] = ~(df['Mapped_File_Category Filled'] & df['Mapped_File_Subcategory Filled'])
    df['needs_category_model'] = ~df['Mapped_File_Category Filled']
    df['needs_subcategory_model'] = ~df['Mapped_File_Subcategory Filled']
    return df

def register_key_output(key_df, output_path, ws, seen_keys=None):
    if seen_keys is None:
        seen_keys = set()
    if key_df.empty:
        print("[INFO] No unseen keys to register. Skipping key registration.")
        return
    key_file_path = os.path.join(output_path, "key_reference_output.parquet")
    os.makedirs(os.path.dirname(key_file_path), exist_ok=True)
    key_df.to_parquet(key_file_path, index=False)
    datastore = ws.get_default_datastore()
    datastore.upload_files([key_file_path], "mapped_data/key_reference/", overwrite=True)
    dataset = Dataset.Tabular.from_parquet_files([(datastore, "mapped_data/key_reference/key_reference_output.parquet")])
    dataset.register(ws, name="bom_key_reference_dataset", description="Key columns to remap BOM", create_new_version=True)
    print("‚úÖ Registered key reference dataset")

def register_final_output_dir(output_dir, ws):
    print(f"[INFO] Registering final mapped output from: {output_dir}")
    parquet_files = glob.glob(os.path.join(output_dir, "*.parquet"))
    if not parquet_files:
        print(f"[WARN] No Parquet files found to register in {output_dir}")
        return
    datastore = ws.get_default_datastore()
    dataset = Dataset.Tabular.from_parquet_files([(datastore, os.path.join(output_dir, "*.parquet"))])
    dataset.register(ws, name="bom_final_mapped_dataset", description="Final mapped output from step1a", create_new_version=True)
    print("‚úÖ Registered: bom_final_mapped_dataset")

def finalize_output(df, output_path):
    df['AI_FINAL_CATEGORY'] = df['Mapped_File_Category']
    df['AI_FINAL_CATEGORY_CONFIDENCE'] = df['Final Category Confidence Score']
    df['AI_MATCHING_REASON_FINAL_CATEGORY'] = df['category_matching_reason']
    df['AI_FINAL_SUBCATEGORY'] = df['Mapped_File_Subcategory']
    df['AI_FINAL_SUBCATEGORY_CONFIDENCE'] = df['Final Subcategory Confidence Score']
    df['AI_MATCHING_REASON_FINAL_SUBCATEG'] = df['subcategory_matching_reason']

    for col in ['AI_FINAL_CATEGORY', 'AI_FINAL_SUBCATEGORY']:
        invalid_mask = df[col].isna() | df[col].astype(str).str.strip().isin(["", "nan", "NaN"])
        if 'CATEGORY' in col:
            df.loc[invalid_mask, 'AI_FINAL_CATEGORY_CONFIDENCE'] = None
            df.loc[invalid_mask, 'AI_MATCHING_REASON_FINAL_CATEGORY'] = None
            df.loc[invalid_mask, 'needs_category_model'] = True
        else:
            df.loc[invalid_mask, 'AI_FINAL_SUBCATEGORY_CONFIDENCE'] = None
            df.loc[invalid_mask, 'AI_MATCHING_REASON_FINAL_SUBCATEG'] = None
            df.loc[invalid_mask, 'needs_subcategory_model'] = True

        df.loc[invalid_mask, 'needs_model'] = True

    df.drop(columns=[
        'Mapped_File_Category', 'Final Category Confidence Score', 'category_matching_reason',
        'Mapped_File_Subcategory', 'Final Subcategory Confidence Score', 'subcategory_matching_reason'
    ], inplace=True, errors='ignore')

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df.to_parquet(output_path, index=False)
    print(f"[‚úÖ] Final output saved to: {output_path}")

def main():

    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True)
    parser.add_argument("--mapping_csv", type=str, required=True)
    parser.add_argument("--key_output", type=str, required=True)        # Should be a DIR now
    parser.add_argument("--final_output", type=str, required=True)
    args = parser.parse_args()

    parquet_files = glob.glob(os.path.join(args.input_path, "*.parquet"))
    if not parquet_files:
        raise FileNotFoundError(f"No parquet files found in {args.input_path}")

    csv_files = glob.glob(os.path.join(args.mapping_csv, "*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No CSV mapping file found in {args.mapping_csv}")
    mapping_df = pd.read_csv(csv_files[0])

    seen_components = set()
    seen_keys = set()
    os.makedirs(args.final_output, exist_ok=True)
    os.makedirs(args.key_output, exist_ok=True)  # Ensure key dir exists

    for file_path in parquet_files:
        print(f"\n Processing: {os.path.basename(file_path)}")

        try:
            df = pd.read_parquet(file_path)

            # Columns required
            mapping_cols = [
                "CMPNT_MATL_NUM", "CMPNT_MATL_DESC", "CMPNT_MATL_TYPE_CD",
                "CMPNT_CAT_CD_DESC", "CMPNT_UOM_CD"]
            
            key_cols = ["CMPNT_MATL_NUM", "CMPNT_MATL_DESC", "CMPNT_MATL_TYPE_CD",
                "CMPNT_CAT_CD_DESC", "CMPNT_UOM_CD", "LOGL_KEY_COMB_COL_VAL"]

            # --- Mapping Subset ---
            df_mapping = df[mapping_cols].copy()
            df_mapping = df_mapping[df_mapping['CMPNT_MATL_DESC'].notna()].drop_duplicates(subset=["CMPNT_MATL_NUM"])
            df_mapping = df_mapping[~df_mapping["CMPNT_MATL_NUM"].isin(seen_components)]
            if df_mapping.empty:
                print("[SKIPPED] All components already processed.")
                continue

            seen_components.update(df_mapping["CMPNT_MATL_NUM"].unique())

            # --- Key Subset ---
            df_key = df[key_cols].copy()
            df_key = df_key[df_key['LOGL_KEY_COMB_COL_VAL'].notna()].drop_duplicates(subset=["LOGL_KEY_COMB_COL_VAL"])
            df_key = df_key[~df_key['LOGL_KEY_COMB_COL_VAL'].isin(seen_keys)]
            


            df_key = df_key[df_key['LOGL_KEY_COMB_COL_VAL'].notna()].drop_duplicates(subset=["LOGL_KEY_COMB_COL_VAL"])
            seen_keys.update(df_key["LOGL_KEY_COMB_COL_VAL"].unique())

            # ‚úÖ Save per-file key reference file
            key_output_file = os.path.join(args.key_output, f"key_{os.path.basename(file_path)}")
            df_key.to_parquet(key_output_file, index=False)
            print(f"üß© Saved key file: {key_output_file}")

            # --- Mapping Logic ---
            df_mapping = ensure_ai_columns(df_mapping)
            df_mapping = apply_existing_ai_overrides(df_mapping)

            cat_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Category')
            cat_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Category')
            df_mapping = map_values(df_mapping, cat_map_1, cat_map_2, JOIN_COL_1, JOIN_COL_2,
                                    DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Category', 'Mapped_File_Category')

            sub_map_1 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_1, 'Mapped_File_Subcategory')
            sub_map_2 = clean_mapping_df(mapping_df, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory')
            df_mapping = map_values(df_mapping, sub_map_1, sub_map_2, JOIN_COL_1, JOIN_COL_2,
                                    DIRECT_MAP_KEY_1, DIRECT_MAP_KEY_2, 'Mapped_File_Subcategory', 'Mapped_File_Subcategory')

            df_mapping = add_flags(df_mapping)

            # ‚úÖ Save mapped output
            mapped_output_file = os.path.join(args.final_output, f"mapped_{os.path.basename(file_path)}")
            finalize_output(df_mapping, mapped_output_file)

        except Exception as e:
            print(f"[‚ùå ERROR] Failed processing {file_path}: {e}")

    print("\n‚úÖ All files processed successfully.")
if __name__ == "__main__":
    main()



Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (packaging 23.2 (/opt/conda/envs/ptca/lib/python3.8/site-packages), Requirement.parse('packaging<=23.0,>=20.0')).
usage: step_1a_extract_and_merge.py [-h] --input_path INPUT_PATH --mapping_csv
                                    MAPPING_CSV --key_output KEY_OUTPUT
                                    --final_output FINAL_OUTPUT
step_1a_extract_and_merge.py: error: unrecognized arguments: --skip_register
Cleaning up all outstanding Run operations, waiting 300.0 seconds
1 items cleaning up...
Cleanup took 0.0940403938293457 seconds
Traceback (most recent call last):
  File "step_1a_extract_and_merge.py", line 229, in <module>
    main()
  File "step_1a_extract_and_merge.py", line 149, in main
    args = parser.parse_args()
  File "/opt/conda/envs/ptca/lib/python3.8/argparse.py", line 1771, in parse_args
    self.error(msg % ' '.join(argv))
  File "/opt/conda/envs/ptca/lib/python3.8/argparse.py", line 2521, in error
    self.exit(2, _('%(prog)s: error: %(message)s\n') % args)
  File "/opt/conda/envs/ptca/lib/python3.8/argparse.py", line 2508, in exit
    _sys.exit(status)
SystemExit: 2

