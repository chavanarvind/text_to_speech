import os
import glob
import joblib
import torch
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler

# --- DEVICE + MODEL SETUP ---
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"üñ•Ô∏è Using device: {device}")
if device == 'cuda':
    print(f"üöÄ GPU available: {torch.cuda.get_device_name(0)}")

encoder = SentenceTransformer(
    'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb',
    device=device
)

ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
scaler = StandardScaler()

# --- PATHS ---
input_path = './data/target_map_cleaned_non_null_target'
embedding_cache_dir = './data/validation_step/model_artifects/bert_embedding'
sample_cache_dir = './data/validation_step/model_artifects/sampled_rows'
output_dir = './data/validation_step/model_artifects'
os.makedirs(embedding_cache_dir, exist_ok=True)
os.makedirs(sample_cache_dir, exist_ok=True)
os.makedirs(output_dir, exist_ok=True)

output_X = []
output_y = []

required_cols = [
    'CMPNT_MATL_DESC', 'CMPNT_MATL_DESC_LEN',
    'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY',
    'Final Category'
]

# --- CATEGORY-SPECIFIC SAMPLING FRACTIONS ---
category_frac_map = {
    'CHM': 0.05,
    'PKG': 0.05,
    'FNW': 0.1,
    'FNW_CHM': 0.3,
    'Liquids and Creams': 0.5,
    'API': 1.0
}

# --- PROCESS FILES ---
all_files = glob.glob(os.path.join(input_path, '*.parquet'))
print(f"üìÅ Found {len(all_files)} parquet files to process.\n")

for file in all_files:
    file_name = os.path.splitext(os.path.basename(file))[0]
    bert_cache_file = os.path.join(embedding_cache_dir, f"{file_name}_bert.npy")
    sample_cache_file = os.path.join(sample_cache_dir, f"{file_name}_sampled.parquet")

    try:
        # Load and clean
        df = pd.read_parquet(file, columns=required_cols)
        df.drop_duplicates(subset=required_cols, inplace=True)
        df = df[df['CMPNT_MATL_DESC'].notna()].copy()
        df['Final Category'] = df['Final Category'].replace({'FNW or CHM': 'FNW_CHM'})
        df.drop_duplicates(inplace=True)

        # Stratified sample with category-specific fraction
        df_sampled = (
            df.groupby('Final Category', group_keys=False)
            .apply(lambda g: g.sample(
                frac=category_frac_map.get(g.name, 0.1),
                random_state=42
            ))
            .reset_index(drop=True)
        )

        if df_sampled.empty:
            print(f"‚ö†Ô∏è Skipping {file_name}: No valid rows after sampling")
            continue

        df_sampled.to_parquet(sample_cache_file, index=False)

        if os.path.exists(bert_cache_file):
            print(f"üîÅ Skipping {file_name}: already embedded.")
            desc_emb = np.load(bert_cache_file)
        else:
            print(f"üîÑ Generating embeddings for: {file_name} ({len(df_sampled)} rows)")
            desc_texts = df_sampled['CMPNT_MATL_DESC'].astype(str).tolist()
            desc_emb = encoder.encode(
                desc_texts,
                batch_size=128,
                show_progress_bar=True,
                convert_to_numpy=True,
                num_workers=4
            )
            np.save(bert_cache_file, desc_emb)

        if len(desc_emb) != len(df_sampled):
            print(f"‚ùå Mismatch in {file_name}: {len(desc_emb)} embeddings vs {len(df_sampled)} rows. Skipping.")
            continue

        df_sampled['__bert_emb'] = list(desc_emb)
        output_X.append(df_sampled)
        output_y.append(df_sampled['Final Category'])

    except Exception as e:
        print(f"‚ùå Error processing {file_name}: {e}")

# --- COMBINE ALL DATA ---
if not output_X:
    raise RuntimeError("‚ùå No valid data processed. Ensure .npy and .parquet files match.")

df_all = pd.concat(output_X, ignore_index=True)
print(f"\n‚úÖ Total records after merge: {len(df_all)}")

# --- FIT AND SAVE ENCODERS ---
ordinal.fit(df_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
scaler.fit(df_all[['CMPNT_MATL_DESC_LEN']])
joblib.dump(ordinal, os.path.join(output_dir, 'ordinal_encoder.pkl'))
joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))
print("‚úÖ OrdinalEncoder and Scaler saved.")

# --- FINAL FEATURE MATRIX ---
meta_scaled = scaler.transform(df_all[['CMPNT_MATL_DESC_LEN']])
cat_encoded = ordinal.transform(df_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
bert_array = np.vstack(df_all['__bert_emb'].values)

X_full = np.hstack([bert_array, meta_scaled, cat_encoded])
y_full = df_all['Final Category'].values

np.save(os.path.join(output_dir, 'X_full.npy'), X_full)
np.save(os.path.join(output_dir, 'y_full.npy'), y_full)
print("‚úÖ Final BERT+Meta feature matrix saved.")
