import sys
import json
from datetime import datetime
from azureml.core import Workspace, Experiment, Environment, Dataset, Datastore
from azureml.core.authentication import ServicePrincipalAuthentication
from azureml.core.runconfig import RunConfiguration
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.data import OutputFileDatasetConfig

# === ENV: dev or prod ===
ENV = 'prod'  # Change to 'prod' when needed

# === Helper Functions ===
def get_workspace_from_config(path):
    with open(path) as f:
        cfg = json.load(f)
    return Workspace(subscription_id=cfg["subscription_id"],
                     resource_group=cfg["resource_group"],
                     workspace_name=cfg["workspace_name"])

def get_workspace_from_sp(args, config_file):
    tenant_id = args[1]
    client_id = args[2]
    client_secret = args[3]
    sp_auth = ServicePrincipalAuthentication(
        tenant_id=tenant_id,
        service_principal_id=client_id,
        service_principal_password=client_secret
    )
    with open(config_file) as f:
        cfg = json.load(f)
    return Workspace(subscription_id=cfg["subscription_id"],
                     resource_group=cfg["resource_group"],
                     workspace_name=cfg["workspace_name"],
                     auth=sp_auth)

def get_run_config(ws, compute_name, env_name):
    run_config = RunConfiguration()
    run_config.target = compute_name
    run_config.environment = Environment.get(workspace=ws, name=env_name)
    return run_config

# === Resolve workspace and environment-specific values ===
config_file = f'./.azureml/{ENV}_config.json'
if len(sys.argv) > 3:
    ws = get_workspace_from_sp(sys.argv, config_file)
else:
    ws = get_workspace_from_config(config_file)

if ENV == 'dev':
    compute_target_name = "llm-gpu-cluster-2"
    compute_target_1_name = "rpmTestLoad"
    datastore_name = "xbomrefadlsg2"
else:  # ENV == 'prod'
    compute_target_name = "bom-cluster-2"
    compute_target_1_name = "rpmProdLoad"
    datastore_name = "xbom_prod_adlsgen2"

compute_target = ws.compute_targets[compute_target_name]
compute_target_1 = ws.compute_targets[compute_target_1_name]


datastore_name = "xbomrefadlsg2" if ENV == 'dev' else "xbom_prod_adlsgen2"
env_name = "Bom_X_Evaluator"

run_config = get_run_config(ws, compute_target_name, env_name)
default_ds = Datastore.get(ws, datastore_name)
today = datetime.today().strftime("%d%m%Y")

# === Datastore and Uploads ===
datastore = ws.get_default_datastore()

# Upload mapping and abbreviation CSVs
mapping_local = './local_data/high_conf_direct_mapping.csv'
abbrev_local = './local_data/abbreviation_expension_updated.csv'

datastore.upload_files(
    files=[mapping_local],
    target_path='high_conf_map',
    overwrite=True,
    show_progress=True
)

datastore.upload_files(
    files=[abbrev_local],
    target_path='abbrev_map',
    overwrite=True,
    show_progress=True
)

# === Dataset References ===
high_conf_dataset = Dataset.File.from_files((datastore, "high_conf_map"))
abbr_dataset = Dataset.File.from_files((datastore, "abbrev_map"))

mapping_csv_mount = high_conf_dataset.as_named_input("high_conf_map").as_mount()
abbrev_csv_mount = abbr_dataset.as_named_input("abbrev_map").as_mount()

# === PipelineData for Intermediate Outputs ===
step1_out = PipelineData(name="step1_output", is_directory=True)
step1a_final_output = PipelineData(name="step1a_final_output", is_directory=True)
step1a_key_output_temp = PipelineData(name="step1a_key_output_temp", is_directory=True)
step2_final_output = PipelineData(name="step2_final_output", is_directory=True)
step3_final_output = PipelineData(name="step3_final_output", is_directory=True)

# === Step 4 Output (Final Merged Output) ===
final_merged_output = OutputFileDatasetConfig(
    name="final_merged_predictions",
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)

# === Step 1: Pull Dataset ===
step1_pull = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", step1_out],
    outputs=[step1_out],
    compute_target=compute_target_1,
    runconfig=run_config,
    allow_reuse=False
)

# === Step 1a: Merge & Map ===
step1a_merge = PythonScriptStep(
    name="Step 1a - Merge & Map",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    arguments=[
        "--input_path", step1_out,
        "--mapping_csv", mapping_csv_mount,
        "--key_output", step1a_key_output_temp,
        "--final_output", step1a_final_output
    ],
    inputs=[step1_out, mapping_csv_mount],
    outputs=[step1a_final_output, step1a_key_output_temp],
    compute_target=compute_target_1,
    runconfig=run_config,
    allow_reuse=False
)

# === Step 2: Preprocessing ===
step2_preprocess = PythonScriptStep(
    name="Step 2 - Preprocessing",
    script_name="step2_pre_process.py",
    source_directory="scripts",
    arguments=[
        "--input_path", step1a_final_output,
        "--abbrev_map", abbrev_csv_mount,
        "--output_path", step2_final_output
    ],
    inputs=[step1a_final_output, abbrev_csv_mount],
    outputs=[step2_final_output],
    compute_target=compute_target_1,
    runconfig=run_config,
    allow_reuse=False
)

# === Step 3: Inference Prediction ===
step3_inference = PythonScriptStep(
    name="Step 3 - Inference Prediction",
    script_name="step3_inference_run.py",
    source_directory="scripts",
    arguments=[
        "--input_path", step2_final_output,
        "--final_output_dir", step3_final_output
    ],
    inputs=[step2_final_output],
    outputs=[step3_final_output],
    compute_target=compute_target,
    runconfig=run_config,
    allow_reuse=False
)

# === Step 4: Merge Inference Results with Key Files (Final Output) ===
step4_merge = PythonScriptStep(
    name="Step 4 - Merge Inference Results with Key Files",
    script_name="step_4_merge_with_key_files.py",
    source_directory="scripts",
    arguments=[
        "--inference_output_dir", step3_final_output,
        "--key_output_dir", step1a_key_output_temp,
        "--final_output_dir", final_merged_output
    ],
    inputs=[step3_final_output, step1a_key_output_temp],
    outputs=[final_merged_output],
    compute_target=compute_target_1,
    runconfig=run_config,
    allow_reuse=False
)

# === Build and Run Pipeline ===
pipeline = Pipeline(workspace=ws, steps=[
    step1_pull,
    step1a_merge,
    step2_preprocess,
    step3_inference,
    step4_merge
])
pipeline.validate()

experiment = Experiment(ws, name="bom_pipeline_pull_merge_preprocess")
run = experiment.submit(pipeline)
run.wait_for_completion(show_output=True)
