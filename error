# step_1a_extract_and_merge_old.py

import os
import glob
import argparse
import pandas as pd
from azureml.core import Run
from multiprocessing import Pool, cpu_count

# === Column to match ===
JOIN_COL_1 = "CMPNT_CAT_CD_DESC"
JOIN_COL_2 = "CMPNT_MATL_TYPE_CD"
DIRECT_MAP_KEY_1 = "Direct Mapping"
DIRECT_MAP_KEY_2 = "Direct Mapping"

# === Merge one file ===
def process_file(file_path, mapping_df, mapped_dir, needs_model_dir):
   

    try:
        df = pd.read_parquet(file_path, columns=["CMPNT_MATL_NUM", 'CMPNT_MATL_DESC','CMPNT_MATL_TYPE_CD',
    'CMPNT_CAT_CD_DESC',
    'CMPNT_UOM_CD'])
        df = df[df['CMPNT_MATL_DESC'].notna()]
        df.drop_duplicates(inplace=True)

        df[JOIN_COL_1] = df[JOIN_COL_1].fillna("").astype(str).str.strip()
        df[JOIN_COL_2] = df[JOIN_COL_2].fillna("").astype(str).str.strip()
        mapping_df[DIRECT_MAP_KEY_1] = mapping_df[DIRECT_MAP_KEY_1].astype(str).str.strip()
        mapping_df[DIRECT_MAP_KEY_2] = mapping_df[DIRECT_MAP_KEY_2].astype(str).str.strip()

        # Stage 1 mapping: based on CMPNT_CAT_CD_DESC only
        stage1_df = df.merge(mapping_df, how="inner", left_on=[JOIN_COL_1], right_on=[DIRECT_MAP_KEY_1])
        stage1_df['matching_reason'] = 'direct_mapping_category'
        stage1_df['confidence_score'] = 1.0
        print(stage1_df[stage1_df['matching_reason'] =='direct_mapping_category'].head(5))
       
        # Stage 2 mapping: based on CMPNT_MATL_TYPE_CD only (excluding already mapped)
        df_not_mapped_in_stage1 = df[~df['CMPNT_MATL_NUM'].isin(stage1_df['CMPNT_MATL_NUM'])]
        stage2_df = df_not_mapped_in_stage1.merge(mapping_df, how="inner", left_on=[JOIN_COL_2], right_on=[DIRECT_MAP_KEY_2])
        stage2_df['matching_reason'] = 'direct_mapping_type_only'
        stage2_df['confidence_score'] = 1.0
        print([stage2_df['matching_reason'] =='direct_mapping_type_only'].head(5))

        # Combine both mappings
        mapped_df = pd.concat([stage1_df, stage2_df], ignore_index=True)
        print("üìå Mapped counts:")
        print(f"  üîπ Stage 1 (by category only): {len(stage1_df)}")
        print(f"  üîπ Stage 2 (by material type only): {len(stage2_df)}")
        print(f"  üîπ Total mapped rows combined: {len(mapped_df)}")

        # Show a few rows for confirmation
        if len(mapped_df) > 0:
         print(" Sample of mapped rows:")
         print(mapped_df[['CMPNT_MATL_NUM', 'Final Category', 'Final Subcategory']].head())
        else:
         print("No mapping found for file:", os.path.basename(file_path))

        # Add mapping flags
        mapped_df['Final Category Filled'] = mapped_df['Final Category'].notna() & mapped_df['Final Category'].str.strip().ne("")
        mapped_df['Final Subcategory Filled'] = mapped_df['Final Subcategory'].notna() & mapped_df['Final Subcategory'].str.strip().ne("")
        mapped_df['needs_model'] = ~mapped_df['Final Category Filled'] | ~mapped_df['Final Subcategory Filled']

        # Split into mapped and needing model
        mapped_only_df = mapped_df[~mapped_df['needs_model']].copy()
        needs_model_df = mapped_df[mapped_df['needs_model']].copy()

        print(f"Fully mapped: {file_path}, records: {len(mapped_only_df)}")
        print(f" Needs model (either cat/subcat): {file_path}, records: {len(needs_model_df)}")

        # Output path
        base_name = os.path.basename(file_path)
        os.makedirs(mapped_dir, exist_ok=True)
        os.makedirs(needs_model_dir, exist_ok=True)

        mapped_only_df.to_parquet(os.path.join(mapped_dir, base_name), index=False)
        needs_model_df.to_parquet(os.path.join(needs_model_dir, base_name), index=False)

    except Exception as e:
        print(f"‚ùå Failed to process {file_path}: {e}")

# === Main script ===
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True)
    parser.add_argument("--mapping_csv", type=str, required=True)
    parser.add_argument("--mapped_output", type=str, required=True)
    parser.add_argument("--needs_model_output", type=str, required=True)
    args = parser.parse_args()

    run = Run.get_context()

    # === Locate CSV inside mounted directory ===
    csv_files = glob.glob(os.path.join(args.mapping_csv, "*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No CSV file found in mapped directory: {args.mapping_csv}")

    mapping_path = csv_files[0]
    print(f"üì• Reading mapping from: {mapping_path}")
    mapping_df = pd.read_csv(mapping_path)
    mapping_df = mapping_df.drop_duplicates()
    print(f"üìä Mapping rows: {len(mapping_df)}")

    # === Process input parquet files ===
    all_files = glob.glob(os.path.join(args.input_path, "*.parquet"))[:3]
    print(f"üìÅ Total input parquet files found: {len(all_files)}")

    for file_path in all_files:
        process_file(file_path, mapping_df, args.mapped_output, args.needs_model_output)

if __name__ == "__main__":
    main()
