import os
import glob
import numpy as np
import pandas as pd
import torch
import joblib
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler

# --- Paths ---
input_path = './data/target_map_cleaned_non_null_target'
embedding_dir = './data/validation_step/model_artifects/bert_embedding'
sample_dir = './data/validation_step/model_artifects/sampled_rows'
output_dir = './data/validation_step/model_artifects'
os.makedirs(embedding_dir, exist_ok=True)
os.makedirs(sample_dir, exist_ok=True)
os.makedirs(output_dir, exist_ok=True)

# --- BERT Encoder ---
encoder = SentenceTransformer(
    'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb',
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# --- Sampling Map ---
category_frac_map = {
    'CHM': 0.10, 'PKG': 0.10, 'FNW': 0.20, 'FNW_CHM': 0.30,
    'Liquids and Creams': 0.50, 'API': 1.0
}

required_cols = [
    'CMPNT_MATL_DESC', 'CMPNT_MATL_DESC_LEN',
    'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY',
    'Final Category'
]

files = sorted(glob.glob(os.path.join(input_path, '*.parquet')))
output_X = []
output_y = []
meta_features = []

for file in files:
    file_name = os.path.splitext(os.path.basename(file))[0]
    print(f"Processing {file_name}")

    try:
        df = pd.read_parquet(file, columns=required_cols)
        df = df[df['CMPNT_MATL_DESC'].notna()]
        df = df.drop_duplicates(subset=required_cols)
        df['Final Category'] = df['Final Category'].replace({'FNW or CHM': 'FNW_CHM'})

        sampled = (
            df.groupby('Final Category', group_keys=False)
              .apply(lambda g: g.sample(frac=category_frac_map.get(g.name, 0.1), random_state=42))
              .reset_index(drop=True)
        )

        if sampled.empty:
            print(f" Skipping {file_name}: no valid sampled rows")
            continue

        # Encode descriptions
        desc_emb = encoder.encode(
            sampled['CMPNT_MATL_DESC'].astype(str).tolist(),
            batch_size=256,
            show_progress_bar=True,
            convert_to_numpy=True,
            num_workers=4
        )

        if len(desc_emb) != len(sampled):
            print(f" Skipped {file_name}: embedding/sample size mismatch")
            continue

        # Save sampled for auditing
        sampled.to_parquet(os.path.join(sample_dir, f'{file_name}_sampled.parquet'), index=False)
        np.save(os.path.join(embedding_dir, f'{file_name}_bert.npy'), desc_emb)

        # Accumulate for X/y
        output_X.append(desc_emb)
        output_y.append(sampled['Final Category'].astype(str).values)
        meta_features.append(sampled[['CMPNT_MATL_DESC_LEN', 'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])

        print(f"Processed: {file_name} ({len(sampled)} rows)")

    except Exception as e:
        print(f"Error processing {file_name}: {e}")

# --- Final Checks ---
if not output_X:
    raise RuntimeError("No valid embeddings or samples generated.")

# --- Stack Features and Labels ---
bert_array = np.vstack(output_X)
y_full = np.concatenate(output_y)
meta_all = pd.concat(meta_features, ignore_index=True)

# --- Encode Meta ---
ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
scaler = StandardScaler()

ordinal.fit(meta_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
scaler.fit(meta_all[['CMPNT_MATL_DESC_LEN']])

meta_scaled = scaler.transform(meta_all[['CMPNT_MATL_DESC_LEN']])
cat_encoded = ordinal.transform(meta_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])

# --- Final Feature Matrix ---
X_full = np.hstack([bert_array, meta_scaled, cat_encoded])

# --- Save Everything ---
np.save(os.path.join(output_dir, 'X_full.npy'), X_full)
np.save(os.path.join(output_dir, 'y_full.npy'), y_full)  # string dtype avoids pickle issues
joblib.dump(ordinal, os.path.join(output_dir, 'ordinal_encoder.pkl'))
joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))

print(f"\n All files saved:")
print(f"   X_full.npy shape = {X_full.shape}")
print(f"   y_full.npy shape = {y_full.shape}, dtype = {y_full.dtype}")
