# Minimal pipeline with Data Pull and Direct Mapping Only

from azureml.core import Workspace, Environment, Experiment
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data.datapath import DataPath
from azureml.data import OutputFileDatasetConfig
from azureml.core import Dataset, Datastore

from datetime import datetime

today = datetime.today().strftime("%d%m%Y")

# Workspace config
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"

ws = Workspace(
    subscription_id=SUBSCRIPTION_ID,
    resource_group=RESOURCE_GROUP,
    workspace_name=WORKSPACE_NAME
)

compute_name = "llm-gpu-cluster-2"
env = Environment.get(workspace=ws, name="Bom_X_Evaluator")
run_config = RunConfiguration()
run_config.target = compute_name
run_config.environment = env

default_ds = ws.get_default_datastore()

# === Shared Outputs ===
data_step1_out = PipelineData(name="data_step1_out", is_directory=True)
data_step1a_mapped = PipelineData(name="data_step1a_mapped", is_directory=True)
data_step1a_key_output = OutputFileDatasetConfig(
    name='data_step1a_key_output',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)

# === Mapping CSV as input ===
default_ds.upload_files(
    files=['./local_data/high_conf_direct_mapping.csv'],
    target_path='high_conf_map',
    overwrite=True,
    show_progress=True
)
high_conf_dataset = Dataset.File.from_files((default_ds, 'high_conf_map'))
high_conf_mount = high_conf_dataset.as_named_input('high_conf_map').as_mount()

# === Step 1: Pull Harmonized BOM Data ===
data_pull_step = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", data_step1_out],
    outputs=[data_step1_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Step 1a: Direct Mapping (from canvas script) ===
direct_mapping_step = PythonScriptStep(
    name="Step 1a - Direct Mapping Logic",
    script_name="step_1a_direct_mapping.py",
    source_directory="scripts",
    inputs=[data_step1_out, high_conf_mount],
    arguments=[
        "--input_path", data_step1_out,
        "--mapping_csv", high_conf_mount,
        "--key_output", data_step1a_key_output,
        "--final_output", data_step1a_mapped
    ],
    outputs=[data_step1a_mapped, data_step1a_key_output],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# === Build and Submit Pipeline ===
pipeline = Pipeline(workspace=ws, steps=[
    data_pull_step,
    direct_mapping_step
])

pipeline.validate()
experiment = Experiment(ws, "direct_mapping_pipeline")
pipeline_run = experiment.submit(pipeline)
pipeline_run.wait_for_completion(show_output=True)
