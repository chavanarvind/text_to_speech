from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.metrics import log_loss, accuracy_score, classification_report, top_k_accuracy_score
import glob
import os
import pandas as pd
import numpy as np

# Load and combine parquet files
input_path = './data/target_map_cleaned_non_null_target'
all_files = glob.glob(os.path.join(input_path, '*.parquet'))
df = pd.concat([pd.read_parquet(f) for f in all_files], ignore_index=True)
df = df[df['Final Category'].notna()].copy()
df['Final Category'] = df['Final Category'].replace({'FNW or CHM': 'FNW_CHM'})

# Features and target
features = [
    'MATL_SHRT_DESC', 'CMPNT_MATL_DESC',
    'MATL_SHRT_DESC_LEN', 'CMPNT_MATL_DESC_LEN',
    'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY'
]
target = 'Final Category'
X = df[features]
y = df[target]
classes = np.unique(y)

# Train-test split
X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Preprocessing pipeline with OrdinalEncoder
preprocessor = ColumnTransformer(transformers=[
    ('text1', TfidfVectorizer(ngram_range=(1, 2), max_features=8000), 'MATL_SHRT_DESC'),
    ('text2', TfidfVectorizer(ngram_range=(1, 2), max_features=8000), 'CMPNT_MATL_DESC'),
    ('num', StandardScaler(), ['MATL_SHRT_DESC_LEN', 'CMPNT_MATL_DESC_LEN']),
    ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),
     ['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']),
])

# Transform data
X_train_transformed = preprocessor.fit_transform(X_train)
X_val_transformed = preprocessor.transform(X_val)

# Initialize SGDClassifier with partial_fit
clf = SGDClassifier(loss='log_loss', max_iter=1, tol=None, warm_start=True,
                    random_state=42, class_weight='balanced')

# Training loop with early stopping
best_loss = float('inf')
patience = 3
no_improve_count = 0
n_epochs = 20

for epoch in range(1, n_epochs + 1):
    clf.partial_fit(X_train_transformed, y_train, classes=classes)

    y_val_proba = clf.predict_proba(X_val_transformed)
    val_loss = log_loss(y_val, y_val_proba, labels=classes)
    val_acc = accuracy_score(y_val, clf.predict(X_val_transformed))
    val_top3 = top_k_accuracy_score(y_val, y_val_proba, k=3, labels=classes)

    print(f"ðŸ“˜ Epoch {epoch}: Val Loss = {val_loss:.4f} | Top-1 Acc = {val_acc:.4f} | Top-3 Acc = {val_top3:.4f}")

    if val_loss < best_loss - 1e-4:
        best_loss = val_loss
        best_model = clf
        no_improve_count = 0
    else:
        no_improve_count += 1
        if no_improve_count >= patience:
            print(f"ðŸ›‘ Early stopping triggered at epoch {epoch}")
            break

# Final classification report
print("\nðŸ“Š Final Classification Report:")
print(classification_report(y_val, best_model.predict(X_val_transformed), digits=3))
