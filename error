from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient
from datetime import datetime
import pandas as pd
import os

# ------------------ CONFIG ------------------ #
storage_account_name = "adlsdtodsdev"
container_name = "bom-xref"

# ADLS folder path
today = datetime.today().strftime("%d%m%Y")
folder_path = f"hbom_category_prediction/hbom_category_prediction_{today}/"
file_name = "sample_output.csv"
blob_path = f"{folder_path}{file_name}"

# ------------------ CREATE CSV ------------------ #
df = pd.DataFrame({
    "Product": ["Widget A", "Widget B", "Widget C"],
    "Price": [25.5, 40.0, 15.75],
    "Quantity": [10, 5, 20]
})
local_file_path = file_name
df.to_csv(local_file_path, index=False)
print(f"[INFO] CSV file saved locally as: {local_file_path}")

# ------------------ AUTH & UPLOAD ------------------ #
account_url = f"https://{storage_account_name}.blob.core.windows.net"
credential = DefaultAzureCredential()

# Connect to blob storage
blob_service_client = BlobServiceClient(account_url=account_url, credential=credential)
blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)

# Upload CSV
with open(local_file_path, "rb") as data:
    blob_client.upload_blob(data, overwrite=True)

print(f"âœ… Uploaded to ADLS Gen2 path: {blob_path}")
