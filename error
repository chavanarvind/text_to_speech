import pandas as pd
import spacy
from collections import Counter, defaultdict
from itertools import islice
from pathlib import Path
import os
import time
from concurrent.futures import ProcessPoolExecutor, as_completed

# Global NLP model (loaded per worker)
nlp = None
def init_spacy():
    global nlp
    if nlp is None:
        nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])

# Config
input_path = './data/target_map_cleaned'
output_path = Path('./data/gram_outputs/frequent_terms_by_category')
output_path.mkdir(parents=True, exist_ok=True)
required_cols = ['Final Category', 'MATL_SHRT_DESC_AND_CMPNT_MATL_DESC']
threshold = 10

def process_file(file_path_str):
    init_spacy()
    file_path = Path(file_path_str)
    start_time = time.time()
    try:
        df = pd.read_parquet(file_path, columns=required_cols)
        df = df[df['Final Category'].notna()]
        df = df.dropna(subset=['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'])
        df = df.drop_duplicates(subset=['Final Category', 'MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'])

        unigram_freq = defaultdict(Counter)
        bigram_freq = defaultdict(Counter)

        for category, group in df.groupby('Final Category'):
            texts = group['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'].astype(str).tolist()

            for doc in nlp.pipe(texts, batch_size=100, n_process=2):
                words = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]
                unigram_freq[category].update(set(words))  # unique per row
                bigrams = zip(words, islice(words, 1, None))
                bigram_freq[category].update(bigrams)

        print(f"âœ… {file_path.name} in {time.time() - start_time:.2f}s")
        return unigram_freq, bigram_freq

    except Exception as e:
        print(f"âŒ {file_path.name}: {e}")
        return defaultdict(Counter), defaultdict(Counter)

if __name__ == '__main__':
    all_files = list(Path(input_path).glob("*.parquet"))
    print(f"ðŸ“ Total files: {len(all_files)}")

    global_unigrams = defaultdict(Counter)
    global_bigrams = defaultdict(Counter)

    with ProcessPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(process_file, str(f)) for f in all_files]
        for f in as_completed(futures):
            file_unigrams, file_bigrams = f.result()
            for cat in file_unigrams:
                global_unigrams[cat].update(file_unigrams[cat])
            for cat in file_bigrams:
                global_bigrams[cat].update(file_bigrams[cat])

    # Flatten and filter results
    unigram_rows = []
    bigram_rows = []

    print("\nðŸ” Preview of results (top 5 per category):\n")

    for cat, counter in global_unigrams.items():
        rows = [
            {'category': cat, 'word': word, 'count': count}
            for word, count in counter.items() if count >= threshold
        ]
        unigram_rows.extend(rows)

        print(f"ðŸ“˜ Top unigrams for {cat}:")
        for word, count in counter.most_common(5):
            print(f"  {word}: {count}")
        print()

    for cat, counter in global_bigrams.items():
        rows = [
            {'category': cat, 'bigram': f"{w1} {w2}", 'count': count}
            for (w1, w2), count in counter.items() if count >= threshold
        ]
        bigram_rows.extend(rows)

        print(f"ðŸ“— Top bigrams for {cat}:")
        for (w1, w2), count in counter.most_common(5):
            print(f"  {w1} {w2}: {count}")
        print()

    # Save final results
    pd.DataFrame(unigram_rows).to_csv(output_path / "all_categories_unigrams.csv", index=False)
    pd.DataFrame(bigram_rows).to_csv(output_path / "all_categories_bigrams.csv", index=False)
    print("âœ… Saved final unigram and bigram CSV files.")
