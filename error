import pandas as pd
import spacy
from collections import Counter, defaultdict
from itertools import islice
from pathlib import Path
import os
import time
from concurrent.futures import ProcessPoolExecutor, as_completed

# Global NLP model (loaded per process)
nlp = None
def init_spacy():
    global nlp
    if nlp is None:
        nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])

# Config
input_path = './data/target_map_cleaned'
output_path = Path('./data/gram_outputs/frequent_terms_by_category')
output_path.mkdir(parents=True, exist_ok=True)
required_cols = ['Final Category', 'MATL_SHRT_DESC_AND_CMPNT_MATL_DESC']
num_chunks = 3
threshold = 20

# Chunk processor
def process_dataframe_chunk(df_chunk, chunk_name):
    init_spacy()
    start_time = time.time()
    unigram_freq = defaultdict(Counter)
    bigram_freq = defaultdict(Counter)

    for category, group in df_chunk.groupby('Final Category'):
        texts = group['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'].astype(str).tolist()
        for doc in nlp.pipe(texts, batch_size=100, n_process=2):
            words = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]
            unigram_freq[category].update(set(words))  # unique per row
            bigram_freq[category].update(zip(words, islice(words, 1, None)))

    print(f"âœ… {chunk_name} in {time.time() - start_time:.2f}s")
    return unigram_freq, bigram_freq

# Load and split files into chunks
def split_and_submit(file_path, executor, futures):
    try:
        df = pd.read_parquet(file_path, columns=required_cols)
        df = df.dropna(subset=['Final Category', 'MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'])
        df = df[df['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'].str.strip() != '']
        df = df.drop_duplicates(subset=['Final Category', 'MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'])

        total_len = len(df)
        if total_len == 0:
            return

        chunk_size = total_len // num_chunks
        for i in range(num_chunks):
            start = i * chunk_size
            end = None if i == num_chunks - 1 else (i + 1) * chunk_size
            chunk = df.iloc[start:end].copy()
            chunk_id = f"{Path(file_path).stem}_chunk{i}"
            futures.append(executor.submit(process_dataframe_chunk, chunk, chunk_id))

    except Exception as e:
        print(f"âŒ Failed to process {file_path}: {e}")

if __name__ == '__main__':
    all_files = list(Path(input_path).glob("*.parquet"))
    # Uncomment below for testing with only one file
    # all_files = [all_files[0]]

    print(f"ðŸ“ Total files: {len(all_files)}")

    global_unigrams = defaultdict(Counter)
    global_bigrams = defaultdict(Counter)

    with ProcessPoolExecutor(max_workers=6) as executor:
        futures = []
        for file_path in all_files:
            split_and_submit(file_path, executor, futures)

        for f in as_completed(futures):
            file_unigrams, file_bigrams = f.result()
            for cat in file_unigrams:
                global_unigrams[cat].update(file_unigrams[cat])
            for cat in file_bigrams:
                global_bigrams[cat].update(file_bigrams[cat])

    # Flatten and export results
    unigram_rows = []
    bigram_rows = []

    print("\nðŸ” Preview of top 5 grams per category:\n")

    for cat, counter in global_unigrams.items():
        filtered = [(w, c) for w, c in counter.items() if c > threshold]
        unigram_rows.extend([
            {'category': cat, 'word': word, 'count': count}
            for word, count in filtered
        ])

        print(f"ðŸ“˜ {cat} - Top unigrams:")
        for word, count in Counter(dict(filtered)).most_common(5):
            print(f"  {word}: {count}")
        print()

    for cat, counter in global_bigrams.items():
        filtered = [(f"{w1} {w2}", c) for (w1, w2), c in counter.items() if c > threshold]
        bigram_rows.extend([
            {'category': cat, 'bigram': bigram, 'count': count}
            for bigram, count in filtered
        ])

        print(f"ðŸ“— {cat} - Top bigrams:")
        for bigram, count in Counter(dict(filtered)).most_common(5):
            print(f"  {bigram}: {count}")
        print()

    # Save results
    pd.DataFrame(unigram_rows).to_csv(output_path / "all_categories_unigrams.csv", index=False)
    pd.DataFrame(bigram_rows).to_csv(output_path / "all_categories_bigrams.csv", index=False)
    print("âœ… Saved CSVs with threshold > 20.")
