
üîÑ [Processing file]: part-00000-00185f20-d84d-4828-b7cb-f6f0a7f512fa-c000.snappy.parquet
"datastore.upload_files" is deprecated after version 1.0.69. Please use "FileDatasetFactory.upload_directory" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.
[‚ùå ERROR] Failed to process /mnt/azureml/cr/j/62ca6bc190614affb42ed740d10f41f0/cap/data-capability/wd/INPUT_step1_output_workspaceblobstore/azureml/c7fc02ca-181e-49ff-b8f9-065b384fb19c/step1_output/part-00000-00185f20-d84d-4828-b7cb-f6f0a7f512fa-c000.snappy.parquet: UserErrorException:
	Message: relative_root: 'mapped_data/key_reference/' is not part of the file_path: '/mnt/azureml/cr/j/62ca6bc190614affb42ed740d10f41f0/cap/data-capability/wd/INPUT_step1a_key_output_temp_workspaceblobstore/azureml/4ff6276a-2c72-4d0d-a361-242b6de4eb09/step1a_key_output_temp/key_reference_output.parquet'.
	InnerException None
	ErrorResponse 
{
    "error": {
        "code": "UserError",
        "message": "relative_root: 'mapped_data/key_reference/' is not part of the file_path: '/mnt/azureml/cr/j/62ca6bc190614affb42ed740d10f41f0/cap/data-capability/wd/INPUT_step1a_key_output_temp_workspaceblobstore/azureml/4ff6276a-2c72-4d0d-a361-242b6de4eb09/step1a_key_output_temp/key_reference_output.parquet'."
    }
}

üîÑ [Processing file]: part-00000-0042fc95-a813-4a78-a5ea-9d565974a2b5-c000.snappy.parquet
[‚úÖ] Final output saved to: /mnt/azureml/cr/j/62ca6bc190614affb42ed740d10f41f0/cap/data-capability/wd/INPUT_step1a_final_output_workspaceblobstore/azureml/4ff6276a-2c72-4d0d-a361-242b6de4eb09/step1a_final_output/mapped_part-00000-0042fc95-a813-4a78-a5ea-9d565974a2b5-c000.snappy.parquet
[INFO] Registering final mapped output from: /mnt/azureml/cr/j/62ca6bc190614affb42ed740d10f41f0/cap/data-capability/wd/INPUT_step1a_final_output_workspaceblobstore/azureml/4ff6276a-2c72-4d0d-a361-242b6de4eb09/step1a_final_output
Cleaning up all outstanding Run operations, waiting 300.0 seconds
1 items cleaning up...
Cleanup took 0.1511058807373047 seconds
Traceback (most recent call last):
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py", line 66, in _validate_has_data
    dataflow.verify_has_data()
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/_loggerfactory.py", line 273, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/dataflow.py", line 947, in verify_has_data
    if len(self.take(1)._to_pyrecords()) == 0:
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/dataflow.py", line 853, in _to_pyrecords
    intermediate_files = _write_preppy('Dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context()), allow_fallback_to_clex=allow_fallback_to_clex, force_clex=force_clex)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py", line 301, in _write_preppy
    raise e
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py", line 290, in _write_preppy
    _execute(
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py", line 631, in _execute
    raise e
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py", line 606, in _execute
    return clex_execute()
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/_dataframereader.py", line 439, in clex_execute
    dataflow_for_execution._engine_api.execute_anonymous_activity(
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/_aml_helper.py", line 44, in wrapper
    return send_message_func(op_code, message, cancellation_token)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/api.py", line 159, in execute_anonymous_activity
    response = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/engine.py", line 291, in send_message
    raise_engine_error(response['error'])
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/dataprep/api/errorhandlers.py", line 10, in raise_engine_error
    raise ExecutionError(error_response)
azureml.dataprep.api.errorhandlers.ExecutionError: 
Error Code: ScriptExecution.StreamAccess.NotFound
Failed Step: cce7f78e-d94b-4007-8b91-fb5a5fff1743
Error Message: ScriptExecutionException was caused by StreamAccessException.
  StreamAccessException was caused by NotFoundException.
    Found no resources for the input provided: 'https://blobdtoamlmarmot.blob.core.windows.net/azureml-blobstore-975b0280-8ce5-42df-bdeb-51637f5ddbc7/mnt/azureml/cr/j/62ca6bc190614affb42ed740d10f41f0/cap/data-capability/wd/INPUT_step1a_final_output_workspaceblobstore/azureml/4ff6276a-2c72-4d0d-a361-242b6de4eb09/step1a_final_output/*.parquet'
| session_id=053affae-d353-415d-9db8-a0aea73667d7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "step_1a_extract_and_merge.py", line 215, in <module>
    main()
  File "step_1a_extract_and_merge.py", line 211, in main
    register_final_output_dir(args.final_output, ws)
  File "step_1a_extract_and_merge.py", line 112, in register_final_output_dir
    dataset = Dataset.Tabular.from_parquet_files([(datastore, os.path.join(output_dir, "*.parquet"))])
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/data/_loggerfactory.py", line 132, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/data/dataset_factory.py", line 142, in from_parquet_files
    return TabularDatasetFactory._from_parquet_files(path,
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/data/dataset_factory.py", line 156, in _from_parquet_files
    dataflow = _transform_and_validate(
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/data/dataset_factory.py", line 1173, in _transform_and_validate
    _validate_has_data(dataflow, 'Failed to validate the data.')
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/azureml/data/dataset_error_handling.py", line 69, in _validate_has_data
    raise DatasetValidationError(error_message + '\n' + e.compliant_message, exception=e)
azureml.data.dataset_error_handling.DatasetValidationError: DatasetValidationError:
	Message: Failed to validate the data.
ScriptExecutionException was caused by StreamAccessException.
  StreamAccessException was caused by NotFoundException.
    Found no resources for the input provided: '[REDACTED]'
| session_id=053affae-d353-415d-9db8-a0aea73667d7
	InnerException None
	ErrorResponse 
{
    "error": {
        "code": "UserError",
        "message": "Failed to validate the data.\nScriptExecutionException was caused by StreamAccessException.\n  StreamAccessException was caused by NotFoundException.\n    Found no resources for the input provided: '[REDACTED]'\n| session_id=053affae-d353-415d-9db8-a0aea73667d7"
    }
}

