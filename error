from azureml.core import Workspace, Dataset, Datastore
import pandas as pd
import os
from datetime import datetime

# === Azure ML Workspace Config ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"
DATASTORE_NAME = "xbomrefadlsg2"

# === Date and paths ===
today = datetime.today().strftime("%d%m%Y")
input_path = f"./data/final_merged_predictions"
output_path = f"./data/final_merged_predictions_updated"
blob_output_path = f"hbom_category_prediction/hbom_category_prediction_{today}_updated/"

os.makedirs(input_path, exist_ok=True)
os.makedirs(output_path, exist_ok=True)

# === Load both mapping CSVs ===
cat_map = pd.read_csv("./data/cmpnt_cat_category_mapping.csv",encoding='latin1')        # CMPNT_CAT_CD_DESC ‚Üí AI_FINAL_CATEGORY
subcat_map = pd.read_csv("./data/cmpnt_cat_subcategory_mapping.csv",encoding='latin1')  # CMPNT_CAT_CD_DESC ‚Üí AI_FINAL_SUBCATEGORY

# === Step 1: Download original dataset ===
#dataset.download(target_path=input_path, overwrite=True)
#print(f"üì• Downloaded dataset to: {input_path}")

# === Step 2: Process and update ===
for file in os.listdir(input_path):
    if file.endswith(".parquet"):
        input_file = os.path.join(input_path, file)
        output_file = os.path.join(output_path, file)

        df = pd.read_parquet(input_file)

        # Merge category mapping
        df = df.merge(cat_map, on="CMPNT_CAT_CD_DESC", how="left", suffixes=("", "_cat_mapped"))
        # Merge subcategory mapping
        df = df.merge(subcat_map, on="CMPNT_CAT_CD_DESC", how="left", suffixes=("", "_subcat_mapped"))

        # === CATEGORY UPDATE LOGIC ===
        mask_cat = df["AI_FINAL_CATEGORY"].isna() & df["AI_FINAL_CATEGORY_cat_mapped"].notna()
        print(f"üîç {file} ‚Üí {mask_cat.sum()} rows updated for CATEGORY.")

        df.loc[mask_cat, "AI_FINAL_CATEGORY"] = df.loc[mask_cat, "AI_FINAL_CATEGORY_cat_mapped"]
        df.loc[mask_cat, "AI_MATCHING_REASON_FINAL_CATEGORY"] = "direct_mapping_lookup"
        df.loc[mask_cat, "AI_FINAL_CATEGORY_CONFIDENCE"] = 1

        # === SUBCATEGORY UPDATE LOGIC ===
        mask_subcat = df["AI_FINAL_SUBCATEGORY"].isna() & df["AI_FINAL_SUBCATEGORY_subcat_mapped"].notna()
        print(f"üîç {file} ‚Üí {mask_subcat.sum()} rows updated for SUBCATEGORY.")

        df.loc[mask_subcat, "AI_FINAL_SUBCATEGORY"] = df.loc[mask_subcat, "AI_FINAL_SUBCATEGORY_subcat_mapped"]
        df.loc[mask_subcat, "AI_MATCHING_REASON_FINAL_SUBCATEGORY"] = "direct_mapping_lookup"
        df.loc[mask_subcat, "AI_FINAL_SUBCATEGORY_CONFIDENCE"] = 1

        # Drop helper columns
        df.drop(columns=["AI_FINAL_CATEGORY_cat_mapped", "AI_FINAL_SUBCATEGORY_subcat_mapped"], errors='ignore', inplace=True)

        df.to_parquet(output_file, index=False)
        print(f"‚úÖ Updated & saved: {file}")

# === Step 3: Upload back to Azure ===
print(f"‚è´ Uploading updated files to: {blob_output_path}")
ds.upload(
    src_dir=output_path,
    target_path=blob_output_path,
    overwrite=True,
    show_progress=True
)

print(f"üéâ All files uploaded to Azure at: {blob_output_path}")
