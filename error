pip install gradio moviepy langdetect faster-whisper pydub requests deep-translator


import gradio as gr
import moviepy.editor as mp
import os
import uuid
import tempfile
from langdetect import detect
from faster_whisper import WhisperModel
from pydub import AudioSegment
import subprocess
import json
import requests
import time

# Azure TTS Config
SPEECH_ENDPOINT = "https://<your-region>.tts.speech.microsoft.com/"
SUBSCRIPTION_KEY = "<your-subscription-key>"
API_VERSION = "2024-04-01"
VOICE = "en-US-AriaNeural"

# Whisper model
model = WhisperModel("base", compute_type="float32")

def extract_audio(video_path):
    audio_path = tempfile.mktemp(suffix=".wav")
    clip = mp.VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path)
    return audio_path

def detect_language(audio_path):
    segments, info = model.transcribe(audio_path, beam_size=5)
    return info["language"]

def transcribe_and_diarize(audio_path):
    segments, info = model.transcribe(audio_path, beam_size=5, word_timestamps=True)
    diarized_text = []
    for seg in segments:
        diarized_text.append((seg.start, seg.end, seg.text))
    return diarized_text, info['language']

def translate_text(text, target_lang):
    from deep_translator import GoogleTranslator
    return GoogleTranslator(source='auto', target=target_lang).translate(text)

def synthesize_with_azure(text, voice=VOICE):
    job_id = str(uuid.uuid4())
    url = f'{SPEECH_ENDPOINT}/texttospeech/batchsyntheses/{job_id}?api-version={API_VERSION}'
    headers = {
        'Content-Type': 'application/json; charset=utf-8',
        'Ocp-Apim-Subscription-Key': SUBSCRIPTION_KEY
    }
    payload = {
        "displayName": "GradioBatchSynthesis",
        "description": "Synth from Gradio app",
        "inputKind": "PlainText",
        "synthesisConfig": {
            "voice": voice,
        },
        "inputs": [{"content": text}],
        "properties": {
            "outputFormat": "audio-24khz-160kbitrate-mono-mp3",
            "wordBoundaryEnabled": False,
            "sentenceBoundaryEnabled": False,
            "timeToLiveInHours": 1
        }
    }
    response = requests.put(url, headers=headers, json=payload)
    if response.status_code >= 400:
        raise Exception(f"Azure TTS submission failed: {response.text}")

    while True:
        check_url = f'{SPEECH_ENDPOINT}/texttospeech/batchsyntheses/{job_id}?api-version={API_VERSION}'
        status_resp = requests.get(check_url, headers={'Ocp-Apim-Subscription-Key': SUBSCRIPTION_KEY})
        status = status_resp.json().get("status")
        if status == "Succeeded":
            result_url = status_resp.json()['outputs']['result']
            break
        elif status == "Failed":
            raise Exception("Azure TTS job failed")
        time.sleep(2)

    audio_response = requests.get(result_url)
    output_audio = tempfile.mktemp(suffix=".mp3")
    with open(output_audio, 'wb') as f:
        f.write(audio_response.content)
    return output_audio

def synthesize_audio(translated_segments, target_lang):
    combined = AudioSegment.silent(duration=0)
    for start, end, text in translated_segments:
        audio_path = synthesize_with_azure(text)
        seg = AudioSegment.from_file(audio_path)
        combined += seg
    output_audio = tempfile.mktemp(suffix=".mp3")
    combined.export(output_audio, format="mp3")
    return output_audio

def combine_audio_video(original_video, new_audio):
    output_path = tempfile.mktemp(suffix=".mp4")
    command = [
        'ffmpeg', '-y', '-i', original_video, '-i', new_audio,
        '-c:v', 'copy', '-map', '0:v:0', '-map', '1:a:0', '-shortest', output_path
    ]
    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return output_path

def full_pipeline(video, target_language):
    temp_video_path = video
    audio_path = extract_audio(temp_video_path)
    diarized_segments, source_lang = transcribe_and_diarize(audio_path)
    translated_segments = [(s, e, translate_text(t, target_language)) for s, e, t in diarized_segments]
    new_audio = synthesize_audio(translated_segments, target_language)
    final_video = combine_audio_video(temp_video_path, new_audio)
    return final_video

iface = gr.Interface(
    fn=full_pipeline,
    inputs=[
        gr.Video(label="Upload your video"),
        gr.Dropdown(label="Select Target Language", choices=["en", "fr", "de", "es", "hi", "ar", "ja", "zh"])
    ],
    outputs=gr.Video(label="Translated Video Output"),
    title="Multilingual Video Translator with Speaker Diarization using Azure TTS"
)

if __name__ == '__main__':
    iface.launch()
