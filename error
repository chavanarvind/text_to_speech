# Updated pipeline_runner.py with separation of Step 5 (Inference) and Step 6 (Finalization)

from azureml.core import Workspace, Environment, Experiment
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data import OutputFileDatasetConfig
from azureml.core import Dataset, Datastore
from azureml.core.authentication import ServicePrincipalAuthentication
from datetime import datetime
import os
import sys

# === Config ===
today = datetime.today().strftime("%d%m%Y")
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"
COMPUTE_NAME = "llm-gpu-cluster-2"
ENV_NAME = "Bom_X_Evaluator"
DATASTORE_NAME = "xbomrefadlsg2"

# === Workspace and Datastore ===
def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        sp_auth = ServicePrincipalAuthentication(
            tenant_id=args[1], client_id=args[2], client_secret=args[3]
        )
        return Workspace(SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME, auth=sp_auth)
    else:
        return Workspace(SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)

ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
default_ds = Datastore.get(ws, DATASTORE_NAME)
run_config = RunConfiguration()
run_config.target = COMPUTE_NAME
run_config.environment = Environment.get(workspace=ws, name=ENV_NAME)

# === Pipeline Outputs ===
data_step1_out = PipelineData("data_step1_out", is_directory=True)
data_step1a_mapped = PipelineData("data_step1a_mapped", is_directory=True)
data_step1a_unmapped = PipelineData("data_step1a_unmapped", is_directory=True)
data_step2_out = PipelineData("data_step2_out", is_directory=True)
data_step3_out = PipelineData("data_step3_out", is_directory=True)
data_step4_out = PipelineData("data_step4_out", is_directory=True)
step1a_key_output_temp = OutputFileDatasetConfig(name='step1a_key_output_temp')
inference_output_per_file = OutputFileDatasetConfig(
    name='inference_file_outputs',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/inference_per_file/")
)
final_merged_output = OutputFileDatasetConfig(
    name='final_merged_predictions',
    destination=(default_ds, f"hbom_category_prediction/final_output_{today}/")
)

# === Upload reference files ===
def upload_ref_csv(local, target):
    default_ds.upload_files(
        files=[local],
        target_path=target,
        overwrite=True,
        show_progress=True
    )
upload_ref_csv('./local_data/high_conf_direct_mapping.csv', 'high_conf_map')
upload_ref_csv('./local_data/abbreviation_expension_updated.csv', 'abbrev_map')

high_conf_mount = Dataset.File.from_files((default_ds, 'high_conf_map')).as_named_input('high_conf_map').as_mount()
abbrev_mount = Dataset.File.from_files((default_ds, 'abbrev_map')).as_named_input('abbrev_map').as_mount()

# === Define Steps ===
data_pull_step = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", data_step1_out],
    outputs=[data_step1_out],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

high_conf_merge_step = PythonScriptStep(
    name="Step 1a - High Confidence Merge",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    inputs=[data_step1_out, high_conf_mount],
    arguments=["--input_path", data_step1_out,
               "--mapping_csv", high_conf_mount,
               "--mapped_output", data_step1a_mapped,
               "--needs_model_output", data_step1a_unmapped,
               "--key_output", step1a_key_output_temp],
    outputs=[data_step1a_mapped, data_step1a_unmapped, step1a_key_output_temp],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

abbrev_expand_step = PythonScriptStep(
    name="Step 2 - Expand Abbreviations",
    script_name="step_3_expand_abbreviation.py",
    source_directory="scripts",
    inputs=[data_step1a_unmapped, abbrev_mount],
    arguments=["--input_path", data_step1a_unmapped,
               "--abbrev_map", abbrev_mount,
               "--output_path", data_step2_out],
    outputs=[data_step2_out],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

clean_text_step = PythonScriptStep(
    name="Step 3 - Text Cleaning",
    script_name="step_2_clean_text.py",
    source_directory="scripts",
    inputs=[data_step2_out],
    arguments=["--input_path", data_step2_out, "--output_path", data_step3_out],
    outputs=[data_step3_out],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

feature_eng_step = PythonScriptStep(
    name="Step 4 - Feature Engineering",
    script_name="step_4_feature_engineering.py",
    source_directory="scripts",
    inputs=[data_step3_out],
    arguments=["--input_path", data_step3_out, "--output_path", data_step4_out],
    outputs=[data_step4_out],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

inference_step = PythonScriptStep(
    name="Step 5 - Inference per file",
    script_name="step_5_run_inference_per_file.py",
    source_directory="scripts",
    inputs=[data_step4_out, data_step1a_mapped, step1a_key_output_temp],
    arguments=["--input_path", data_step4_out,
               "--additional_mapped_dir", data_step1a_mapped,
               "--key_output_dir", step1a_key_output_temp,
               "--final_output_dir", inference_output_per_file],
    outputs=[inference_output_per_file],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

finalize_step = PythonScriptStep(
    name="Step 6 - Finalize Output",
    script_name="step_6_finalize_output.py",
    source_directory="scripts",
    inputs=[inference_output_per_file, step1a_key_output_temp],
    arguments=["--inference_output_dir", inference_output_per_file,
               "--key_output_dir", step1a_key_output_temp,
               "--final_merged_output_dir", final_merged_output],
    outputs=[final_merged_output],
    runconfig=run_config,
    compute_target=COMPUTE_NAME,
    allow_reuse=False
)

# === Pipeline and Run ===
pipeline = Pipeline(workspace=ws, steps=[
    data_pull_step,
    high_conf_merge_step,
    abbrev_expand_step,
    clean_text_step,
    feature_eng_step,
    inference_step,
    finalize_step
])

pipeline.validate()
experiment = Experiment(ws, "full_inference_pipeline_v2")
pipeline_run = experiment.submit(pipeline)
pipeline_run.wait_for_completion(show_output=True)
