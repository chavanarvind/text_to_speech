# pipeline_runner_updated.py

from azureml.core import Workspace, Environment, Experiment
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data.datapath import DataPath
from azureml.data import OutputFileDatasetConfig
from azureml.core import Dataset
from azureml.core.authentication import ServicePrincipalAuthentication
import os
import sys
from datetime import datetime

# === Load workspace ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"

def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        tenant_id = args[1]
        client_id = args[2]
        client_secret = args[3]
        sp_auth = ServicePrincipalAuthentication(
            tenant_id=tenant_id,
            service_principal_id=client_id,
            service_principal_password=client_secret
        )
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME,
            auth=sp_auth
        )
    else:
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME
        )

# Initialize workspace
ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
compute_name = "llm-gpu-cluster-2"
env = Environment.get(workspace=ws, name="Bom_X_Evaluator")
default_ds = ws.datastores["xbomrefadlsg2"]

run_config = RunConfiguration()
run_config.target = compute_name
run_config.environment = env

today = datetime.today().strftime("%d%m%Y")

# Output folders

# Intermediate outputs
step1a_key_output_temp = OutputFileDatasetConfig(name='step1a_key_output_temp')

# === Step 5 Output: Raw inference per file ===
final_output_to_adls = OutputFileDatasetConfig(  # <- used in step 5
    name='final_predictions_to_adls',
    destination=(default_ds, f"hbom_category_prediction_inference_per_file/hbom_category_prediction_{today}/")
)

# === Step 6 Output: Final merged and deduplicated predictions ===
final_merged_output = OutputFileDatasetConfig(  # <- used in step 6
    name='final_merged_predictions',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)
# === Upload high_conf_mapping.csv from local folder to datastore ===
default_ds1 = ws.get_default_datastore()

# Upload file as before
default_ds1.upload_files(
    files=['./local_data/high_conf_direct_mapping.csv'],
    target_path='high_conf_map',   # <== IMPORTANT: Folder name
    overwrite=True,
    show_progress=True
)

# Register or create file dataset
high_conf_dataset = Dataset.File.from_files((default_ds1, 'high_conf_map'))
high_conf_mount = high_conf_dataset.as_named_input('high_conf_map').as_mount()

default_ds1.upload_files(
    files=['./local_data/abbreviation_expension_updated.csv'],
    target_path='abbrev_map',
    overwrite=True,
    show_progress=True
)
abbrev_dataset = Dataset.File.from_files((default_ds1, 'abbrev_map'))
abbrev_mount = abbrev_dataset.as_named_input('abbrev_map').as_mount()
# PipelineData for steps
step1_out = PipelineData(name="data_step1_out", is_directory=True)
step1a_mapped = PipelineData(name="data_step1a_mapped", is_directory=True)
step1a_unmapped = PipelineData(name="data_step1a_unmapped", is_directory=True)
step2_out = PipelineData(name="data_step2_out", is_directory=True)
step3_out = PipelineData(name="data_step3_out", is_directory=True)
step4_out = PipelineData(name="data_step4_out", is_directory=True)

# Steps
step1 = PythonScriptStep(
    name="Step 1 - Pull Dataset",
    script_name="step_1_data_pull.py",
    source_directory="scripts",
    arguments=["--output_path", step1_out],
    outputs=[step1_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

step1a = PythonScriptStep(
    name="Step 1a - High Confidence Merge",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    inputs=[step1_out, high_conf_mount],  # 
    arguments=[
        "--input_path", step1_out,
        "--mapping_csv", high_conf_mount,  # 
        "--mapped_output", step1a_mapped,
        "--needs_model_output", step1a_unmapped,
        "--key_output", step1a_key_output_temp
    ],
    outputs=[step1a_mapped, step1a_unmapped, step1a_key_output_temp],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

step2 = PythonScriptStep(
    name="Step 2 - Text Cleaning",
    script_name="step_2_clean_text.py",
    source_directory="scripts",
    inputs=[step2_out],
    arguments=["--input_path", step2_out, "--output_path", step3_out],
    outputs=[step3_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

step3 = PythonScriptStep(
    name="Step 3 - Expand Abbreviations",
    script_name="step_3_expand_abbreviation.py",
    source_directory="scripts",
    inputs=[step1a_unmapped, abbrev_mount],  #
    arguments=[
        "--input_path", step1a_unmapped,
        "--abbrev_map", abbrev_mount,  # 
        "--output_path", step2_out
    ],
    outputs=[step2_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

step4 = PythonScriptStep(
    name="Step 4 - Feature Engineering",
    script_name="step_4_feature_engineering.py",
    source_directory="scripts",
    inputs=[step3_out],
    arguments=["--input_path", step3_out, "--output_path", step4_out],
    outputs=[step4_out],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

step5 = PythonScriptStep(
    name="Step 5 - Run Inference",
    script_name="step_5_run_inference_per_file.py",
    source_directory="scripts",
    inputs=[step4_out, step1a_mapped, step1a_key_output_temp],
    arguments=[
        "--input_path", step4_out,
        "--additional_mapped_dir", step1a_mapped,
        "--key_output_dir", step1a_key_output_temp,
        "--final_output_dir", final_output_to_adls
    ],
    outputs=[final_output_to_adls],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

step6 = PythonScriptStep(
    name="Step 6 - Finalize Output",
    script_name="step_6_finalize_output.py",
    source_directory="scripts",
    inputs=[final_output_to_adls, step1a_key_output_temp],
    arguments=[
        "--inference_output_dir", final_output_to_adls,
        "--key_output_dir", step1a_key_output_temp,
        "--final_output_dir", final_merged_output
    ],
    outputs=[final_merged_output],
    runconfig=run_config,
    compute_target=compute_name,
    allow_reuse=False
)

# Build pipeline
pipeline = Pipeline(workspace=ws, steps=[step1, step1a, step3, step2, step4, step5, step6])
pipeline.validate()

# Submit pipeline
experiment = Experiment(ws, "full_inference_pipeline_split")
pipeline_run = experiment.submit(pipeline)
pipeline_run.wait_for_completion(show_output=True)
