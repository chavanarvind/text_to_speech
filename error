import os
import re
import glob
import pandas as pd
from multiprocessing import Pool, cpu_count
from functools import partial

# --- Load abbreviation map ---
abbrev_csv_path = './data/abbreviation_expension_updated.csv'
abbrev_df = pd.read_csv(abbrev_csv_path)
abbrev_map = {k.lower(): v for k, v in zip(abbrev_df['Abbreviation_list'], abbrev_df['Abbreviation_Expension'])}
abbrev_pattern = re.compile(r'\b(' + '|'.join(re.escape(k) for k in abbrev_map.keys()) + r')\b', flags=re.IGNORECASE)

# --- Abbreviation replacement ---
def expand_abbreviations(text):
    return abbrev_pattern.sub(lambda m: abbrev_map.get(m.group(0).lower(), m.group(0)), text)

# --- Precompiled regex patterns ---
patterns = {
    'non_alphanumeric': re.compile(r'[^A-Za-z0-9&% ]+'),
    'percent_space': re.compile(r"\s*%\s*"),
    'remove_canada': re.compile(r'canada\s*\d*|can\s*\d*|ca\s*\d*|ca$|can$|ca\s'),
    'units': re.compile(r"(\D)(\d+)(\s*)(ml|l|gr|gm|g|ct)"),
    'spf_space': re.compile(r"(\s)(spf)\s*([\d+])"),
    'units_no_space': re.compile(r'(\d+)\s*(ml|l|gr|gm|g|ct)(?: |$)'),
    'spf_number': re.compile(r"(\D)(spf\d+)")
}

# --- Regex cleaning ---
def clean_series(series):
    return (series.str.lower()
        .str.replace(patterns['non_alphanumeric'], '', regex=True)
        .str.replace(patterns['percent_space'], '% ', regex=True)
        .str.replace(patterns['remove_canada'], '', regex=True)
        .str.replace(patterns['units'], r'\1 \2\3\4 ', regex=True)
        .str.replace(patterns['spf_space'], r'\1\2\3', regex=True)
        .str.replace(patterns['units_no_space'], lambda z: z.group().replace(" ", ""), regex=True)
        .str.replace(patterns['spf_number'], r'\1 \2 ', regex=True)
        .str.replace(r'\b\d{5,}\b$', '', regex=True)  # remove long trailing numbers
        .str.strip()
    )

# --- Combined processing step ---
def process_file(file, input_path, output_path):
    try:
        df = pd.read_parquet(file)

        # Filter only rows with non-null Final Category
        df = df[df['Final Category'].notna()]
        if df.empty:
            print(f"⚠️ Skipped (no valid rows): {os.path.basename(file)}")
            return

        # Step 1: Expand abbreviations
        df['MATL_SHRT_DESC'] = df['MATL_SHRT_DESC'].fillna('').apply(expand_abbreviations)
        df['CMPNT_MATL_DESC'] = df['CMPNT_MATL_DESC'].fillna('').apply(expand_abbreviations)

        # Step 2: Apply cleaning regex
        df['MATL_SHRT_DESC'] = clean_series(df['MATL_SHRT_DESC'])
        df['CMPNT_MATL_DESC'] = clean_series(df['CMPNT_MATL_DESC'])

        # Combine cleaned columns
        df['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'] = (
            df['MATL_SHRT_DESC'] + ' ' + df['CMPNT_MATL_DESC']
        ).str.strip()

        # Save cleaned output
        rel_path = os.path.relpath(file, input_path)
        output_file = os.path.join(output_path, rel_path)
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df.to_parquet(output_file, index=False)

        print(f"✅ Done: {os.path.basename(file)}")

    except Exception as e:
        print(f"❌ Failed: {os.path.basename(file)} -> {e}")

# --- Run in parallel ---
if __name__ == '__main__':
    input_path = './data/target_map_parquet_files'
    output_path = './data/target_map_cleaned_non_null_target'

    all_files = glob.glob(os.path.join(input_path, '*.parquet'))
    worker_func = partial(process_file, input_path=input_path, output_path=output_path)

    with Pool(processes=cpu_count()) as pool:
        pool.map(worker_func, all_files)
