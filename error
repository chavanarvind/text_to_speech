import os
import glob
import re
import pandas as pd
import spacy
from multiprocessing import Pool, cpu_count

# Load spaCy model globally (each process loads separately)
nlp = spacy.load("en_core_web_sm")

# RULE_HINT logic using spaCy (batched via nlp.pipe)
def compute_rule_hint_column(series):
    hints = []
    for doc in nlp.pipe(series.fillna('').astype(str), disable=["ner", "parser", "tagger"]):
        lemmas = set(token.lemma_ for token in doc)
        if lemmas & {'api', 'active'}:
            hints.append('API')
        elif lemmas & {'cream', 'gel', 'emulsion', 'ointment', 'moisturiser'}:
            hints.append('Liquids and Creams')
        elif lemmas & {'bottle', 'cap', 'carton', 'label', 'cotton', 'lotion', 'mouthwash', 'pkg', 'pack', 'karton', 'package'}:
            hints.append('PKG')
        elif lemmas & {'excipient', 'binder', 'bind', 'filler', 'fill', 'lubricant', 'lubricate', 'acid'}:
            hints.append('CHM')
        else:
            hints.append(None)
    return pd.Series(hints, index=series.index)

# Flat unit group dictionary for fast mapping
unit_group_map = {
    'KG': 'CHM', 'KGS': 'CHM', 'KGA': 'CHM', 'KGW': 'CHM', 'G': 'CHM', 'GR': 'CHM', 'GM': 'CHM', 'MG': 'CHM',
    'LB': 'CHM', 'LBS': 'CHM', 'OZ': 'CHM', 'OZA': 'CHM', 'GW': 'CHM', 'TON': 'CHM', 'DR': 'CHM',
    'L': 'Liquid', 'LT': 'Liquid', 'ML': 'Liquid', 'CC': 'Liquid', 'CL': 'Liquid', 'CCM': 'Liquid', 'GLL': 'Liquid',
    'EA': 'Discrete', 'PC': 'Discrete', 'PCS': 'Discrete', 'Pcs': 'Discrete', 'PKT': 'Discrete', 'PK': 'Discrete',
    'PAK': 'Discrete', 'PCK': 'Discrete', 'CS': 'Discrete', 'CSE': 'Discrete', 'CT': 'Discrete', 'CA': 'Discrete',
    'ST': 'Discrete', 'GRO': 'Discrete', 'BX': 'Discrete',
    'BOT': 'Containers', 'BOTTLE': 'Containers', 'ROLL': 'Containers', 'ROL': 'Containers', 'REEL': 'Containers',
    'KAR': 'Containers',
    'FT': 'Dimensional', 'YD': 'Dimensional', 'KM': 'Dimensional', 'DM': 'Dimensional', 'M': 'Dimensional',
    'M1': 'Dimensional', 'M2': 'Dimensional', 'KM2': 'Dimensional', 'YD2': 'Dimensional', 'FT3': 'Dimensional',
    'SQM': 'Dimensional', 'sqm': 'Dimensional', 'MYD': 'Dimensional', 'MI': 'Dimensional', 'SM': 'Dimensional',
    'LM': 'Dimensional', 'LF': 'Dimensional', 'MH': 'Dimensional', 'KN': 'Dimensional', 'CH': 'Dimensional',
    'TH': 'Unclassified', 'THU': 'Unclassified', 'IM': 'Unclassified', 'NOS': 'Unclassified', 'NO': 'Unclassified',
    'TS': 'Unclassified', 'KA': 'Unclassified', 'ZPC': 'Unclassified', 'ZCT': 'Unclassified', '0%': 'Unclassified',
    'KP': 'Unclassified', 'GP': 'Unclassified', 'KAI': 'Unclassified', 'SY': 'Unclassified', 'UN': 'Unclassified',
    'MU': 'Unclassified', 'UM': 'Unclassified', 'HU': 'Unclassified'
}

# Process a single file
def process_file(file):
    try:
        df = pd.read_parquet(file)

        # Clean CMPNT_CAT_CD_DESC into CMPNT_CAT_CLEAN
        df['CMPNT_CAT_CLEAN'] = (
            df['CMPNT_CAT_CD_DESC']
            .fillna('')
            .str.lower()
            .str.replace(r'[^a-z0-9\s]', ' ', regex=True)
            .str.replace(r'\s+', ' ', regex=True)
            .str.strip()
            .replace('', pd.NA)
        )

        # Combine with MATL_SHRT_DESC_AND_CMPNT_MATL_DESC for richer RULE_HINT
        combined_text = (
            df['CMPNT_CAT_CLEAN'].fillna('') + ' ' +
            df['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'].fillna('')
        ).str.strip()

        # Apply RULE_HINT
        df['RULE_HINT'] = compute_rule_hint_column(combined_text)

        # Assign UNIT_GROUP
        df['UNIT_GROUP'] = (
            df['CMPNT_UOM_CD']
            .fillna('')
            .str.upper()
            .map(unit_group_map)
            .fillna('Unclassified')
        )

        # Save to new folder
        output_file = file.replace("target_map_cleaned", "target_map_with_rules")
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df.to_parquet(output_file, index=False)

        print(f"✅ Done: {os.path.basename(file)}")
    except Exception as e:
        print(f"❌ Failed: {os.path.basename(file)} -> {e}")

# Main: run in parallel
if __name__ == '__main__':
    input_path = './data/target_map_cleaned'
    all_files = glob.glob(os.path.join(input_path, '*.parquet'))

    with Pool(processes=cpu_count()) as pool:
        pool.map(process_file, all_files)



/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
/anaconda/envs/azureml_py38/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.
  warnings.warn(Warnings.W108)
