import pandas as pd
import spacy
from collections import Counter
from itertools import islice
from pathlib import Path
import os
import time
from multiprocessing import Pool, cpu_count

# Enable tagger for proper lemmatization
nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])

input_path = './data/target_map_cleaned'
output_path = Path('./data/gram_outputs/frequent_terms_by_category')
output_path.mkdir(parents=True, exist_ok=True)

required_cols = ['Final Category', 'MATL_SHRT_DESC_AND_CMPNT_MATL_DESC']
threshold = 10

def process_category(category, descriptions, threshold):
    descriptions = [desc for desc in descriptions if desc]

    # Unigram frequency from unique lemmas per row
    unique_lemmas = []
    for doc in nlp.pipe(descriptions, batch_size=100):
        lemmas = {token.lemma_ for token in doc if token.is_alpha and not token.is_stop}
        unique_lemmas.extend(lemmas)
    unigram_counts = Counter(unique_lemmas)
    unigrams = [
        {'category': category, 'word': word, 'count': count}
        for word, count in unigram_counts.items() if count >= threshold
    ]

    # Bigram frequency (repetition allowed)
    bigrams_list = []
    for doc in nlp.pipe(descriptions, batch_size=100):
        words = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]
        bigrams_list.extend(zip(words, islice(words, 1, None)))
    bigram_counts = Counter(bigrams_list)
    bigrams = [
        {'category': category, 'bigram': f'{w1} {w2}', 'count': count}
        for (w1, w2), count in bigram_counts.items() if count >= threshold
    ]

    return unigrams, bigrams

def process_file(file_path_str):
    file_path = Path(file_path_str)
    start_time = time.time()
    try:
        df = pd.read_parquet(file_path, columns=required_cols)
        df = df[df['Final Category'].notna()]
        df = df.dropna(subset=['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'])

        file_unigrams = []
        file_bigrams = []

        for category, group in df.groupby('Final Category'):
            descriptions = group['MATL_SHRT_DESC_AND_CMPNT_MATL_DESC'].astype(str).tolist()
            unigrams, bigrams = process_category(category, descriptions, threshold)
            file_unigrams.extend(unigrams)
            file_bigrams.extend(bigrams)

        print(f"‚úÖ Processed {file_path.name} in {time.time() - start_time:.2f}s")
        return file_unigrams, file_bigrams

    except Exception as e:
        print(f"‚ùå Failed on {file_path.name}: {e}")
        return [], []

# Run multiprocessing
if __name__ == '__main__':
    all_files = [str(f) for f in Path(input_path).glob("*.parquet")]
    print(f"üìÅ Total files to process: {len(all_files)}")

    all_unigrams, all_bigrams = [], []

    with Pool(processes=cpu_count()) as pool:
        for unigrams, bigrams in pool.map(process_file, all_files):
            all_unigrams.extend(unigrams)
            all_bigrams.extend(bigrams)

    # Save to CSV
    pd.DataFrame(all_unigrams).to_csv(output_path / "all_categories_unigrams.csv", index=False)
    pd.DataFrame(all_bigrams).to_csv(output_path / "all_categories_bigrams.csv", index=False)
    print("‚úÖ Saved unigrams and bigrams.")
