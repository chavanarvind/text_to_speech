import time
import os
from typing import Union, Any

import time
from typing import Union, Any

import numpy as np
import pandas as pd
check_list = ['02109345', '02109344', '02109429', '02112146', '02111779','000000000000153931']
from cross_hound.config import ERP_COLS, TRU_COLS
from cross_hound.constants import DATA_DIR, TRU_MAT_NUM_ORIGINAL, TRU_MAT_NUM, ERP_MAT_NUM, ERP_PART_DESC, \
    TRU_SPEC_DESC, TRU_SPEC_ID, ERP_PART_ID, TRU_MAT_DESC
from cross_hound.off_specs_classifier import classify_as_off_spec
cmp_check_list = ['02109345', '02109344', '02109429', '02112146', '02111779']


def process_trend_data_from_azure(trend_df: pd.DataFrame) -> pd.DataFrame:
    # Step 1: Clean and normalize text columns
    trend_df['TRD_NM'] = trend_df['TRD_NM'].str.replace(r'\s+', ' ', regex=True).str.strip()
    trend_df['TRD_NM'] = trend_df['TRD_NM'].str.lower()
    trend_df['RM_TITLE_DESC'] = trend_df['RM_TITLE_DESC'].str.lower()

    # Step 2: Filter rows where 'TRD_NM' is not equal to 'RM_TITLE_DESC'
    trend_df = trend_df[trend_df['TRD_NM'] != trend_df['RM_TITLE_DESC']]

    # Step 3: Remove duplicates based on specific columns
    cols = ['ERP_PART_NUM', 'TRD_NM', 'CHILD_NM', 'RM_TITLE_DESC']
    trend_df_dedup = trend_df.drop_duplicates(subset=cols)
    trend_df_dedup = trend_df_dedup[trend_df_dedup['TRD_NM'].notna()]

    # Step 4: Concatenate trade names within each group
    trade_names_wide = trend_df_dedup.groupby(['ERP_PART_NUM', 'CHILD_NM'])['TRD_NM'].apply(
        lambda x: '||'.join(list(x))
    ).reset_index()
    trade_names_wide.rename(columns={'TRD_NM': 'TRD_NM_CONCAT'}, inplace=True)
    trade_names_wide.set_index(['ERP_PART_NUM', 'CHILD_NM'], inplace=True)

    # Step 5: Unify trade names by removing duplicate words
    def unify_trade_names(s: pd.Series) -> str:
        l = list(s)
        lt = []
        for x in l:
            lt.extend(str(x).split())
        lt = list(set(lt))
        lt = ' '.join(lt)
        return lt

    trade_names_wide['TRD_NM_FULL'] = trend_df_dedup.groupby(['ERP_PART_NUM', 'CHILD_NM'])['TRD_NM'].apply(
        lambda x: unify_trade_names(x)
    )
    trade_names_wide.reset_index(inplace=True)

    return trade_names_wide

def get_data_from_local(data_dir: str = DATA_DIR):
    erp = pd.read_excel(f"{data_dir}/IM_BOM_Extract.xlsx", dtype=str, keep_default_na=False, header=0,
                        engine='openpyxl')
    erp_em = pd.read_excel(f"{data_dir}/EM_BOM_Extract.xlsx", dtype=str, keep_default_na=False, header=0,
                           engine='openpyxl')
    tru = pd.read_excel(f"{data_dir}/TRU_BOM_Extract.xlsx", dtype=str, keep_default_na=False, header=0,
                        engine='openpyxl')
    trade_names = pd.read_csv('trade_names.csv')
    return erp, erp_em, tru, trade_names


def get_data_from_azure(dataset_name: Any) -> pd.DataFrame:
    assert dataset_name is not None, 'No dataset_name provided'
    from azureml.core import Run, Dataset, Workspace
    import pandas as pd
    import pyarrow
    from deltalake import DeltaTable

    run = Run.get_context()

    if run.identity.startswith('OfflineRun'):  # We are running local.
        ws = Workspace.from_config(path='../.azureml/dev_config.json')
    else:  # We are running a pipeline inside azureml
        ws = run.experiment.workspace

    try:
        dataset = Dataset.get_by_id(ws, id=dataset_name)
    except:
        try:
            dataset = Dataset.get_by_name(ws, name=dataset_name)
        except:
            print('Reading dataset: ', dataset_name)
            print('Type: ', type(dataset_name))
            dataset = dataset_name

    try:  # back compatibility
        df = dataset.to_pandas_dataframe()
        df = df.astype(str)
    except:  # This should be the default.
        data_dir = f'data_dir/{dataset_name}'
        dataset.download(target_path=data_dir, overwrite=False)

        # Load the Delta Table
        delta_table = DeltaTable(data_dir)

        # Access the schema of the Delta Table
        schema = delta_table.schema().to_pyarrow()

        # Filter out timestamp columns
        non_timestamp_columns = [
            field.name for field in schema if not isinstance(field.type, pyarrow.TimestampType)
        ]

        # Create a PyArrow Table with only non-timestamp columns
        filtered_table = delta_table.to_pyarrow_table(columns=non_timestamp_columns)

        # Convert to Pandas DataFrame
        df: pd.DataFrame = filtered_table.to_pandas()
        df = df.astype(str)

    # Adjust timestamps if needed
    #df = adjust_timestamps(df)
    #df = remove_timezone(df)

    print('#' * 20)
    print('Data Ingested: ', dataset_name)
    print('Columns: ')
    for col in df.columns:
        print(col)
        
    return df





def inferred_tru_type(tru_spec_id: str):
    if tru_spec_id.startswith('PC'):
        return 'PC'
    elif tru_spec_id.startswith('DC'):
        return 'DC'
    elif tru_spec_id.startswith('GC'):
        return 'CG'
    elif tru_spec_id.startswith('RM'):
        return 'RM'
    else:
        return 'Unknown'


def pad_zeroes(txt, length=18):
    txt = str(txt)
    padded = '0' * length + txt
    padded = padded[-length:]
    return padded


def has_leading_zeroes(txt):
    try:
        txt = str(txt)
        return txt.startswith('0')
    except:
        return False


def custom_cast(txt):
    try:
        return str(int(txt))
    except:
        return txt


def ingest_data_and_prepare_examples(data_dir: Union[str, None] = None,
                                     erp_im_name: Union[str, None] = None,
                                     erp_em_name: Union[str, None] = None,
                                     tru_name: Union[str, None] = None,
                                     trade_names_dataset_name: Union[str, None] = None,
                                     run_params: dict = None,
                                     output_dir: str = None) -> (pd.DataFrame,
                                                                 pd.DataFrame,
                                                                 pd.DataFrame,
                                                                 pd.DataFrame,
                                                                 pd.DataFrame):
    """
    Read the data sources, get rid of duplicates and harmonise both EM and IM ERP data.

    """
    start = time.time()
    if data_dir:  # Assume data lives in a local directory
        erp_im, erp_em, tru, trade_names = get_data_from_local(data_dir)
    else:  # Try to fetch the data from azure
        print('Getting Data from Azure Workspace')
        erp_im = get_data_from_azure(erp_im_name)
        erp_em = get_data_from_azure(erp_em_name)
        #test
        erp_im = erp_im[erp_im["CMPNT_MATL_NUM"].isin(check_list)]

        # Save to Azure ML pipeline logs folder
        user_logs_path = os.path.join(os.getcwd(), "user_logs")
        os.makedirs(user_logs_path, exist_ok=True)
        erp_im.to_csv(os.path.join(user_logs_path, "erp_im_tracked_ids_step1_read_data.csv"), index=False)
        
        print("Initial ERP_PART_ID Presence Check:")
        print("In ERP IM:", set(cmp_check_list) & set(erp_im[ERP_PART_ID].unique()))
        print("In ERP EM:", set(cmp_check_list) & set(erp_em[ERP_PART_ID].unique()))
        tru = get_data_from_azure(tru_name)
        trade_names = get_data_from_azure(trade_names_dataset_name)
        trade_names = process_trend_data_from_azure(trade_names)

    # While reading from azure, nan values get assigned the string NA
    for df in [erp_im, erp_em, tru]:
        df.replace('NA', np.nan, inplace=True)
        df.replace('', np.nan, inplace=True)
        df.replace(' ', np.nan, inplace=True)
        df.replace('null', np.nan, inplace=True)
        df.replace('Null', np.nan, inplace=True)
        df.replace('NULL', np.nan, inplace=True)

        ### renaming the column of P360 table  ( tru table)
    # col_dir = {'ERP_PART_NUM': 'ERP_PART_NUM', 'ERP_DESC': 'ERP Description', 'RGN_CD': 'REGION',
    #            'RGN2_CD': 'Region2','ERPRegion': 'ERP Region', 'FG_NM': 'FP_Name', 'FG_SPEC_NM': 'FG Spec Name', 'FG_ST_CD': 'FG State',
    #            'FD_NM': 'FD Spec', 'FD_RVSN_NBR': 'FD Spec Rev', 'FD_TITLE_DESC': 'FD Title', 'FD_ST_CD': 'FD State',
    #            'FD_MFG_LOC_TXT': '[FD Manufacturing_Location]', 'CHILD_NM': 'RM Spec', 'CHILD_RVSN_NBR': 'RM Rev',
    #            'CHILD_TITLE_DESC': 'RM Title', 'RM_ST_CD': 'RM State', 'SEQ_NUM': 'Sequence', 'QTY_VAL': 'Quantity',
    #            'WW_VAL': 'Ww', 'UOM_TXT': 'Uofm', 'USG_CD': 'Usage', 'TEMPL_TYPE_CD': 'PC_Template'}
    #
    # tru.rename(columns=col_dir, inplace=True)
    print("Tru data size" ,tru.shape ,"tru.columns:", tru.columns , )
    print("ERP EM  data size" ,tru.shape ,"erp_em.columns:", erp_em.columns)
    print("ERP IM data size" ,tru.shape ,"erp_im.columns:", erp_im.columns)
    # Save to Azure ML pipeline logs folder
    df_check = erp_im[erp_im["CMPNT_MATL_NUM"].isin(cmp_check_list)]
    user_logs_path = os.path.join(os.getcwd(), "user_logs")
    os.makedirs(user_logs_path, exist_ok=True)
    df_check.to_csv(os.path.join(user_logs_path, "erp_im_tracked_ids_step2_assign_NA.csv"), index=False)
    ### removed before final checkout
    tru[['ERP_PART_NUM','RGN_CD']].to_csv(f'{output_dir}/tru.csv')
    erp_em[['MATL_NUM','SRC_SYS_CD']].to_csv(f'{output_dir}/erp_em.csv' )
    erp_im[['MATL_NUM','SRC_SYS_CD']].to_csv(f'{output_dir}/erp_im.csv' )

    ##
    erp_em.rename(columns={'PADDED_MATL_NUM': 'Padded_MATL_NUM'}, inplace=True)

    ## Drop leading zeroes
    print("erp_im['NEW_MATCH_MATL_NUM'].str.startswith('0').sum()")
    print(erp_im['NEW_MATCH_MATL_NUM'].str.startswith('0').sum())
    print("erp_im['MATL_NUM'].str.startswith('0').sum()")
    print(erp_im['MATL_NUM'].str.startswith('0').sum())
    print("erp_em['Padded_MATL_NUM'].str.startswith('0').sum()")
    print(erp_em['Padded_MATL_NUM'].str.startswith('0').sum())

    # Consume new columns.
    if 'Padded_MATL_NUM' in erp_em.columns:
        print('Swapping Columns in EM')
        erp_em.rename(columns={ERP_MAT_NUM: f'{ERP_MAT_NUM}_original',
                               'Padded_MATL_NUM': ERP_MAT_NUM},
                      inplace=True)

    if 'NEW_MATCH_MATL_NUM' in erp_im.columns:
        print('Swapping Columns in IM')
        # erp_im.rename(columns={ERP_MAT_NUM: f'{ERP_MAT_NUM}_original',
        #                       'NEW_MATCH_MATL_NUM': ERP_MAT_NUM},
        #              inplace=True)

        erp_im.rename(columns={'NEW_MATCH_MATL_NUM': f'{ERP_MAT_NUM}_original'},
                      inplace=True)
    # Save to Azure ML pipeline logs folder
    df_check = erp_im[erp_im["CMPNT_MATL_NUM"].isin(cmp_check_list)]
    user_logs_path = os.path.join(os.getcwd(), "user_logs")
    os.makedirs(user_logs_path, exist_ok=True)
    df_check.to_csv(os.path.join(user_logs_path, "erp_im_tracked_ids_step3_assign_NEW_COL_ADD.csv"), index=False)
        
    print("\n[STEP 2] After NULL Filter — Tracked CMPNT_MATL_NUM presence")
    df_im_step2 = erp_im[erp_im["CMPNT_MATL_NUM"].isin(cmp_check_list)][["CMPNT_MATL_NUM", "CMPNT_MATL_DESC"]]
    if not df_im_step2.empty:
       print(df_im_step2.to_markdown(index=False))
    else:
      print("All tracked CMPNT_MATL_NUMs were dropped by NULL filter in ERP IM")
    print("After column renaming / ID swap:")

    print("ERP IM - ERP_PART_IDs still present:", set(cmp_check_list) & set(erp_im[ERP_PART_ID].unique()))
    print("ERP EM - ERP_PART_IDs still present:", set(cmp_check_list) & set(erp_em[ERP_PART_ID].unique()))
    # DROP Leading Zeroes for ERP Part ID and TRU Spec Id
    erp_im[f'{ERP_PART_ID}_original'] = erp_im[ERP_PART_ID].copy()
    erp_em[f'{ERP_PART_ID}_original'] = erp_em[ERP_PART_ID].copy()
    tru[f'{TRU_SPEC_ID}_original'] = tru[TRU_SPEC_ID].copy()

    erp_im[ERP_PART_ID] = [custom_cast(x) for x in erp_im[ERP_PART_ID]]
    erp_em[ERP_PART_ID] = [custom_cast(x) for x in erp_em[ERP_PART_ID]]
    tru[TRU_SPEC_ID] = [custom_cast(x) for x in tru[TRU_SPEC_ID]]
    # Save to Azure ML pipeline logs folder
    df_check = erp_im[erp_im["CMPNT_MATL_NUM"].isin(cmp_check_list)]
    user_logs_path = os.path.join(os.getcwd(), "user_logs")
    os.makedirs(user_logs_path, exist_ok=True)
    df_check.to_csv(os.path.join(user_logs_path, "erp_im_tracked_ids_step4_remove_leading_zero.csv"), index=False)
    cmp_check_list1 = ['2109345', '2109344', '2109429', '2112146', '2111779']
    print("drop zero from ERP IM - ERP_PART_IDs:", set(cmp_check_list) & set(erp_im[ERP_PART_ID].unique()))
    print("drop zero from ERP IM with updated list:", set(cmp_check_list1) & set(erp_im[ERP_PART_ID].unique()))
    # count the number of duplicate rows
    num_duplicates_im = erp_im.duplicated(subset=['SRC_SYS_CD',
                                                  'MATL_NUM',
                                                  'MATL_SHRT_DESC',
                                                  'CMPNT_MATL_TYPE_CD',
                                                  'CMPNT_MATL_NUM',
                                                  'CMPNT_MATL_DESC']).sum()
    run_params['Number of erp im duplicate rows'] = num_duplicates_im

    num_duplicates_em = erp_em.duplicated(subset=['SRC_SYS_CD',
                                                  'MATL_NUM',
                                                  'MATL_SHRT_DESC',
                                                  'CMPNT_MATL_NUM']).sum()
    run_params['Number of erp em duplicate rows'] = num_duplicates_em

    num_duplicates_tru = tru.duplicated(subset=[TRU_MAT_NUM_ORIGINAL, 'CHILD_NM', TRU_SPEC_DESC]).sum()
    run_params['Number of tru duplicate rows'] = num_duplicates_tru

    # Collect initial data diagnosis
    run_params['erp im records'] = len(erp_im)
    run_params['erp im null text'] = erp_im[ERP_PART_DESC].isna().sum()
    run_params['erp em records'] = len(erp_em)
    run_params['erp em null text'] = erp_em['MATL_SHRT_DESC'].isna().sum()
    run_params['tru records'] = len(tru)
    run_params['tru null text'] = tru[TRU_SPEC_DESC].isna().sum()

    run_params['Unique codes in erp IM data'] = erp_im[ERP_MAT_NUM].nunique()
    run_params['Unique codes in erp EM data'] = erp_em[ERP_MAT_NUM].nunique()
    run_params['Unique codes in tru data'] = tru[TRU_MAT_NUM_ORIGINAL].nunique()
    run_params['Unique CHILD_NMs'] = tru[TRU_SPEC_ID].nunique()

    print(' ' * 60)
    print('#' * 60)
    print('Initial Data Input Diagnosis')
    print('#' * 60)
    for key, val in run_params.items():
        print(key, val)
    print('#' * 60)
    print(' ' * 60)

    # External manufacturing does not have PLNT_CD info, so we created an artificial one to make it compatible with IM
    erp_em['PLNT_CD'] = 'External' + '_' + erp_em['SRC_SYS_CD']
    erp_em['CMPNT_MATL_TYPE_CD'] = None

    # Make sure both EM and IM have the same columns for merging later
    erp_em = erp_em[ERP_COLS]

    erp_em[ERP_MAT_NUM] = erp_em[ERP_MAT_NUM].astype(str)
    erp_em = erp_em.drop_duplicates(subset=['SRC_SYS_CD',
                                            'PLNT_CD',
                                            'MATL_NUM',
                                            'MATL_SHRT_DESC',
                                            'CMPNT_MATL_TYPE_CD',
                                            'CMPNT_MATL_NUM',
                                            'CMPNT_MATL_DESC']).copy()

    erp_im[ERP_MAT_NUM] = erp_im[ERP_MAT_NUM].astype(str)
    erp_im = erp_im.drop_duplicates(subset=['SRC_SYS_CD',
                                            'PLNT_CD',
                                            'MATL_NUM',
                                            'MATL_SHRT_DESC',
                                            'CMPNT_MATL_TYPE_CD',
                                            'CMPNT_MATL_NUM',
                                            'CMPNT_MATL_DESC']).copy()

    # Drop NA's
    # Diagnostic: Check if any tracked ERP_PART_IDs are about to be dropped due to missing values
    print("Pre-Filter Diagnostic — ERP IM")
    print("Rows where ERP_PART_ID is in cmp_check_list:")
    print(erp_im[erp_im[ERP_PART_ID].isin(cmp_check_list)])
    print("\nRows where ERP_PART_ID is in cmp_check_list AND ERP_PART_DESC is missing:")
    print(erp_im[erp_im[ERP_PART_ID].isin(cmp_check_list) & erp_im[ERP_PART_DESC].isna()])
    print("\nRows where ERP_PART_ID is missing (NaN):")
    print(erp_im[erp_im[ERP_PART_ID].isna()])

    erp_im = erp_im[erp_im[ERP_PART_DESC].notna()].copy()
    erp_im = erp_im[erp_im[ERP_PART_ID].notna()].copy()

    erp_em = erp_em[erp_em[ERP_PART_DESC].notna()].copy()
    erp_em = erp_em[erp_em[ERP_PART_ID].notna()].copy()
    print("After NULL ERP_PART_DESC filter:")
    cmp_check_list1 = ['2109345', '2109344', '2109429', '2112146', '2111779']
    df_check = erp_im[erp_im["CMPNT_MATL_NUM"].isin(cmp_check_list1)]
    #user_logs_path = os.path.join(os.getcwd(), "user_logs")
    #os.makedirs(user_logs_path, exist_ok=True)
    #df_check.to_csv(os.path.join(user_logs_path, "erp_im_tracked_ids_drop_na.csv"), index=False)
    cmp_check_list1 = ['2109345', '2109344', '2109429', '2112146', '2111779']
    print("drop zero from ERP IM - ERP_PART_IDs:", set(cmp_check_list) & set(erp_im[ERP_PART_ID].unique()))
    print("drop zero from ERP IM with updated list:", set(cmp_check_list1) & set(erp_im[ERP_PART_ID].unique()))
    # count the number of duplicate rows

    print("ERP IM - ERP_PART_IDs still present:", set(cmp_check_list) & set(erp_im[ERP_PART_ID].unique()))
    print("ERP EM - ERP_PART_IDs still present:", set(cmp_check_list) & set(erp_em[ERP_PART_ID].unique()))

    # Drop 'HALB'. Only present in IM
    erp_im = erp_im[erp_im["CMPNT_MATL_TYPE_CD"] != "HALB"].reset_index(drop=True)
    print("After HALB filter in ERP IM:")
    print("HALB - Remaining ERP_PART_IDs:", set(cmp_check_list1) & set(erp_im[ERP_PART_ID].unique()))
    ################################
    # Tru processing
    ################################
    tru = tru.rename(columns={TRU_MAT_NUM_ORIGINAL: TRU_MAT_NUM})

    # Pad tru. Needed for the EM data
    tru[f'{TRU_MAT_NUM}_padded'] = [pad_zeroes(x, 18) for x in tru[TRU_MAT_NUM]]
    tru[TRU_MAT_NUM] = tru[TRU_MAT_NUM].astype(str)
    tru[f'{TRU_MAT_NUM}_original'] = tru[TRU_MAT_NUM].copy()

    ############################################################
    # Discover bundles
    ############################################################
    # Important to discover them before dropping duplicates

    pcs = tru['SPEC_FLOW_CD'].str.extract(r'(PC-\d+)', expand=False)
    dcs = tru['SPEC_FLOW_CD'].str.extract(r'(DC-\d+)', expand=False)
    bundles = pd.concat([pd.DataFrame({'p1': pcs, 'p2': dcs}), pd.DataFrame({'p1': dcs, 'p2': pcs})], ignore_index=True)
    bundles = bundles.drop_duplicates().dropna()

    run_params['Unique RM Specs duplicated'] = tru.duplicated(subset=[TRU_MAT_NUM, TRU_SPEC_ID]).sum()
    tru = tru.drop_duplicates(subset=[TRU_MAT_NUM, TRU_SPEC_ID])

    # Drop NAs
    run_params['Unique RM Specs associated to null descriptions'] = tru[tru[TRU_SPEC_DESC].isna()][
        TRU_SPEC_ID].nunique()
    tru = tru[tru[TRU_SPEC_DESC].notna()].copy()

    print('Null value in  RM Specs', tru[TRU_SPEC_ID].isna().sum())
    tru[TRU_SPEC_DESC] = tru[TRU_SPEC_DESC].replace('N/A', np.nan)
    tru[TRU_SPEC_ID] = tru[TRU_SPEC_ID].replace('N/A', np.nan)
    print('tru tru spec id null: ', len(tru[TRU_SPEC_ID].isna()))
    tru.dropna(subset = [TRU_SPEC_ID] , inplace =True)
    tru.to_csv(f'{output_dir}/tru_null_RM.csv')

    run_params['Unique RM Specs after dropping null descriptions'] = tru[TRU_SPEC_ID].nunique()
    print('Unique RM Specs associated to null descriptions',
          run_params['Unique RM Specs associated to null descriptions'])

    valid_cols = list(filter(lambda x: not x.lower().startswith('unnamed'), tru.columns))
    tru = tru[valid_cols]
    tru['spec_type_inferred'] = [inferred_tru_type(x) for x in tru[TRU_SPEC_ID]]
    tru_counts_specs = tru.groupby('spec_type_inferred').size()
    for i, s in tru_counts_specs.items():
        run_params[f'Count of specs of type {i}'] = s

    # Provisional key. This is only use for the embeddings. Later is overwritten by below method
    tru['key_tru'] = tru[TRU_MAT_NUM] + "_" + tru[TRU_SPEC_ID]

    # Add column to make it compatible with original sample data
    if 'FG_NM' not in tru.columns:
        tru['FG_NM'] = 'Dummy fp name ' + tru[TRU_SPEC_ID].copy()
    if 'FD_NM' not in tru.columns:
        tru['FD_NM'] = 'Dummy fd spec ' + tru[TRU_SPEC_ID].copy()

    tru[TRU_MAT_DESC] = tru[TRU_MAT_DESC].fillna('unknown')

    ## Add trade names
    records_tru = len(tru)
    tru = tru.merge(trade_names,
                    left_on=[f'{TRU_MAT_NUM}_original', TRU_SPEC_ID],
                    right_on=['ERP_PART_NUM', 'CHILD_NM'],
                    how='left')
    # tru.to_csv(f'{output_dir}/tru_after_trade_mrg.csv')
    assert len(tru) == records_tru, 'Duplicates found while merging with trade names'

    #########################
    # Merge IM AND EM
    #########################
    print('Merging ERP datasets')
    erp = pd.concat([erp_im.reset_index(drop=True), erp_em.reset_index(drop=True)], ignore_index=True)
    erp = erp[ERP_COLS]
    erp['MATL_SHRT_DESC'] = erp['MATL_SHRT_DESC'].fillna('unknown')
    erp = create_key_column(erp, 'erp')
    ("Merging ERP datasets - Remaining ERP_PART_IDs:", set(cmp_check_list1) & set(erp_im[ERP_PART_ID].unique()))
  
    ###########################
    # Discover common keys
    ###########################

    erp_em['indicator'] = 'indicator'
    tru_pad_needed = tru.merge(erp_em[[ERP_MAT_NUM, 'indicator']].drop_duplicates(),
                               left_on=f'{TRU_MAT_NUM}_padded',
                               right_on=ERP_MAT_NUM,
                               how='left')['indicator'].notna()
    tru[TRU_MAT_NUM] = [padded if needed else original for padded, needed, original in
                        zip(tru[f'{TRU_MAT_NUM}_padded'], tru_pad_needed, tru[f'{TRU_MAT_NUM}_original'])]
    erp_em.drop(columns='indicator', inplace=False)
    
  

    # Now TRU_MAT_NUM holds in theory the information to make the matches downstream
    print("\n[STEP 6] Tracked CMPNT_MATL_NUM check before ERP–TRU match")

    erp_cmp_ids = set(erp["CMPNT_MATL_NUM"])
    tru_parent_ids = set(tru[TRU_MAT_NUM])

    print(" Tracked components still in ERP:", set(cmp_check_list) & erp_cmp_ids)
    print(" Tracked components still in ERP_LIST1:", set(cmp_check_list1) & erp_cmp_ids)
    print(" Their parent materials in TRU:", set(cmp_check_list) & tru_parent_ids)
    print(" Their parent materials in TRU_LIST1:", set(cmp_check_list1) & tru_parent_ids)
    user_logs_path = os.path.join(os.getcwd(), "user_logs")
    os.makedirs(user_logs_path, exist_ok=True)
    erp.to_csv(os.path.join(user_logs_path, "erp.csv"), index=False)
    tru.to_csv(os.path.join(user_logs_path, "tru.csv"), index=False)
    common_codes = (set(erp[ERP_MAT_NUM]).intersection(set(tru[TRU_MAT_NUM])))
    

    erp = erp[erp[ERP_MAT_NUM].isin(common_codes)].reset_index(drop=True)
    erp_cmp_ids = set(erp["CMPNT_MATL_NUM"])
    print(" Tracked components still in ERP_final:", set(cmp_check_list) & erp_cmp_ids)
    print(" Tracked components still in ERP_LIST1_final:", set(cmp_check_list1) & erp_cmp_ids)

    tru_no_common = tru[~tru[TRU_MAT_NUM].isin(common_codes)]
    tru = tru[tru[TRU_MAT_NUM].isin(common_codes)].reset_index(drop=True)

    
    ###########################
    # Off specs
    ###########################
    erp['is_off_spec_erp'] = classify_as_off_spec(erp[[ERP_PART_ID, ERP_PART_DESC]])
    tru['is_off_spec_tru'] = classify_as_off_spec(tru[[TRU_SPEC_ID, TRU_SPEC_DESC]])

    tru_no_common['is_off_spec_tru'] = classify_as_off_spec(tru_no_common[[TRU_SPEC_ID, TRU_SPEC_DESC]])
    tru_off_specs_no_common = tru_no_common.query('is_off_spec_tru').reset_index(drop=True)


    erp_discarded = erp.query('is_off_spec_erp').reset_index(drop=True)
    erp_cmp_ids = set(erp_discarded["CMPNT_MATL_NUM"])
    tru_parent_ids = set(tru[TRU_MAT_NUM])
    print(" erp_discarded components still in ERP_final:", set(cmp_check_list) & erp_cmp_ids)
    print(" erp_discarded components still in ERP_LIST1_final:", set(cmp_check_list1) & erp_cmp_ids)

    df_check = erp_discarded
    user_logs_path = os.path.join(os.getcwd(), "user_logs")
    os.makedirs(user_logs_path, exist_ok=True)
    df_check.to_csv(os.path.join(user_logs_path, "erp_discarded.csv"), index=False)
    erp = erp.query('not is_off_spec_erp').reset_index(drop=True)
    tru_discarded = tru.query('is_off_spec_tru').reset_index(drop=True)
    df_check = tru_discarded
    user_logs_path = os.path.join(os.getcwd(), "user_logs")
    os.makedirs(user_logs_path, exist_ok=True)
    df_check.to_csv(os.path.join(user_logs_path, "tru_discarded.csv"), index=False)
    tru = tru.query('not is_off_spec_tru').reset_index(drop=True)

    run_params['Unique RM Specs discarded by off spec'] = len(set(tru_discarded[TRU_SPEC_ID]) - set(tru[TRU_SPEC_ID]))
    run_params['erp off specs'] = len(erp_discarded[ERP_PART_ID].values)
    run_params['tru off specs'] = len(tru_discarded[TRU_SPEC_ID].values)
    run_params['Unique RM Specs after discarding off specs'] = tru[TRU_SPEC_ID].nunique()
    print('Unique RM Specs after discarding off specs', run_params['Unique RM Specs after discarding off specs'])
    print(f'{len(erp_discarded)} rows discarded from ERP as they are irrelevant materials')
    print(f'{len(tru_discarded)} rows discarded from TRU as they are irrelevant materials')

    tru = tru[TRU_COLS + ['key_tru']]
    tru_discarded = tru_discarded[TRU_COLS + ['key_tru']]

    print('#' * 60)
    common_codes = tru[TRU_MAT_NUM].nunique()
    print('Common number of codes in both sources: ', common_codes)
    run_params['Number of Material Codes to consider in this run'] = common_codes
    print(f'Ingest Data took {time.time() - start} seconds')
    print('-' * 60)

    assert erp[ERP_PART_ID].isna().sum() == 0, 'null erp parts id detected'
    assert tru[TRU_SPEC_ID].isna().sum() == 0, 'null tru specs id detected'

    return erp, tru, erp_discarded, tru_discarded, run_params, tru_no_common,tru_off_specs_no_common, bundles


def create_key_column(df, source) -> pd.DataFrame:
    """
    Create a unique row identifier.
    """
    if source == 'erp':
        df[f'key_{source}'] = df[ERP_MAT_NUM] + "_" + df[ERP_PART_ID] + "_" + df['PLNT_CD'] + "_" + df['SRC_SYS_CD']
    elif source == 'tru':
        df[f'key_{source}'] = df[TRU_MAT_NUM] + "_" + df[TRU_SPEC_ID] + "_" + df['PLNT_CD'] + "_" + df['SRC_SYS_CD']
    else:
        raise ValueError(f'{source} not recognised as a valid source')
    return df
