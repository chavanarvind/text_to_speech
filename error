import pandas as pd
import spacy
from collections import Counter
from itertools import islice
from pathlib import Path

# Load SpaCy (disable unneeded components)
nlp = spacy.load("en_core_web_sm", disable=["ner", "parser", "tagger"])

# Define paths
input_path = './data/target_map_parquet_files'
output_path = Path('./outputs/frequent_terms_by_category')
output_path.mkdir(parents=True, exist_ok=True)

# Required columns only
required_cols = ['Final Category', 'CMPNT_MATL_DESC']
all_files = list(Path(input_path).glob("*.parquet"))
df = pd.concat([pd.read_parquet(f, columns=required_cols) for f in all_files], ignore_index=True)

# Filter valid Final Category
df = df[df['Final Category'].notna()]
df['CMPNT_MATL_DESC'] = df['CMPNT_MATL_DESC'].fillna('').str.lower()

# Initialize results
all_unigrams = []
all_bigrams = []

# Process each category
for category, group in df.groupby('Final Category'):
    text = ' '.join(group['CMPNT_MATL_DESC'].tolist())
    doc = nlp(text)
    words = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]

    # Unigrams with threshold
    unigram_counts = Counter(words)
    for word, count in unigram_counts.items():
        if count >= 10:
            all_unigrams.append({'category': category, 'word': word, 'count': count})

    # Bigrams with threshold
    bigram_counts = Counter(zip(words, islice(words, 1, None)))
    for (w1, w2), count in bigram_counts.items():
        if count >= 10:
            all_bigrams.append({'category': category, 'bigram': f'{w1} {w2}', 'count': count})

# Save combined CSVs
pd.DataFrame(all_unigrams).to_csv(output_path / "all_categories_unigrams.csv", index=False)
pd.DataFrame(all_bigrams).to_csv(output_path / "all_categories_bigrams.csv", index=False)
