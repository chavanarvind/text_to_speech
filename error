# script_2_generate_key_files_only.py
# ------------------------------------
# Extracts consistent key columns from raw BOM parquet files.
# Handles missing columns gracefully by filling with None.

import os
import glob
import argparse
import pandas as pd
from azureml.core import Run

# ✅ Full set of key columns (from your input structure + screenshot)
KEY_COLS = [
    "SRC_SYS_CD",
    "MATL_NUM",
    "PLNT_CD",
    "CMPNT_MATL_NUM",
    "CMPNT_MATL_DESC",
    "CMPNT_MATL_TYPE_CD",
    "CMPNT_CAT_CD_DESC",
    "CMPNT_UOM_CD",
    "CREATED_DTTM",
    "File_Name"
]

def process_file(file_path, key_output_dir):
    try:
        print(f"\n[INFO] Processing: {os.path.basename(file_path)}")

        # Read full Parquet input
        full_df = pd.read_parquet(file_path)

        # Create consistent key_df
        key_df = pd.DataFrame()
        for col in KEY_COLS:
            key_df[col] = full_df[col] if col in full_df.columns else None

        key_df = key_df.drop_duplicates()

        # Save key file
        os.makedirs(key_output_dir, exist_ok=True)
        key_path = os.path.join(key_output_dir, os.path.basename(file_path))
        key_df.to_parquet(key_path, index=False)

        print(f"[✅] Key file saved: {key_path} (rows: {len(key_df)})")

    except Exception as e:
        print(f"[❌] Error processing {file_path}: {e}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True, help="Path to input Parquet files")
    parser.add_argument("--key_output", type=str, required=True, help="Output directory for key files")
    args = parser.parse_args()

    run = Run.get_context()

    input_files = glob.glob(os.path.join(args.input_path, "*.parquet"))
    for file_path in input_files:
        process_file(file_path, args.key_output)

if __name__ == "__main__":
    main()
