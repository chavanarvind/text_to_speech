code 1
import os
import glob
import numpy as np
import pandas as pd
import torch
import joblib
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler

# --- Paths ---
input_path = './sub_cat_data/target_map_cleaned'
embedding_dir = './sub_cat_data/validation_step/model_artifects_PKG/bert_embedding_pkg_only'
sample_dir = './sub_cat_data/validation_step/model_artifects_PKG/sampled_rows_pkg_only'
output_dir = './sub_cat_data/validation_step/model_artifects_PKG/model_artifects_pkg_only'
os.makedirs(embedding_dir, exist_ok=True)
os.makedirs(sample_dir, exist_ok=True)
os.makedirs(output_dir, exist_ok=True)

# --- BERT Encoder ---
encoder = SentenceTransformer(
    'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb',
    device='cuda' if torch.cuda.is_available() else 'cpu'
)

# --- Sampling Fraction Map (Fixed per PKG Final Category) ---
category_frac_map = {
    "PKG - Labels": 0.03,
    "PKG - Corrugated": 0.03,
    "PKG - Cartons": 0.1,
    "PKG - Labels or PKG - Leaflets": 0.1,
    "PKG - Other Packaging": 0.1,
    "PKG - Films (Polybags)": 0.1,
    "PKG - Tubes": 0.1,
    "PKG - Caps/Closures/Lids": 0.1,
    "PKG - Bottles (PET)": 0.1,
    "PKG - Films (Other Plastic)": 0.1,
    "PKG - Bottles (HDPE)": 0.1,
    "PKG - Other Rigid Packaging (Glass packaging)": 0.1,
    "PKG - Bottles (PP)": 0.1,
    "PKG - Promotional Materials": 0.3,
    "PKG - Cosmetics": 0.3,
    "PKG - Pumps": 0.3,
    "PKG - Leaflets": 0.3,
    "PKG - Foils": 0.3,
    "PKG - Thermoformed (All other)": 0.3,
    "PKG - Jars": 0.3,
    "PKG - Injection Molded Components": 0.3,
    "PKG - Sleeves": 0.3,
    "PKG - Films (Paper)": 0.3,
    "PKG - Inserts": 1.0,
    "PKG - Metals (Tins and Cans)": 1.0,
    "PKG - Films (PVC)": 1.0,
    "PKG - Bottles (PET) or PKG - Bottles (HDPE) or PKG - Bottles (PP)": 1.0,
    "PKG - Films (Laminates)": 1.0,
    "PKG - Metals (Tins and Cans) or PKG - Pumps": 1.0,
    "PKG - Films (Laminates) or PKG - Tubes": 1.0,
}

# --- Required Columns ---
required_cols = [
    'CMPNT_MATL_DESC', 'CMPNT_MATL_DESC_LEN',
    'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY',
    'Final Category','Final Subcategory'
]

# --- Initialize Output Lists ---
files = sorted(glob.glob(os.path.join(input_path, '*.parquet')))
output_X = []
output_y = []
meta_features = []

# --- File Loop ---
for file in files:
    file_name = os.path.splitext(os.path.basename(file))[0]
    print(f"Processing {file_name}")

    try:
        df = pd.read_parquet(file, columns=required_cols)
        df = df[df['CMPNT_MATL_DESC'].notna()]
        df = df.drop_duplicates(subset=required_cols)

        # Keep only PKG Final Categories
        df = df[df['Final Subcategory'].notna() & (df['Final Category'] == 'PKG')]
            
        
        print("PKG Categories Found:", df['Final Subcategory'].unique())

        if df.empty:
            print(f" Skipping {file_name}: no PKG data")
            continue

        # Sampling
        sampled = (
            df.groupby('Final Subcategory', group_keys=False)
              .apply(lambda g: g.sample(frac=category_frac_map.get(g.name, 0.1), random_state=42))
              .reset_index(drop=True)
        )

        if sampled.empty:
            print(f" Skipping {file_name}: no valid sampled rows")
            continue

        # BERT Encoding
        desc_emb = encoder.encode(
            sampled['CMPNT_MATL_DESC'].astype(str).tolist(),
            batch_size=256,
            show_progress_bar=True,
            convert_to_numpy=True,
            num_workers=4
        )

        if len(desc_emb) != len(sampled):
            print(f" Skipped {file_name}: embedding/sample size mismatch")
            continue

        # Save sampled + embeddings
        sampled.to_parquet(os.path.join(sample_dir, f'{file_name}_sampled.parquet'), index=False)
        np.save(os.path.join(embedding_dir, f'{file_name}_bert.npy'), desc_emb)

        # Accumulate
        output_X.append(desc_emb)
        output_y.append(sampled['Final Subcategory'].astype(str).values)
        meta_features.append(sampled[['CMPNT_MATL_DESC_LEN', 'UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])

        print(f"Processed: {file_name} ({len(sampled)} rows)")

    except Exception as e:
        print(f"Error processing {file_name}: {e}")

# --- Final Check ---
if not output_X:
    raise RuntimeError("No valid embeddings or samples generated.")

# --- Stack Arrays ---
bert_array = np.vstack(output_X)
y_full = np.concatenate(output_y)
meta_all = pd.concat(meta_features, ignore_index=True)

# --- Encode Meta Features ---
ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
scaler = StandardScaler()

ordinal.fit(meta_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
scaler.fit(meta_all[['CMPNT_MATL_DESC_LEN']])

meta_scaled = scaler.transform(meta_all[['CMPNT_MATL_DESC_LEN']])
cat_encoded = ordinal.transform(meta_all[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])

# --- Final X Matrix ---
X_full = np.hstack([bert_array, meta_scaled, cat_encoded])

# --- Save Outputs ---
np.save(os.path.join(output_dir, 'X_full_PKG.npy'), X_full)
np.save(os.path.join(output_dir, 'y_full_PKG.npy'), y_full)
joblib.dump(ordinal, os.path.join(output_dir, 'ordinal_encoder_PKG.pkl'))
joblib.dump(scaler, os.path.join(output_dir, 'scaler_PKG.pkl'))

print(f"\n All files saved:")
print(f"   X_full.npy shape = {X_full.shape}")
print(f"   y_full.npy shape = {y_full.shape}, dtype = {y_full.dtype}")

code2
import numpy as np
import pandas as pd
import joblib
from lightgbm import LGBMClassifier, early_stopping, log_evaluation
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, f1_score

# --- Load Feature Matrix and Labels ---
X = np.load('./sub_cat_data/validation_step/model_artifects_PKG/model_artifects_pkg_only/X_full_PKG.npy',allow_pickle=True)
y = np.load('./sub_cat_data/validation_step/model_artifects_PKG/model_artifects_pkg_only/y_full_PKG.npy',allow_pickle=True)

# --- Initial Split: 80% (train+test), 20% validation ---
X_train_test, X_val, y_train_test, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# --- Split again: 75% train, 25% test from that 80% block ---
X_train, X_test, y_train, y_test = train_test_split(
    X_train_test, y_train_test, test_size=0.25, stratify=y_train_test, random_state=42
)

print(f"ðŸ”¹ Train size:      {X_train.shape[0]}")
print(f"ðŸ”¹ Test size:       {X_test.shape[0]}")
print(f"ðŸ”¹ Validation size: {X_val.shape[0]}")

# --- Train the model on (train + test) set ---
X_final_train = np.vstack([X_train, X_test])
y_final_train = np.concatenate([y_train, y_test])

model = LGBMClassifier(
    class_weight='balanced',
    n_estimators=1000,
    random_state=42
)

model.fit(
    X_final_train, y_final_train,
    eval_set=[(X_val, y_val)],
    eval_metric='logloss',
    callbacks=[early_stopping(stopping_rounds=10), log_evaluation(period=10)]
)

# --- Predict on Validation Set ---
y_pred = model.predict(X_val)
y_proba = model.predict_proba(X_val)
y_score = np.max(y_proba, axis=1)

# --- Evaluation Summary ---
print("\nClassification Report (Validation):")
report = classification_report(y_val, y_pred, zero_division=0)
print(report)

print("\nConfusion Matrix:")
print(confusion_matrix(y_val, y_pred))

f1 = f1_score(y_val, y_pred, average='weighted')
print(f"\n Weighted F1 Score (Validation): {f1:.4f}")

# --- Save Model and Prediction Summary ---
joblib.dump(model, './sub_cat_data/validation_step/model_artifects_PKG/model_artifects_pkg_only/final_model_pkg.joblib')
print(" Model saved: final_model.joblib")

# Save predictions
pred_df = pd.DataFrame({
    'Actual': y_val,
    'Predicted': y_pred,
    'Confidence': y_score
})
pred_df.to_csv('./sub_cat_data/validation_step/model_artifects_PKG/model_artifects_pkg_only/validation_predictions_pkg.csv', index=False)
print(" Saved: validation_predictions.csv")

