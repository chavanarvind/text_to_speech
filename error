# Output folders

# Intermediate outputs
step1a_key_output_temp = OutputFileDatasetConfig(name='step1a_key_output_temp')
final_output_to_adls = OutputFileDatasetConfig(
    name='final_predictions_to_adls',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/inference_per_file/")
)
final_merged_output = OutputFileDatasetConfig(
    name='final_merged_predictions',
    destination=(default_ds, f"hbom_category_prediction/final_output_{today}/")
)

# === Step 5 Output: Raw inference per file ===
inference_output = OutputFileDatasetConfig(
    name='inference_file_outputs',
    destination=(default_ds, f"hbom_category_prediction_inference_per_file/hbom_category_prediction_{today}/")
)

# === Step 6 Output: Final merged and deduplicated predictions ===
final_merged_output = OutputFileDatasetConfig(
    name='final_merged_predictions',
    destination=(default_ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)

# Upload reference CSVs
high_conf_dataset = Dataset.File.from_files((default_ds, 'high_conf_map')).as_named_input('high_conf_map').as_mount()
abbrev_dataset = Dataset.File.from_files((default_ds, 'abbrev_map')).as_named_input('abbrev_map').as_mount()

# PipelineData for steps
step1_out = PipelineData(name="data_step1_out", is_directory=True)
step1a_mapped = PipelineData(name="data_step1a_mapped", is_directory=True)
step1a_unmapped = PipelineData(name="data_step1a_unmapped", is_directory=True)
step2_out = PipelineData(name="data_step2_out", is_directory=True)
step3_out = PipelineData(name="data_step3_out", is_directory=True)
step4_out = PipelineData(name="data_step4_out", is_directory=True)
