import os
import argparse
import pandas as pd
import numpy as np
import torch
import joblib
import subprocess
from datetime import datetime
from azureml.core import Run, Model
from sentence_transformers import SentenceTransformer
from azureml.core import Run, Datastore, Dataset, Workspace
import time
import shutil

subprocess.run(["pip", "install", "lightgbm"], check=True)

def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def main(input_path, additional_mapped_dir):
    run = Run.get_context()
    ws = run.experiment.workspace

    log(" Loading model artifacts...")
    model_dir = Model.get_model_path("RPM_Category_model_full_cat", _workspace=ws)
    model = joblib.load(os.path.join(model_dir, "final_model.joblib"))
    ordinal = joblib.load(os.path.join(model_dir, "ordinal_encoder.pkl"))
    scaler = joblib.load(os.path.join(model_dir, "scaler.pkl"))

    encoder = SentenceTransformer(
        'pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb',
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    encoder.max_seq_length = 128

    log(" Scanning input files...")
    files = [f for f in os.listdir(input_path) if f.endswith(".parquet")]
    log(f" Found {len(files)} files to process.")

    all_results = []

    log(f" Loading pre-mapped data from: {additional_mapped_dir}")
    mapped_files = [f for f in os.listdir(additional_mapped_dir) if f.endswith(".parquet")]
    mapped_dfs = []
    for f in mapped_files:
        df = pd.read_parquet(os.path.join(additional_mapped_dir, f))

        required_cols_defaults = {
            'Score': 1.0,
            'Predicted': None,
            'Final_Prediction': None,
            'Subcategory_Score': 1.0,
            'Subcategory_Model': 'direct_mapping',
            'Predicted_Subcategory': None,
            'prediction_flag': 'Mapped_PreExtracted',
            'inferred_category_model': 'direct_mapping_step_1a'
        }

        for col, default in required_cols_defaults.items():
            if col not in df.columns:
                df[col] = default

        if 'Final_Prediction' not in df.columns or df['Final_Prediction'].isnull().all():
            df['Final_Prediction'] = df.get('Final Category', None)
        if 'Predicted_Subcategory' not in df.columns or df['Predicted_Subcategory'].isnull().all():
            df['Predicted_Subcategory'] = df.get('Final Subcategory', None)

        df['source_file'] = f
        mapped_dfs.append(df)

    if mapped_dfs:
        mapped_combined_df = pd.concat(mapped_dfs, ignore_index=True)
        all_results.append(mapped_combined_df)
        log(f" Loaded and added {len(mapped_combined_df)} mapped rows from step_1a.")
    else:
        log(" No additional mapped data found.")

    for f in files:
        log(f"Processing file: {f}")
        file_path = os.path.join(input_path, f)
        df_all = pd.read_parquet(file_path)
        log(f" Original row count: {len(df_all)}")

        if 'CMPNT_MATL_NUM' not in df_all.columns:
            log(f" Skipping {f}: 'CMPNT_MATL_NUM' column not found.")
            continue

        if 'needs_model' in df_all.columns:
            df_mapped = df_all[df_all['needs_model'] == False].copy()
            df_pred = df_all[df_all['needs_model'] == True].copy()
            df_pred = df_all[(df_all['needs_model'] == True) & (df_all['Final Category'].isnull())].copy()
        else:
            df_mapped = pd.DataFrame(columns=df_all.columns)
            df_pred = df_all.copy()

        df_mapped['Final_Prediction'] = df_mapped['Final Category']
        df_mapped['Score'] = 1.0
        df_mapped['prediction_flag'] = 'Mapped'
        df_mapped['inferred_category_model'] = 'direct_mapping'

        if not df_pred.empty:
            log(f" Rows needing prediction: {len(df_pred)}")
            log(" Encoding descriptions with BERT...")
            desc_emb = encoder.encode(
                df_pred['CMPNT_MATL_DESC_CLEAN'].astype(str).tolist(),
                batch_size=256,
                show_progress_bar=True,
                convert_to_numpy=True)

            log("⚙️ Transforming structured features...")
            length_scaled = scaler.transform(df_pred[['CMPNT_MATL_DESC_LEN']])
            cat_encoded = ordinal.transform(df_pred[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
            X_pred = np.hstack([desc_emb, length_scaled, cat_encoded])

            log(" Predicting categories...")
            y_proba = model.predict_proba(X_pred)
            y_score = np.max(y_proba, axis=1)
            y_pred = model.predict(X_pred)

            df_pred['Score'] = y_score
            df_pred['Predicted'] = y_pred
            df_pred['Final_Prediction'] = np.where(y_score < 0.6, 'Other', y_pred)
            df_pred['prediction_flag'] = np.where(y_score < 0.6, 'Low Confidence', 'High Confidence')
            df_pred['inferred_category_model'] = 'lightgbm_Bert_RPM_Category_model'

            log(f"Finished prediction. High-confidence rows: {(y_score >= 0.6).sum()}")

        df_out = pd.concat([df_pred, df_mapped], axis=0).sort_index()
        df_out.loc[df_out['prediction_flag'].str.contains("Mapped"), 'Final Subcategory'] = df_out['Final Subcategory']
        df_out.loc[df_out['prediction_flag'].str.contains("Mapped"), 'Subcategory_Score'] = 1.0
        df_out.loc[df_out['prediction_flag'].str.contains("Mapped"), 'Subcategory_Model'] = 'direct_mapping'
        df_out.loc[df_out['prediction_flag'].str.contains("Mapped"), 'Predicted_Subcategory'] = df_out['Final Subcategory']

        log(" Predicting subcategories for missing values...")
        df_sub_mapped = df_out[df_out['Final Subcategory'].notna()].copy()
        df_sub_pred = df_out[df_out['Final Subcategory'].isna()].copy()

        subcat_predictions = []
        valid_categories = ['CHM', 'PKG', 'FNW']
        for cat in valid_categories:
            subset = df_sub_pred[df_sub_pred['Final_Prediction'] == cat].copy()
            if not subset.empty:
                log(f" Subcategory prediction for {cat}: {len(subset)} rows")
                model_name = f"RPM_Category_model_full_{cat}_V1"
                model_path = Model.get_model_path(model_name, _workspace=ws)
                sub_model = joblib.load(os.path.join(model_path, "final_model.joblib"))
                sub_encoder = joblib.load(os.path.join(model_path, "ordinal_encoder.pkl"))
                sub_scaler = joblib.load(os.path.join(model_path, "scaler.pkl"))

                desc_emb = encoder.encode(
                    subset['CMPNT_MATL_DESC_CLEAN'].astype(str).tolist(),
                    batch_size=256,
                    show_progress_bar=True,
                    convert_to_numpy=True)

                length_scaled = sub_scaler.transform(subset[['CMPNT_MATL_DESC_LEN']])
                cat_encoded = sub_encoder.transform(subset[['UNIT_GROUP', 'CMPNT_MATL_TYPE_CATEGORY']])
                X_sub = np.hstack([desc_emb, length_scaled, cat_encoded])
                y_sub = sub_model.predict(X_sub)
                subset['Final Subcategory'] = y_sub
                subset['Predicted_Subcategory'] = y_sub
                subset['Subcategory_Score'] = 0.9
                subset['Subcategory_Model'] = model_name
                subcat_predictions.append(subset)

        fallback_rows = df_sub_pred[~df_sub_pred['Final_Prediction'].isin(valid_categories)].copy()
        fallback_rows['Final Subcategory'] = fallback_rows['Final_Prediction']
        fallback_rows['Subcategory_Score'] = 0.9
        fallback_rows['Subcategory_Model'] = 'default_fallback'

        df_sub_mapped['Subcategory_Score'] = 1.0
        df_sub_mapped['Subcategory_Model'] = 'direct_mapping'
        df_sub_mapped['Predicted_Subcategory'] = df_sub_mapped['Final Subcategory']

        df_sub_pred_final = pd.concat(subcat_predictions + [fallback_rows])
        df_out = pd.concat([df_sub_pred_final, df_sub_mapped], axis=0).sort_index()

        df_out['source_file'] = f
        all_results.append(df_out)

    log(" Combining all results into one DataFrame...")
    final_df = pd.concat(all_results, axis=0, ignore_index=True)
    final_df.loc[final_df['prediction_flag'].str.contains("Mapped"), 'Final Subcategory'] = final_df['Final Subcategory']
    final_df.loc[final_df['prediction_flag'].str.contains("Mapped"), 'Subcategory_Score'] = 1.0
    final_df.loc[final_df['prediction_flag'].str.contains("Mapped"), 'Subcategory_Model'] = 'direct_mapping'
    final_df.loc[final_df['prediction_flag'].str.contains("Mapped"), 'Predicted_Subcategory'] = final_df['Final Subcategory']

    log(" Removing duplicates by CMPNT_MATL_NUM...")
    final_df['priority'] = final_df['prediction_flag'].map({
    'Mapped_PreExtracted': 2,
    'Mapped': 2,
    'High Confidence': 1,
    'Low Confidence': 0}).fillna(0)
    # Sort to put mapped rows first
    final_df = final_df.sort_values(by=['CMPNT_MATL_NUM', 'priority'], ascending=[True, False])
    # Drop duplicates, keeping highest priority (mapped rows)
    final_df = final_df.drop_duplicates(subset='CMPNT_MATL_NUM', keep='first')

    #final_df = final_df.drop_duplicates(subset='CMPNT_MATL_NUM')

    
    # Save separate samples for mapped and predicted
    sample_dir = os.path.join("outputs", "samples_by_source")
    os.makedirs(sample_dir, exist_ok=True)

    # Mapped rows
    mapped_rows = final_df[final_df['prediction_flag'].str.contains("Mapped")]
    mapped_rows.head(100).to_csv(os.path.join(sample_dir, "sample_mapped_rows.csv"), index=False)

    # Predicted rows (inferred by model)
    predicted_rows = final_df[final_df['prediction_flag'].isin(["High Confidence", "Low Confidence"])]
    predicted_rows.head(100).to_csv(os.path.join(sample_dir, "sample_predicted_rows.csv"), index=False)
    final_df.to_parquet("outputs/final_prediction_combined.parquet", index=False)
    log(" Saved combined output as final_prediction_combined.parquet and sample CSV")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True)
    parser.add_argument("--additional_mapped_dir", type=str, required=True)
    parser.add_argument("--key_output_dir", type=str, required=True)
    args = parser.parse_args()

    main(args.input_path, args.additional_mapped_dir)

    log(" Mapping predictions back to original key files...")
    key_files = [f for f in os.listdir(args.key_output_dir) if f.endswith(".parquet")]
    mapped_keys_dir = os.path.join("outputs", "mapped_keys")
    os.makedirs(mapped_keys_dir, exist_ok=True)

    
    final_df = pd.read_parquet("outputs/final_prediction_combined.parquet")
    # Rename columns for final output
    final_df = final_df.rename(columns={
        "Final_Prediction": "AI_FINAL_CATEGORY",
        "Final Subcategory": "AI_FINAL_SUBCATEGORY",
        "Score": "AI_FINAL_CATEGORY_CONFIDENCE",
        "Subcategory_Score": "AI_FINAL_SUBCATEGORY_CONFIDENCE",
        "inferred_category_model": "AI_MATCHING_REASON_FINAL_CATEGORY",
        "Subcategory_Model": "AI_MATCHING_REASON_FINAL_SUBCATEGORY"
    })

    

    for key_file in key_files:
        key_path = os.path.join(args.key_output_dir, key_file)
        try:
            key_df = pd.read_parquet(key_path)
            merged_df = key_df.merge(
                final_df[[
                    "CMPNT_MATL_NUM",
                    "AI_FINAL_CATEGORY",
                    "AI_FINAL_SUBCATEGORY",
                    "AI_FINAL_CATEGORY_CONFIDENCE",
                    "AI_FINAL_SUBCATEGORY_CONFIDENCE",
                    "AI_MATCHING_REASON_FINAL_CATEGORY",
                    "AI_MATCHING_REASON_FINAL_SUBCATEGORY"
                ]],
                how="left", on="CMPNT_MATL_NUM"
            )
            out_path = os.path.join(mapped_keys_dir, key_file)
            merged_df.to_parquet(out_path, index=False)
            log(f"✅ Saved mapped key file: {out_path}")
        except Exception as e:
            log(f"❌ Failed to process key file {key_file}: {e}")

    # ✅ Upload to ADLS2 (bom-xref) in hbom_category_prediction_DDMMYYYY format


    curr_time = time.strftime("%d%m%Y", time.localtime())
    output_folder_name = f"hbom_category_prediction_{curr_time}"
    upload_dir = os.path.join("outputs", "upload_dir")
    os.makedirs(upload_dir, exist_ok=True)

    for key_file in key_files:
        try:
            source_path = os.path.join(mapped_keys_dir, key_file)
            target_path = os.path.join(upload_dir, key_file)
            if os.path.exists(source_path):
                shutil.copy2(source_path, target_path)
        except Exception as e:
            log(f"❌ Failed to copy {key_file} to upload_dir: {e}")

    try:
        run = Run.get_context()
        if run.identity.startswith("OfflineRun"):
            ws = Workspace.from_config()
        else:
            ws = run.experiment.workspace

        ds = Datastore.get(ws, "bom-xref")

        Dataset.File.upload_directory(
            src_dir=upload_dir,
            target=ds.path(output_folder_name),
            overwrite=True
        )

        log(f"✅ Uploaded merged files to ADLS2 path: bom-xref/{output_folder_name}/")
    except Exception as e:
        log(f"❌ ADLS2 upload failed: {e}")


[2025-06-09 11:00:11]  Scanning input files...
[2025-06-09 11:00:11]  Found 2 files to process.
[2025-06-09 11:00:11]  Loading pre-mapped data from: /mnt/azureml/cr/j/db7cff379ee0422f9b281724bc09bfa9/cap/data-capability/wd/INPUT_data_step1a_mapped_workspaceblobstore/azureml/96890332-7990-4be5-a5ea-0e82612bd78d/data_step1a_mapped
[2025-06-09 11:00:43]  Loaded and added 7552186 mapped rows from step_1a.
[2025-06-09 11:00:43] Processing file: part-00000-0042fc95-a813-4a78-a5ea-9d565974a2b5-c000.snappy.parquet
[2025-06-09 11:00:45]  Original row count: 1057526
Cleaning up all outstanding Run operations, waiting 300.0 seconds
1 items cleaning up...
Cleanup took 0.09696078300476074 seconds
Traceback (most recent call last):
  File "step_5_run_inference.py", line 223, in <module>
    main(args.input_path, args.additional_mapped_dir)
  File "step_5_run_inference.py", line 130, in main
    df_out.loc[df_out['prediction_flag'].str.contains("Mapped"), 'Subcategory_Score'] = 1.0
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/pandas/core/indexing.py", line 849, in __setitem__
    iloc._setitem_with_indexer(indexer, value, self.name)
  File "/opt/conda/envs/ptca/lib/python3.8/site-packages/pandas/core/indexing.py", line 1751, in _setitem_with_indexer
    raise ValueError(
ValueError: cannot set a frame with no defined index and a scalar
