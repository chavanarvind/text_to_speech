# pipeline_runner_updated.py

from azureml.core import Workspace, Experiment, Environment, Dataset
from azureml.pipeline.core import Pipeline, PipelineData
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data import DataPath
from azureml.core.authentication import ServicePrincipalAuthentication
from datetime import datetime
import sys

# === Workspace Config ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"


def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        tenant_id = args[1]
        client_id = args[2]
        client_secret = args[3]
        sp_auth = ServicePrincipalAuthentication(
            tenant_id=tenant_id,
            service_principal_id=client_id,
            service_principal_password=client_secret
        )
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME,
            auth=sp_auth
        )
    else:
        return Workspace(
            subscription_id=SUBSCRIPTION_ID,
            resource_group=RESOURCE_GROUP,
            workspace_name=WORKSPACE_NAME
        )


# === Initialize ===
ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
compute_target = ws.compute_targets["llm-gpu-cluster-2"]
env = Environment.get(workspace=ws, name="Bom_X_Evaluator")
run_config = RunConfiguration()
run_config.target = compute_target.name
run_config.environment = env

today = datetime.today().strftime("%d%m%Y")

# === Datastore and Uploads ===
datastore = ws.get_default_datastore()

# Upload mapping and abbreviation CSVs
mapping_local = './local_data/high_conf_direct_mapping.csv'
abbrev_local = './local_data/abbreviation_expension_updated.csv'

# Upload mapping CSV
datastore.upload_files(
    files=[mapping_local],
    target_path='high_conf_map',
    overwrite=True,
    show_progress=True
)

# Upload abbreviation CSV
datastore.upload_files(
    files=[abbrev_local],
    target_path='abbrev_map',
    overwrite=True,
    show_progress=True
)

# === Dataset References ===
high_conf_dataset = Dataset.File.from_files((datastore, "high_conf_map"))
abbr_dataset = Dataset.File.from_files((datastore, "abbrev_map"))
input_data = Dataset.File.from_files(path=(datastore, "raw_input_data/"))

mapping_csv_mount = high_conf_dataset.as_named_input("high_conf_map").as_mount()
abbrev_csv_mount = abbr_dataset.as_named_input("abbrev_map").as_mount()

# === Intermediate Outputs ===
step1a_output = PipelineData("step1a_output", datastore=datastore)
step2_output = PipelineData("step2_output", datastore=datastore)

# === Step 1a: Mapping Logic ===
step1a_merge = PythonScriptStep(
    name="Step 1a - Merge & Map",
    script_name="step_1a_extract_and_merge.py",
    source_directory="scripts",
    arguments=[
        "--input_path", input_data.as_mount(),
        "--mapping_csv", mapping_csv_mount,
        "--key_output", "key_reference_output.parquet",
        "--final_output", step1a_output
    ],
    inputs=[input_data, mapping_csv_mount],
    outputs=[step1a_output],
    compute_target=compute_target,
    runconfig=run_config,
    allow_reuse=False
)

# === Step 2: Preprocessing ===
step2_preprocess = PythonScriptStep(
    name="Step 2 - Preprocessing",
    script_name="step2_pre_process.py",
    source_directory="scripts",
    arguments=[
        "--input_path", step1a_output,
        "--abbrev_map", abbrev_csv_mount,
        "--output_path", step2_output
    ],
    inputs=[step1a_output, abbrev_csv_mount],
    outputs=[step2_output],
    compute_target=compute_target,
    runconfig=run_config,
    allow_reuse=False
)

# === Build and Run Pipeline ===
pipeline = Pipeline(workspace=ws, steps=[step1a_merge, step2_preprocess])
pipeline.validate()

experiment = Experiment(ws, name="bom_pipeline_pull_merge_preprocess")
run = experiment.submit(pipeline)
run.wait_for_completion(show_output=True)
