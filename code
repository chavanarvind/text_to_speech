import pandas as pd
import pyarrow.parquet as pq
import os

# === Ensure required folders exist ===
os.makedirs('./data', exist_ok=True)
os.makedirs('./data/cleaned_parts', exist_ok=True)

# === FILE PATHS ===
csv_path = './data/combined.csv'
parquet_path = './data/combined.parquet'
target_map_path = './data/cleaned_parts/target_map.csv'

# === STEP 1: Convert CSV to Parquet in chunks if not already done ===
if not os.path.exists(parquet_path):
    print("üîÅ Converting CSV to Parquet...")
    first = True
    for chunk in pd.read_csv(csv_path, chunksize=1_000_000, dtype=str):
        chunk = chunk.drop_duplicates()
        chunk.to_parquet(parquet_path, index=False, engine='pyarrow', append=not first)
        first = False
    print("‚úÖ CSV converted to Parquet:", parquet_path)
else:
    print("‚úÖ Parquet file already exists:", parquet_path)

# === STEP 2: Load target map ===
print("üì• Loading target map...")
target_map = pd.read_csv(target_map_path, usecols=['CMPNT_CAT_CD_DESC', 'Final Category'], dtype=str)
target_map = target_map.drop_duplicates()
target_map['CMPNT_CAT_CD_DESC'] = target_map['CMPNT_CAT_CD_DESC'].astype('category')

# === STEP 3: Read Parquet in batches, merge, and overwrite Parquet file ===
print("üîÅ Merging with target_map and updating the same file...")

# Use PyArrow to read in batches
pf = pq.ParquetFile(parquet_path)
first = True

for batch in pf.iter_batches(batch_size=1_000_000):
    chunk = pd.DataFrame(batch.to_pandas())
    chunk = chunk.drop_duplicates()
    chunk['CMPNT_CAT_CD_DESC'] = chunk['CMPNT_CAT_CD_DESC'].astype('category')
    merged = chunk.merge(target_map, how='left', on='CMPNT_CAT_CD_DESC')
    merged.to_parquet(parquet_path, index=False, engine='pyarrow', append=not first)
    first = False

print("‚úÖ Done. File updated:", parquet_path)
