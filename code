import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import os
import shutil

# === Ensure required folders exist ===
os.makedirs('./data', exist_ok=True)
os.makedirs('./data/cleaned_parts', exist_ok=True)
os.makedirs('./data/temp_chunks', exist_ok=True)

# === FILE PATHS ===
csv_path = './data/combined.csv'
temp_parquet_dir = './data/temp_chunks'
final_parquet_path = './data/combined.parquet'
target_map_path = './data/cleaned_parts/target_map.csv'

# === STEP 0: Delete old output if it exists ===
if os.path.exists(final_parquet_path):
    print("🗑️ Existing combined.parquet found. Removing...")
    os.remove(final_parquet_path)

# === STEP 1: Convert CSV to temporary Parquet chunks ===
print("🔁 Converting CSV to temporary Parquet chunks...")
chunk_files = []
for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=1_000_000, dtype=str)):
    chunk = chunk.drop_duplicates()
    chunk_path = os.path.join(temp_parquet_dir, f"chunk_{i}.parquet")
    chunk.to_parquet(chunk_path, index=False, engine='pyarrow')
    chunk_files.append(chunk_path)
print(f"✅ {len(chunk_files)} chunks written to {temp_parquet_dir}")

# === STEP 2: Load and prepare target map ===
print("📥 Loading target map...")
target_map = pd.read_csv(target_map_path, usecols=['CMPNT_CAT_CD_DESC', 'Final Category'], dtype=str)
target_map = target_map.drop_duplicates()
target_map['CMPNT_CAT_CD_DESC'] = target_map['CMPNT_CAT_CD_DESC'].astype('category')

# === STEP 3: Merge chunks and write to final Parquet (overwrite) ===
print("🔁 Merging chunks and writing to combined.parquet...")
writer = None

for chunk_file in chunk_files:
    chunk = pd.read_parquet(chunk_file)
    chunk = chunk.drop_duplicates()
    chunk['CMPNT_CAT_CD_DESC'] = chunk['CMPNT_CAT_CD_DESC'].astype('category')

    merged = chunk.merge(target_map, how='left', on='CMPNT_CAT_CD_DESC')
    table = pa.Table.from_pandas(merged)

    if writer is None:
        writer = pq.ParquetWriter(final_parquet_path, table.schema, compression='snappy')

    writer.write_table(table)

if writer:
    writer.close()
    print("✅ Final Parquet file written at:", final_parquet_path)
else:
    print("⚠️ No chunks processed. Final file not written.")

# === STEP 4: Clean up temporary chunks ===
shutil.rmtree(temp_parquet_dir)
print("🧹 Temporary chunk files deleted.")
