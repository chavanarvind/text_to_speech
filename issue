#script data pull
import os
import argparse
from azureml.core import Run, Dataset

def main(output_path):
    # Get run context and workspace
    run = Run.get_context()
    ws = run.experiment.workspace

    # Get dataset by name
    dataset = Dataset.get_by_name(ws, name='harmonized_bom_data_asset')

    # Ensure output directory exists and download dataset
    os.makedirs(output_path, exist_ok=True)
    dataset.download(target_path=output_path, overwrite=True)

    print(f" Dataset downloaded to: {output_path}")

#script 2 
import os
import glob
import argparse
import pandas as pd
from azureml.core import Run

# === Column to match ===
JOIN_COL_1 = "CMPNT_CAT_CD_DESC"
JOIN_COL_2 = "CMPNT_MATL_TYPE_CD"
DIRECT_MAP_KEY_1 = "Direct Mapping"
DIRECT_MAP_KEY_2 = "Direct Mapping"

# === Key Columns to output ===
KEY_COLS = ["SRC_SYS_CD", "MATL_NUM", "PLNT_CD", "CMPNT_MATL_NUM"]

def process_file(file_path, mapping_df, mapped_dir, needs_model_dir, key_output_dir):
    try:
        print(f"\n[INFO] Processing: {os.path.basename(file_path)}")

        # Load necessary columns
        df = pd.read_parquet(file_path, columns=[
            "CMPNT_MATL_NUM", "CMPNT_MATL_DESC", "CMPNT_MATL_TYPE_CD",
            "CMPNT_CAT_CD_DESC", "CMPNT_UOM_CD"
        ])
        print(f"[INFO] Initial rows in input: {len(df)}")
        df = df[df['CMPNT_MATL_DESC'].notna()].drop_duplicates()
        print(f"[INFO] After dropping NA/duplicates: {len(df)}")

        # Normalize text in input
        df[JOIN_COL_1] = df[JOIN_COL_1].fillna("").astype(str).str.strip().str.upper()
        df[JOIN_COL_2] = df[JOIN_COL_2].fillna("").astype(str).str.strip().str.upper()

        # Clean mapping_df
        mapping_df.columns = mapping_df.columns.str.strip()
        print(f"[DEBUG] mapping_df columns after strip: {mapping_df.columns.tolist()}")
        for col in ['Final Category', 'Final Subcategory']:
            if col not in mapping_df.columns:
                print(f"[WARNING] '{col}' not in mapping_df. Adding as None.")
                mapping_df[col] = None

        print(f"[DEBUG] mapping_df dtypes before merge:\n{mapping_df.dtypes}")

        mapping_df[DIRECT_MAP_KEY_1] = mapping_df[DIRECT_MAP_KEY_1].fillna("").astype(str).str.strip().str.upper()
        mapping_df[DIRECT_MAP_KEY_2] = mapping_df[DIRECT_MAP_KEY_2].fillna("").astype(str).str.strip().str.upper()

        print(f"[INFO] Mapping DF has {len(mapping_df)} rows")

        # Stage 1 match
        stage1_df = df.merge(mapping_df, how="inner", left_on=JOIN_COL_1, right_on=DIRECT_MAP_KEY_1)
        stage1_df['matching_reason'] = 'direct_mapping_category'
        stage1_df['confidence_score'] = 1.0
        print(f"[STAGE 1] Matched rows: {len(stage1_df)}")
        print(f"[DEBUG] stage1_df columns: {stage1_df.columns.tolist()}")

        # Stage 2 match
        unmatched_df = df[~df['CMPNT_MATL_NUM'].isin(stage1_df['CMPNT_MATL_NUM'])]
        stage2_df = unmatched_df.merge(mapping_df, how="inner", left_on=JOIN_COL_2, right_on=DIRECT_MAP_KEY_2)
        stage2_df['matching_reason'] = 'direct_mapping_type_only'
        stage2_df['confidence_score'] = 1.0
        print(f"[STAGE 2] Matched rows: {len(stage2_df)}")
        print(f"[DEBUG] stage2_df columns: {stage2_df.columns.tolist()}")

        # Combine
        mapped_df = pd.concat([stage1_df, stage2_df], ignore_index=True)
        print(f"[INFO] Total mapped rows: {len(mapped_df)}")
        print(f"[DEBUG] mapped_df columns before Final Filled flags: {mapped_df.columns.tolist()}")

        # Ensure Final Category/Subcategory exist in mapped_df
        for col in ['Final Category', 'Final Subcategory']:
            if col not in mapped_df.columns:
                print(f"[WARNING] '{col}' not in mapped_df after merge. Adding as None.")
                mapped_df[col] = None

        # Fill flags
        try:
            mapped_df['Final Category Filled'] = mapped_df['Final Category'].notna() & mapped_df['Final Category'].astype(str).str.strip().ne("")
            mapped_df['Final Subcategory Filled'] = mapped_df['Final Subcategory'].notna() & mapped_df['Final Subcategory'].astype(str).str.strip().ne("")
            mapped_df['needs_model'] = ~mapped_df['Final Category Filled'] | ~mapped_df['Final Subcategory Filled']
        except Exception as e:
            print(f"[ERROR] Failed during flag creation: {e}")
            mapped_df['needs_model'] = True
            mapped_df['Final Category Filled'] = False
            mapped_df['Final Subcategory Filled'] = False

        print(f"[DEBUG] mapped_df columns after Final Filled flags: {mapped_df.columns.tolist()}")

        # Identify unmapped
        mapped_ids = set(mapped_df['CMPNT_MATL_NUM'])
        all_ids = set(df['CMPNT_MATL_NUM'])
        unmapped_ids = all_ids - mapped_ids

        if unmapped_ids:
            print(f"[WARNING] Total unmapped rows: {len(unmapped_ids)}")
            unmapped_df = df[df['CMPNT_MATL_NUM'].isin(unmapped_ids)].copy()
            unmapped_df['Final Category'] = None
            unmapped_df['Final Subcategory'] = None
            unmapped_df['needs_model'] = True
            unmapped_df['matching_reason'] = 'unmapped_fallback'
            unmapped_df['confidence_score'] = 0.0
            unmapped_df['Final Category Filled'] = False
            unmapped_df['Final Subcategory Filled'] = False
            for col in mapped_df.columns:
                if col not in unmapped_df.columns:
                    unmapped_df[col] = None
            mapped_df = pd.concat([mapped_df, unmapped_df[mapped_df.columns]], ignore_index=True)

        # Split
        mapped_only_df = mapped_df[~mapped_df['needs_model']].copy()
        needs_model_df = mapped_df[mapped_df['needs_model']].copy()

        print(f"[INFO] Fully mapped rows: {len(mapped_only_df)}")
        print(f"[INFO] Rows needing model: {len(needs_model_df)}")

        # Save
        os.makedirs(mapped_dir, exist_ok=True)
        os.makedirs(needs_model_dir, exist_ok=True)
        mapped_only_df.to_parquet(os.path.join(mapped_dir, os.path.basename(file_path)), index=False)
        needs_model_df.to_parquet(os.path.join(needs_model_dir, os.path.basename(file_path)), index=False)

        # Save key columns
        full_cols = pd.read_parquet(file_path).columns
        missing_cols = [col for col in KEY_COLS if col not in full_cols]
        if not missing_cols:
            key_df = pd.read_parquet(file_path, columns=KEY_COLS).drop_duplicates()
            os.makedirs(key_output_dir, exist_ok=True)
            key_df.to_parquet(os.path.join(key_output_dir, os.path.basename(file_path)), index=False)
            print(f"[INFO] Saved key reference: {os.path.basename(file_path)}")
        else:
            print(f"[INFO] Skipped key output â€” missing columns: {missing_cols}")

    except Exception as e:
        print(f"[ERROR] Full failure on file {file_path}: {e}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", type=str, required=True)
    parser.add_argument("--mapping_csv", type=str, required=True)
    parser.add_argument("--mapped_output", type=str, required=True)
    parser.add_argument("--needs_model_output", type=str, required=True)
    parser.add_argument("--key_output", type=str, required=True)
    args = parser.parse_args()

    run = Run.get_context()

    csv_files = glob.glob(os.path.join(args.mapping_csv, "*.csv"))
    if not csv_files:
        raise FileNotFoundError(f"No CSV file found in mapped directory: {args.mapping_csv}")

    mapping_df = pd.read_csv(csv_files[0]).drop_duplicates()

    all_files = glob.glob(os.path.join(args.input_path, "*.parquet"))
    for file_path in all_files:
        process_file(file_path, mapping_df, args.mapped_output, args.needs_model_output, args.key_output)

if __name__ == "__main__":
    main()

#pipeline
from azureml.core import Workspace, Environment, Experiment, Dataset, Datastore
from azureml.pipeline.core import Pipeline
from azureml.pipeline.steps import PythonScriptStep
from azureml.core.runconfig import RunConfiguration
from azureml.data import OutputFileDatasetConfig
from datetime import datetime
import sys

# === Date ===
today = datetime.today().strftime("%d%m%Y")

# === Load Workspace ===
SUBSCRIPTION_ID = "a8d518a9-4587-4ba2-9a60-68b980c2f000"
RESOURCE_GROUP = "AZR-WDZ-DTO-AML-Development"
WORKSPACE_NAME = "AML-DTO-Marmot-dev"

def get_workspace(use_sp_auth=False, args=None):
    if use_sp_auth and args and len(args) >= 4:
        from azureml.core.authentication import ServicePrincipalAuthentication
        sp_auth = ServicePrincipalAuthentication(
            tenant_id=args[1],
            service_principal_id=args[2],
            service_principal_password=args[3]
        )
        return Workspace(subscription_id=SUBSCRIPTION_ID,
                         resource_group=RESOURCE_GROUP,
                         workspace_name=WORKSPACE_NAME,
                         auth=sp_auth)
    else:
        return Workspace(subscription_id=SUBSCRIPTION_ID,
                         resource_group=RESOURCE_GROUP,
                         workspace_name=WORKSPACE_NAME)

# === Init ===
ws = get_workspace(use_sp_auth=(len(sys.argv) > 3), args=sys.argv)
#compute_name = "llm-gpu-cluster-2"
compute_name = "rpmdataprocess"
env = Environment.get(workspace=ws, name="Bom_X_Evaluator")
ds = ws.datastores["xbomrefadlsg2"]

run_config = RunConfiguration()
run_config.target = compute_name
run_config.environment = env

# === Use registered datasets ===
raw_input = Dataset.get_by_name(ws, "raw_predictions").as_named_input("inference_output_dir").as_mount()
key_input = Dataset.get_by_name(ws, "hbom_key_files").as_named_input("key_output_dir").as_mount()

# === Output location for final .parquet ===
final_merged_output = OutputFileDatasetConfig(
    name="final_merged_predictions",
    destination=(ds, f"hbom_category_prediction/hbom_category_prediction_{today}/")
)

# === Step 6: Finalize Output ===
finalize_step = PythonScriptStep(
    name="Step 6 - Finalize Output (Priority + Dedup)",
    script_name="step_6_finalize_output_global_debug.py",
    source_directory="scripts",
    arguments=[
        "--inference_output_dir", raw_input,
        "--key_output_dir", key_input,
        "--final_output_dir", final_merged_output
    ],
    inputs=[raw_input, key_input],
    outputs=[final_merged_output],
    compute_target=compute_name,
    runconfig=run_config,
    allow_reuse=False
)

# === Build & Submit ===
pipeline = Pipeline(workspace=ws, steps=[finalize_step])
pipeline.validate()

experiment = Experiment(ws, "step6_finalize_only_pipeline")
run = experiment.submit(pipeline)
run.wait_for_completion(show_output=True)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_path", type=str, required=True)
    args = parser.parse_args()

    main(args.output_path)
